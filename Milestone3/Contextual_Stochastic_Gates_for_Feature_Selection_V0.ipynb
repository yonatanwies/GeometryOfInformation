{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AveOccup  MedInc  Uniform_Noise  Cosine_Values  AveRooms  HouseAge  \\\n",
      "0  2.555556  8.3252       0.143745       1.000000  6.984127      41.0   \n",
      "1  2.109842  8.3014      -0.515390       1.000000  6.238137      21.0   \n",
      "2  2.802260  7.2574       0.677635       1.000000  8.288136      52.0   \n",
      "3  2.547945  5.6431      -0.038881       0.999999  5.817352      52.0   \n",
      "4  2.181467  3.8462       0.285882       0.999998  6.281853      52.0   \n",
      "\n",
      "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
      "0        0.496714       322.0    -122.23   1.023810     37.88  \n",
      "1       -0.138264      2401.0    -122.22   0.971880     37.86  \n",
      "2        0.647689       496.0    -122.24   1.073446     37.85  \n",
      "3        1.523030       558.0    -122.25   1.073059     37.85  \n",
      "4       -0.234153       565.0    -122.25   1.081081     37.85  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Values'] = cosine_values\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "print(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define the Hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypernetwork takes contextual variables as input and outputs parameters for the Bernoulli distributions of the stochastic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, context_dim, feature_dim):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        # Define a simple feed-forward network as an example\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(context_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, feature_dim)\n",
    "            # No sigmoid activation here because we do not want to restrict the output between 0 and 1 yet\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Outputs the parameters for the Bernoulli distributions\n",
    "        return self.network(context)\n",
    "\n",
    "\n",
    "def stochastic_gates(mu, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Applies Gaussian-based continuous relaxation to the Bernoulli variables.\n",
    "    :param mu: Mean parameters of the Bernoulli distributions.\n",
    "    :param sigma: Standard deviation for the Gaussian noise.\n",
    "    :return: Relaxed gates after applying noise and clipping.\n",
    "    \"\"\"\n",
    "    # Add Gaussian noise to the parameters\n",
    "    epsilon = torch.randn_like(mu) * sigma\n",
    "    # Apply the continuous relaxation\n",
    "    gates = torch.clamp(mu + epsilon, 0, 1)\n",
    "    return gates\n",
    "\n",
    "# Fixed sigma value as per the process described\n",
    "fixed_sigma = 1.0  # or any other value specified in the article or experiment setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class HyperNetwork(nn.Module):\n",
    "#     def __init__(self, context_dim, feature_dim):\n",
    "#         super(HyperNetwork, self).__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(context_dim, 64),  # Adjust sizes as necessary\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, feature_dim),\n",
    "#             nn.Sigmoid()  # Ensures output is between 0 and 1\n",
    "#         )\n",
    "\n",
    "#     def forward(self, context):\n",
    "#         return self.network(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Prediction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction network maps the selected features to the response variable. It takes both the original features and the stochastic gates as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),  # Adjust sizes as necessary\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Assuming a single continuous outcome\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.network(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stochastic_gates(bernoulli_params, device='cpu'):\n",
    "#     sigma = 1.0  # Hyperparameter for Gaussian noise\n",
    "#     epsilon = Normal(0, sigma).sample(bernoulli_params.shape).to(device)\n",
    "#     gates = torch.sigmoid(bernoulli_params + epsilon)  # Apply the sigmoid to approximate the Bernoulli distribution\n",
    "#     return gates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PredictionNetwork(input_feature_dim)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# list_contexts = [2, 3, 4, 5, 6, 7, 8, 9, 10]  # List of context dimensions to try\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize dictionary to store feature combinations and their best validation loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# feature_combinations = {}\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# best_val_loss = float('inf')\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m explanatory_feature_names \u001b[38;5;241m=\u001b[39m feature_names[\u001b[43mcontext_dim\u001b[49m:]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with first \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as context\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Reinitialize networks for each context dimension\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'context_dim' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Assuming the definitions of HyperNetwork, PredictionNetwork, stochastic_gates, and the dataset preparation code are as previously defined\n",
    "\n",
    "# Define a function to dynamically adjust the size of the prediction network based on context dimension\n",
    "def adjust_prediction_network(input_feature_dim):\n",
    "    return PredictionNetwork(input_feature_dim)\n",
    "\n",
    "# list_contexts = [2, 3, 4, 5, 6, 7, 8, 9, 10]  # List of context dimensions to try\n",
    "\n",
    "# Initialize dictionary to store feature combinations and their best validation loss\n",
    "# feature_combinations = {}\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# explanatory_feature_names = feature_names[context_dim:]\n",
    "# print(f'\\nTraining with first {context_dim} features as context\\n' + '-'*50)\n",
    "\n",
    "# Reinitialize networks for each context dimension\n",
    "hypernetwork = HyperNetwork(context_dim, X_train_tensor.shape[1] - context_dim)  # Adjust the input size of hypernetwork based on context_dim\n",
    "prediction_network = adjust_prediction_network(X_train_tensor.shape[1] - context_dim)  # Adjust the input size of prediction network based on remaining features\n",
    "\n",
    "# Reinitialize the optimizer\n",
    "optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Training step\n",
    "    contexts = X_train_tensor[:, :context_dim]\n",
    "    features = X_train_tensor[:, context_dim:]\n",
    "    \n",
    "    bernoulli_params = hypernetwork(contexts)\n",
    "    gates = stochastic_gates(bernoulli_params, sigma=fixed_sigma)    \n",
    "    # selected_features = features * gates\n",
    "    predictions = prediction_network(selected_features)\n",
    "    \n",
    "    loss = loss_fn(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        contexts_val = X_test_tensor[:, :context_dim]\n",
    "        features_val = X_test_tensor[:, context_dim:]\n",
    "        \n",
    "        bernoulli_params_val = hypernetwork(contexts_val)\n",
    "        gates_val = stochastic_gates(bernoulli_params, sigma=fixed_sigma)    \n",
    "        selected_features_val = features_val * gates_val\n",
    "        predictions_val = prediction_network(selected_features_val)\n",
    "        \n",
    "        val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "    \n",
    "    # Store feature combinations based on the top N important features for each epoch\n",
    "    N = 5  # Adjust based on the desired top N features\n",
    "    avg_bernoulli_params = bernoulli_params_val.mean(dim=0)\n",
    "    top_features_indices = torch.argsort(avg_bernoulli_params, descending=True)[:N]\n",
    "    top_features_tuple = tuple(top_features_indices.cpu().numpy())\n",
    "    \n",
    "    feature_names_tuple = tuple(explanatory_feature_names[top_features_indices])\n",
    "    \n",
    "    if feature_names_tuple in feature_combinations:\n",
    "        if feature_combinations[feature_names_tuple]['val_loss'] > val_loss.item():\n",
    "            feature_combinations[feature_names_tuple]['val_loss'] = val_loss.item()\n",
    "    else:\n",
    "        feature_combinations[feature_names_tuple] = {'val_loss': val_loss.item(), 'avg_params': avg_bernoulli_params[top_features_indices].cpu().numpy()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "# Display the best feature combination\n",
    "best_combination = min(feature_combinations.items(), key=lambda x: x[1]['val_loss'])\n",
    "print(\"\\nBest feature combination (names):\", best_combination[0])\n",
    "print(\"Validation loss for best combination:\", best_combination[1]['val_loss'])\n",
    "print(\"Average Bernoulli parameters for best combination:\", best_combination[1]['avg_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "    \n",
    "#     # Training step\n",
    "#     contexts = X_train_tensor[:, :context_dim]\n",
    "#     features = X_train_tensor[:, context_dim:]\n",
    "    \n",
    "#     bernoulli_params = hypernetwork(contexts)\n",
    "#     gates = stochastic_gates(bernoulli_params)\n",
    "#     selected_features = features * gates\n",
    "#     predictions = prediction_network(selected_features)\n",
    "    \n",
    "#     loss = loss_fn(predictions, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Validation step\n",
    "#     with torch.no_grad():\n",
    "#         contexts_val = X_test_tensor[:, :context_dim]\n",
    "#         features_val = X_test_tensor[:, context_dim:]\n",
    "        \n",
    "#         bernoulli_params_val = hypernetwork(contexts_val)\n",
    "#         gates_val = stochastic_gates(bernoulli_params_val)\n",
    "#         selected_features_val = features_val * gates_val\n",
    "#         predictions_val = prediction_network(selected_features_val)\n",
    "        \n",
    "#         val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the best feature combinations found by the model, we can analyze the gates (stochastic gates) produced by the hypernetwork for each feature across the validation set. The gates closest to 1 indicate features that are most relevant for the prediction according to the model. However, since the gates are produced in a stochastic manner, a straightforward approach is to use the mean of the Bernoulli parameters (π) output by the hypernetwork as an approximation of feature importance. This way, we can determine which features are consistently considered important across different contexts.\n",
    "\n",
    "Below is the code to achieve this, including a modification to compute the average Bernoulli parameters across the validation set and then display the best feature combinations based on these averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (11) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m bernoulli_params \u001b[38;5;241m=\u001b[39m hypernetwork(contexts)\n\u001b[0;32m     13\u001b[0m gates \u001b[38;5;241m=\u001b[39m stochastic_gates(bernoulli_params)\n\u001b[1;32m---> 14\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgates\u001b[49m\n\u001b[0;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m prediction_network(selected_features)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predictions, y_train_tensor)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (11) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "list_contexts = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # List of context dimensions to try\n",
    "for context_dim in list_contexts:\n",
    "    # Assume first 3 features are \"contextual\" for demonstration purposes\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Training step\n",
    "        contexts = X_train_tensor[:, :context_dim]\n",
    "        features = X_train_tensor[:, context_dim:]\n",
    "        \n",
    "        bernoulli_params = hypernetwork(contexts)\n",
    "        gates = stochastic_gates(bernoulli_params)\n",
    "        selected_features = features * gates\n",
    "        predictions = prediction_network(selected_features)\n",
    "        \n",
    "        loss = loss_fn(predictions, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            contexts_val = X_test_tensor[:, :context_dim]\n",
    "            features_val = X_test_tensor[:, context_dim:]\n",
    "            \n",
    "            bernoulli_params_val = hypernetwork(contexts_val)\n",
    "            gates_val = stochastic_gates(bernoulli_params_val)\n",
    "            selected_features_val = features_val * gates_val\n",
    "            predictions_val = prediction_network(selected_features_val)\n",
    "            \n",
    "            val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "    feature_dim = X_train_tensor.shape[1] - context_dim\n",
    "\n",
    "    # Update Hypernetwork and PredictionNetwork definitions if needed\n",
    "\n",
    "    # Initialize models\n",
    "    hypernetwork = HyperNetwork(context_dim, feature_dim)\n",
    "    prediction_network = PredictionNetwork(feature_dim)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    hypernetwork = HyperNetwork(context_dim=context_dim, feature_dim=X_train_tensor.shape[1])\n",
    "\n",
    "\n",
    "    # Initialize dictionary to store feature combinations and their best validation loss\n",
    "    feature_combinations = {}\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Training step\n",
    "        contexts = X_train_tensor[:, :context_dim]\n",
    "        features = X_train_tensor[:, context_dim:]\n",
    "        \n",
    "        bernoulli_params = hypernetwork(contexts)\n",
    "        gates = stochastic_gates(bernoulli_params)\n",
    "        selected_features = features * gates\n",
    "        predictions = prediction_network(selected_features)\n",
    "        \n",
    "        loss = loss_fn(predictions, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            contexts_val = X_test_tensor[:, :context_dim]\n",
    "            features_val = X_test_tensor[:, context_dim:]\n",
    "            \n",
    "            bernoulli_params_val = hypernetwork(contexts_val)\n",
    "            gates_val = stochastic_gates(bernoulli_params_val)\n",
    "            selected_features_val = features_val * gates_val\n",
    "            predictions_val = prediction_network(selected_features_val)\n",
    "            \n",
    "            val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "\n",
    "            # Compute average Bernoulli parameters for the validation set\n",
    "        with torch.no_grad():\n",
    "            contexts_val = X_test_tensor[:, :context_dim]\n",
    "            avg_bernoulli_params = hypernetwork(contexts_val).mean(dim=0)\n",
    "\n",
    "        # Sort features based on average Bernoulli parameters\n",
    "        sorted_features = torch.argsort(avg_bernoulli_params, descending=True)\n",
    "        sorted_params = torch.sort(avg_bernoulli_params, descending=True).values\n",
    "\n",
    "        # Print the sorted features and their corresponding Bernoulli parameters\n",
    "        print(\"Feature Importance based on average Bernoulli parameters:\")\n",
    "        for i, (feature_index, importance) in enumerate(zip(sorted_features, sorted_params)):\n",
    "            print(f\"Feature {feature_index + context_dim} (Original Index): Importance {importance.item()}\")\n",
    "\n",
    "        # Optionally, select top N features to consider as \"best\" feature combinations\n",
    "        N = 5  # Change N based on how many top features you want to consider\n",
    "        top_features = sorted_features[:N]\n",
    "        print(f\"\\nTop {N} Most Important Features (Adjusted Index): {top_features + context_dim}\")\n",
    "\n",
    "\n",
    "        # Store feature combinations based on the top N important features for each epoch\n",
    "        N = 5  # Change based on desired top N features\n",
    "        avg_bernoulli_params = bernoulli_params_val.mean(dim=0)\n",
    "        top_features_indices = torch.argsort(avg_bernoulli_params, descending=True)[:N]\n",
    "        top_features_tuple = tuple(top_features_indices.cpu().numpy())\n",
    "        \n",
    "        if top_features_tuple in feature_combinations:\n",
    "            if feature_combinations[top_features_tuple]['val_loss'] > val_loss.item():\n",
    "                feature_combinations[top_features_tuple]['val_loss'] = val_loss.item()\n",
    "        else:\n",
    "            feature_combinations[top_features_tuple] = {'val_loss': val_loss.item(), 'avg_params': avg_bernoulli_params[top_features_indices].cpu().numpy()}\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    # Display the best feature combination\n",
    "    best_combination = min(feature_combinations.items(), key=lambda x: x[1]['val_loss'])\n",
    "    print(\"\\nBest feature combination (indices):\", best_combination[0])\n",
    "    print(\"Validation loss for best combination:\", best_combination[1]['val_loss'])\n",
    "    print(\"Average Bernoulli parameters for best combination:\", best_combination[1]['avg_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
