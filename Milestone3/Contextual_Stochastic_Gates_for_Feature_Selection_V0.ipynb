{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AveOccup  MedInc  Uniform_Noise  Cosine_Values  AveRooms  HouseAge  \\\n",
      "0  2.555556  8.3252       0.143745       1.000000  6.984127      41.0   \n",
      "1  2.109842  8.3014      -0.515390       1.000000  6.238137      21.0   \n",
      "2  2.802260  7.2574       0.677635       1.000000  8.288136      52.0   \n",
      "3  2.547945  5.6431      -0.038881       0.999999  5.817352      52.0   \n",
      "4  2.181467  3.8462       0.285882       0.999998  6.281853      52.0   \n",
      "\n",
      "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
      "0        0.496714       322.0    -122.23   1.023810     37.88  \n",
      "1       -0.138264      2401.0    -122.22   0.971880     37.86  \n",
      "2        0.647689       496.0    -122.24   1.073446     37.85  \n",
      "3        1.523030       558.0    -122.25   1.073059     37.85  \n",
      "4       -0.234153       565.0    -122.25   1.081081     37.85  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Values'] = cosine_values\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "print(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define the Hypernetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypernetwork takes contextual variables as input and outputs parameters for the Bernoulli distributions of the stochastic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, context_dim, feature_dim):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(context_dim, 64),  # Adjust sizes as necessary\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, feature_dim),\n",
    "            nn.Sigmoid()  # Ensures output is between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, context):\n",
    "        return self.network(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Prediction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction network maps the selected features to the response variable. It takes both the original features and the stochastic gates as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),  # Adjust sizes as necessary\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Assuming a single continuous outcome\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.network(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gates(bernoulli_params, device='cpu'):\n",
    "    sigma = 1.0  # Hyperparameter for Gaussian noise\n",
    "    epsilon = Normal(0, sigma).sample(bernoulli_params.shape).to(device)\n",
    "    gates = torch.sigmoid(bernoulli_params + epsilon)  # Apply the sigmoid to approximate the Bernoulli distribution\n",
    "    return gates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avivg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16512])) that is different to the input size (torch.Size([16512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\avivg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4128])) that is different to the input size (torch.Size([4128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 4.889840602874756, Validation Loss: 4.750686168670654\n",
      "Epoch 11, Training Loss: 4.373039722442627, Validation Loss: 4.240394592285156\n",
      "Epoch 21, Training Loss: 3.894871473312378, Validation Loss: 3.7698419094085693\n",
      "Epoch 31, Training Loss: 3.453162908554077, Validation Loss: 3.3406975269317627\n",
      "Epoch 41, Training Loss: 3.040623664855957, Validation Loss: 2.938717842102051\n",
      "Epoch 51, Training Loss: 2.661078929901123, Validation Loss: 2.5778002738952637\n",
      "Epoch 61, Training Loss: 2.3257458209991455, Validation Loss: 2.259857654571533\n",
      "Epoch 71, Training Loss: 2.0462188720703125, Validation Loss: 1.9626799821853638\n",
      "Epoch 81, Training Loss: 1.821929931640625, Validation Loss: 1.7591900825500488\n",
      "Epoch 91, Training Loss: 1.6745803356170654, Validation Loss: 1.620639443397522\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.5713249444961548, Validation Loss: 1.5317533016204834\n",
      "Epoch 11, Training Loss: 1.5220485925674438, Validation Loss: 1.4840314388275146\n",
      "Epoch 21, Training Loss: 1.4977829456329346, Validation Loss: 1.4694759845733643\n",
      "Epoch 31, Training Loss: 1.482914686203003, Validation Loss: 1.4629521369934082\n",
      "Epoch 41, Training Loss: 1.4752850532531738, Validation Loss: 1.4683964252471924\n",
      "Epoch 51, Training Loss: 1.4687414169311523, Validation Loss: 1.4382028579711914\n",
      "Epoch 61, Training Loss: 1.4637579917907715, Validation Loss: 1.4313669204711914\n",
      "Epoch 71, Training Loss: 1.470191240310669, Validation Loss: 1.4284188747406006\n",
      "Epoch 81, Training Loss: 1.4533123970031738, Validation Loss: 1.4246701002120972\n",
      "Epoch 91, Training Loss: 1.4487234354019165, Validation Loss: 1.4211912155151367\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Training step\n",
    "    contexts = X_train_tensor[:, :context_dim]\n",
    "    features = X_train_tensor[:, context_dim:]\n",
    "    \n",
    "    bernoulli_params = hypernetwork(contexts)\n",
    "    gates = stochastic_gates(bernoulli_params)\n",
    "    selected_features = features * gates\n",
    "    predictions = prediction_network(selected_features)\n",
    "    \n",
    "    loss = loss_fn(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        contexts_val = X_test_tensor[:, :context_dim]\n",
    "        features_val = X_test_tensor[:, context_dim:]\n",
    "        \n",
    "        bernoulli_params_val = hypernetwork(contexts_val)\n",
    "        gates_val = stochastic_gates(bernoulli_params_val)\n",
    "        selected_features_val = features_val * gates_val\n",
    "        predictions_val = prediction_network(selected_features_val)\n",
    "        \n",
    "        val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the best feature combinations found by the model, we can analyze the gates (stochastic gates) produced by the hypernetwork for each feature across the validation set. The gates closest to 1 indicate features that are most relevant for the prediction according to the model. However, since the gates are produced in a stochastic manner, a straightforward approach is to use the mean of the Bernoulli parameters (π) output by the hypernetwork as an approximation of feature importance. This way, we can determine which features are consistently considered important across different contexts.\n",
    "\n",
    "Below is the code to achieve this, including a modification to compute the average Bernoulli parameters across the validation set and then display the best feature combinations based on these averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance based on average Bernoulli parameters:\n",
      "Feature 7 (Original Index): Importance 0.592074990272522\n",
      "Feature 10 (Original Index): Importance 0.5558695793151855\n",
      "Feature 5 (Original Index): Importance 0.5503217577934265\n",
      "Feature 11 (Original Index): Importance 0.5348395109176636\n",
      "Feature 1 (Original Index): Importance 0.5235576629638672\n",
      "Feature 4 (Original Index): Importance 0.5093138813972473\n",
      "Feature 9 (Original Index): Importance 0.45627906918525696\n",
      "Feature 8 (Original Index): Importance 0.4559199810028076\n",
      "Feature 2 (Original Index): Importance 0.4457564949989319\n",
      "Feature 3 (Original Index): Importance 0.4329746961593628\n",
      "Feature 6 (Original Index): Importance 0.38816526532173157\n",
      "\n",
      "Top 5 Most Important Features (Adjusted Index): tensor([ 7, 10,  5, 11,  1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m X_train_tensor[:, :context_dim]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "list_contexts = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # List of context dimensions to try\n",
    "for context_dim in list_contexts:\n",
    "    # Assume first 3 features are \"contextual\" for demonstration purposes\n",
    "    context_dim = 3\n",
    "    feature_dim = X_train_tensor.shape[1] - context_dim\n",
    "\n",
    "    # Update Hypernetwork and PredictionNetwork definitions if needed\n",
    "\n",
    "    # Initialize models\n",
    "    hypernetwork = HyperNetwork(context_dim, feature_dim)\n",
    "    prediction_network = PredictionNetwork(feature_dim)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    hypernetwork = HyperNetwork(context_dim=1, feature_dim=X_train_tensor.shape[1])\n",
    "\n",
    "    # Compute average Bernoulli parameters for the validation set\n",
    "    with torch.no_grad():\n",
    "        contexts_val = X_test_tensor[:, :context_dim]\n",
    "        avg_bernoulli_params = hypernetwork(contexts_val).mean(dim=0)\n",
    "\n",
    "    # Sort features based on average Bernoulli parameters\n",
    "    sorted_features = torch.argsort(avg_bernoulli_params, descending=True)\n",
    "    sorted_params = torch.sort(avg_bernoulli_params, descending=True).values\n",
    "\n",
    "    # Print the sorted features and their corresponding Bernoulli parameters\n",
    "    print(\"Feature Importance based on average Bernoulli parameters:\")\n",
    "    for i, (feature_index, importance) in enumerate(zip(sorted_features, sorted_params)):\n",
    "        print(f\"Feature {feature_index + context_dim} (Original Index): Importance {importance.item()}\")\n",
    "\n",
    "    # Optionally, select top N features to consider as \"best\" feature combinations\n",
    "    N = 5  # Change N based on how many top features you want to consider\n",
    "    top_features = sorted_features[:N]\n",
    "    print(f\"\\nTop {N} Most Important Features (Adjusted Index): {top_features + context_dim}\")\n",
    "\n",
    "\n",
    "    # Initialize dictionary to store feature combinations and their best validation loss\n",
    "    feature_combinations = {}\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Training step\n",
    "        contexts = X_train_tensor[:, :context_dim]\n",
    "        features = X_train_tensor[:, context_dim:]\n",
    "        \n",
    "        bernoulli_params = hypernetwork(contexts)\n",
    "        gates = stochastic_gates(bernoulli_params)\n",
    "        selected_features = features * gates\n",
    "        predictions = prediction_network(selected_features)\n",
    "        \n",
    "        loss = loss_fn(predictions, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            contexts_val = X_test_tensor[:, :context_dim]\n",
    "            features_val = X_test_tensor[:, context_dim:]\n",
    "            \n",
    "            bernoulli_params_val = hypernetwork(contexts_val)\n",
    "            gates_val = stochastic_gates(bernoulli_params_val)\n",
    "            selected_features_val = features_val * gates_val\n",
    "            predictions_val = prediction_network(selected_features_val)\n",
    "            \n",
    "            val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "        # Store feature combinations based on the top N important features for each epoch\n",
    "        N = 5  # Change based on desired top N features\n",
    "        avg_bernoulli_params = bernoulli_params_val.mean(dim=0)\n",
    "        top_features_indices = torch.argsort(avg_bernoulli_params, descending=True)[:N]\n",
    "        top_features_tuple = tuple(top_features_indices.cpu().numpy())\n",
    "        \n",
    "        if top_features_tuple in feature_combinations:\n",
    "            if feature_combinations[top_features_tuple]['val_loss'] > val_loss.item():\n",
    "                feature_combinations[top_features_tuple]['val_loss'] = val_loss.item()\n",
    "        else:\n",
    "            feature_combinations[top_features_tuple] = {'val_loss': val_loss.item(), 'avg_params': avg_bernoulli_params[top_features_indices].cpu().numpy()}\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    # Display the best feature combination\n",
    "    best_combination = min(feature_combinations.items(), key=lambda x: x[1]['val_loss'])\n",
    "    print(\"\\nBest feature combination (indices):\", best_combination[0])\n",
    "    print(\"Validation loss for best combination:\", best_combination[1]['val_loss'])\n",
    "    print(\"Average Bernoulli parameters for best combination:\", best_combination[1]['avg_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.4472154378890991, Validation Loss: 1.4151694774627686\n",
      "Epoch 11, Training Loss: 1.4414145946502686, Validation Loss: 1.4116743803024292\n",
      "Epoch 21, Training Loss: 1.4412184953689575, Validation Loss: 1.4077492952346802\n",
      "Epoch 31, Training Loss: 1.4331527948379517, Validation Loss: 1.4035084247589111\n",
      "Epoch 41, Training Loss: 1.4280548095703125, Validation Loss: 1.4010308980941772\n",
      "Epoch 51, Training Loss: 1.4253065586090088, Validation Loss: 1.3972856998443604\n",
      "Epoch 61, Training Loss: 1.4205454587936401, Validation Loss: 1.3926745653152466\n",
      "Epoch 71, Training Loss: 1.424072265625, Validation Loss: 1.3922308683395386\n",
      "Epoch 81, Training Loss: 1.415163278579712, Validation Loss: 1.387839436531067\n",
      "Epoch 91, Training Loss: 1.4128848314285278, Validation Loss: 1.3898916244506836\n",
      "Epoch 100, Training Loss: 1.4109370708465576, Validation Loss: 1.38599693775177\n",
      "\n",
      "Best feature combination (indices): (0, 5, 2, 7, 3)\n",
      "Validation loss for best combination: 1.3809586763381958\n",
      "Average Bernoulli parameters for best combination: [0.933841   0.93379706 0.91557854 0.9152301  0.89982474]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming the boston dataset and the feature names are loaded as shown previously\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mboston\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_names)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Modify the feature names array to match the division between \"contextual\" and \"explanatory\" features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# For simplicity, we consider the first 'context_dim' features as contextual and the rest as explanatory here\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Adjust this as per your actual model's design\u001b[39;00m\n\u001b[0;32m      7\u001b[0m explanatory_feature_names \u001b[38;5;241m=\u001b[39m feature_names[context_dim:]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boston' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming the boston dataset and the feature names are loaded as shown previously\n",
    "feature_names = np.array(boston.feature_names)\n",
    "\n",
    "# Modify the feature names array to match the division between \"contextual\" and \"explanatory\" features\n",
    "# For simplicity, we consider the first 'context_dim' features as contextual and the rest as explanatory here\n",
    "# Adjust this as per your actual model's design\n",
    "explanatory_feature_names = feature_names[context_dim:]\n",
    "\n",
    "# Initialize dictionary to store feature combinations and their best validation loss\n",
    "feature_combinations = {}\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Training step\n",
    "    contexts = X_train_tensor[:, :context_dim]\n",
    "    features = X_train_tensor[:, context_dim:]\n",
    "    \n",
    "    bernoulli_params = hypernetwork(contexts)\n",
    "    gates = stochastic_gates(bernoulli_params)\n",
    "    selected_features = features * gates\n",
    "    predictions = prediction_network(selected_features)\n",
    "    \n",
    "    loss = loss_fn(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        contexts_val = X_test_tensor[:, :context_dim]\n",
    "        features_val = X_test_tensor[:, context_dim:]\n",
    "        \n",
    "        bernoulli_params_val = hypernetwork(contexts_val)\n",
    "        gates_val = stochastic_gates(bernoulli_params_val)\n",
    "        selected_features_val = features_val * gates_val\n",
    "        predictions_val = prediction_network(selected_features_val)\n",
    "        \n",
    "        val_loss = loss_fn(predictions_val, y_test_tensor)\n",
    "\n",
    "    # Store feature combinations based on the top N important features for each epoch\n",
    "    N = 5  # Adjust based on the desired top N features\n",
    "    avg_bernoulli_params = bernoulli_params_val.mean(dim=0)\n",
    "    top_features_indices = torch.argsort(avg_bernoulli_params, descending=True)[:N]\n",
    "    top_features_tuple = tuple(top_features_indices.cpu().numpy())\n",
    "    \n",
    "    feature_names_tuple = tuple(explanatory_feature_names[top_features_indices])\n",
    "    \n",
    "    if feature_names_tuple in feature_combinations:\n",
    "        if feature_combinations[feature_names_tuple]['val_loss'] > val_loss.item():\n",
    "            feature_combinations[feature_names_tuple]['val_loss'] = val_loss.item()\n",
    "    else:\n",
    "        feature_combinations[feature_names_tuple] = {'val_loss': val_loss.item(), 'avg_params': avg_bernoulli_params[top_features_indices].cpu().numpy()}\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "# Display the best feature combination\n",
    "best_combination = min(feature_combinations.items(), key=lambda x: x[1]['val_loss'])\n",
    "print(\"\\nBest feature combination (names):\", best_combination[0])\n",
    "print(\"Validation loss for best combination:\", best_combination[1]['val_loss'])\n",
    "print(\"Average Bernoulli parameters for best combination:\", best_combination[1]['avg_params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
