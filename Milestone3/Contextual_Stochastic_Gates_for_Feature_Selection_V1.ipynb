{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Uniform_Noise</th>\n",
       "      <th>Cosine_Noise</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Gaussian_Noise</th>\n",
       "      <th>Population</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.247299</td>\n",
       "      <td>4.5625</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>-0.958842</td>\n",
       "      <td>4.845138</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-0.706843</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>-122.59</td>\n",
       "      <td>1.027611</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.267930</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>-0.134879</td>\n",
       "      <td>0.105413</td>\n",
       "      <td>5.262517</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.869742</td>\n",
       "      <td>2415.0</td>\n",
       "      <td>-119.19</td>\n",
       "      <td>1.012179</td>\n",
       "      <td>34.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.445217</td>\n",
       "      <td>5.2174</td>\n",
       "      <td>0.784740</td>\n",
       "      <td>0.261862</td>\n",
       "      <td>7.306957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.116566</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>-117.21</td>\n",
       "      <td>1.078261</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.345733</td>\n",
       "      <td>2.3083</td>\n",
       "      <td>-0.577435</td>\n",
       "      <td>-0.877524</td>\n",
       "      <td>5.485777</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.198482</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-122.63</td>\n",
       "      <td>1.262582</td>\n",
       "      <td>38.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.496000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.833185</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>5.442667</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.133279</td>\n",
       "      <td>936.0</td>\n",
       "      <td>-117.24</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>34.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveOccup  MedInc  Uniform_Noise  Cosine_Noise  AveRooms  HouseAge  \\\n",
       "0  2.247299  4.5625       0.760274     -0.958842  4.845138      46.0   \n",
       "1  3.267930  4.5000      -0.134879      0.105413  5.262517      17.0   \n",
       "2  3.445217  5.2174       0.784740      0.261862  7.306957       5.0   \n",
       "3  2.345733  2.3083      -0.577435     -0.877524  5.485777      20.0   \n",
       "4  2.496000  6.0000       0.833185      0.042673  5.442667      26.0   \n",
       "\n",
       "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
       "0       -0.706843      1872.0    -122.59   1.027611     37.97  \n",
       "1       -1.869742      2415.0    -119.19   1.012179     34.23  \n",
       "2        0.116566      3962.0    -117.21   1.078261     33.95  \n",
       "3        1.198482      1072.0    -122.63   1.262582     38.96  \n",
       "4       -0.133279       936.0    -117.24   0.781333     34.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Noise'] = cosine_values\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "display(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################################################################################################\n",
      "tensor([[0.4252, 0.5598, 0.4586,  ..., 0.6082, 0.4405, 0.5994],\n",
      "        [0.5043, 0.4448, 0.5515,  ..., 0.4905, 0.4877, 0.5178],\n",
      "        [0.4207, 0.4792, 0.4999,  ..., 0.5122, 0.4387, 0.4857],\n",
      "        ...,\n",
      "        [0.4273, 0.4794, 0.4464,  ..., 0.6425, 0.4298, 0.6029],\n",
      "        [0.4002, 0.6063, 0.5224,  ..., 0.5475, 0.3892, 0.5517],\n",
      "        [0.5236, 0.5496, 0.5005,  ..., 0.5129, 0.5045, 0.5714]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 1, Loss: 1209.390380859375, Val Loss: 314.52813720703125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3881, 0.4911, 0.4577,  ..., 0.6336, 0.4640, 0.5660],\n",
      "        [0.4377, 0.5947, 0.4717,  ..., 0.5144, 0.4160, 0.5034],\n",
      "        [0.2758, 0.3813, 0.4345,  ..., 0.5816, 0.3625, 0.5342],\n",
      "        ...,\n",
      "        [0.4443, 0.5240, 0.5238,  ..., 0.5918, 0.4606, 0.5684],\n",
      "        [0.2906, 0.5680, 0.5466,  ..., 0.6144, 0.4243, 0.4365],\n",
      "        [0.4301, 0.5554, 0.4398,  ..., 0.6078, 0.4249, 0.5305]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 2, Loss: 1207.3076171875, Val Loss: 313.050537109375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3712, 0.5224, 0.4626,  ..., 0.4552, 0.4854, 0.5016],\n",
      "        [0.4051, 0.4531, 0.4836,  ..., 0.5267, 0.3813, 0.5099],\n",
      "        [0.3097, 0.4790, 0.4629,  ..., 0.4814, 0.3697, 0.4535],\n",
      "        ...,\n",
      "        [0.4208, 0.5243, 0.4802,  ..., 0.5679, 0.4543, 0.5280],\n",
      "        [0.3941, 0.6403, 0.5055,  ..., 0.5130, 0.4254, 0.5660],\n",
      "        [0.4029, 0.3704, 0.4413,  ..., 0.6622, 0.4114, 0.6117]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 3, Loss: 1204.8551025390625, Val Loss: 312.4458312988281\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3705, 0.5254, 0.5167,  ..., 0.4796, 0.3709, 0.4886],\n",
      "        [0.4904, 0.4693, 0.5596,  ..., 0.5652, 0.4391, 0.5681],\n",
      "        [0.3005, 0.5624, 0.6268,  ..., 0.4613, 0.3733, 0.4985],\n",
      "        ...,\n",
      "        [0.3779, 0.4724, 0.4347,  ..., 0.5880, 0.5091, 0.5041],\n",
      "        [0.3680, 0.4995, 0.6208,  ..., 0.5887, 0.4516, 0.4611],\n",
      "        [0.3973, 0.5309, 0.4884,  ..., 0.5496, 0.4324, 0.5544]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 4, Loss: 1204.38427734375, Val Loss: 312.9378967285156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3886, 0.5744, 0.4597,  ..., 0.5426, 0.4317, 0.5725],\n",
      "        [0.4306, 0.5408, 0.4745,  ..., 0.5072, 0.4021, 0.5480],\n",
      "        [0.4706, 0.4512, 0.4825,  ..., 0.5984, 0.4303, 0.5117],\n",
      "        ...,\n",
      "        [0.4532, 0.5013, 0.5374,  ..., 0.6247, 0.4184, 0.5349],\n",
      "        [0.4239, 0.6385, 0.4420,  ..., 0.4157, 0.4241, 0.5761],\n",
      "        [0.4339, 0.5148, 0.4545,  ..., 0.5924, 0.4086, 0.5166]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 5, Loss: 1206.4217529296875, Val Loss: 312.8304138183594\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2819, 0.3782, 0.4390,  ..., 0.6702, 0.3701, 0.4494],\n",
      "        [0.4296, 0.5300, 0.4942,  ..., 0.5043, 0.3873, 0.5485],\n",
      "        [0.3497, 0.4413, 0.4487,  ..., 0.5912, 0.4186, 0.5161],\n",
      "        ...,\n",
      "        [0.4583, 0.4661, 0.5753,  ..., 0.5219, 0.4941, 0.4566],\n",
      "        [0.4241, 0.5100, 0.5428,  ..., 0.5509, 0.4711, 0.5066],\n",
      "        [0.5248, 0.5080, 0.4721,  ..., 0.5507, 0.4513, 0.4955]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 6, Loss: 1205.446533203125, Val Loss: 312.4711608886719\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3103, 0.5517, 0.4737,  ..., 0.6315, 0.4414, 0.5941],\n",
      "        [0.4697, 0.5156, 0.6131,  ..., 0.5415, 0.4705, 0.4935],\n",
      "        [0.4527, 0.4737, 0.5546,  ..., 0.4994, 0.4264, 0.5011],\n",
      "        ...,\n",
      "        [0.4581, 0.4855, 0.4761,  ..., 0.5661, 0.4567, 0.5362],\n",
      "        [0.4061, 0.4304, 0.4675,  ..., 0.5947, 0.4892, 0.5561],\n",
      "        [0.3283, 0.4525, 0.4606,  ..., 0.6912, 0.5234, 0.4579]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 7, Loss: 1205.3101806640625, Val Loss: 312.3532409667969\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3787, 0.5828, 0.5173,  ..., 0.5162, 0.4014, 0.5675],\n",
      "        [0.4596, 0.5067, 0.5313,  ..., 0.4799, 0.3832, 0.5018],\n",
      "        [0.3923, 0.6174, 0.5443,  ..., 0.4989, 0.4081, 0.5214],\n",
      "        ...,\n",
      "        [0.4680, 0.4506, 0.4600,  ..., 0.5236, 0.4506, 0.5185],\n",
      "        [0.4335, 0.4434, 0.4539,  ..., 0.6020, 0.4560, 0.6525],\n",
      "        [0.4057, 0.5530, 0.4670,  ..., 0.6586, 0.4517, 0.5509]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 8, Loss: 1204.42041015625, Val Loss: 312.49981689453125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3654, 0.4801, 0.4280,  ..., 0.6337, 0.4579, 0.5003],\n",
      "        [0.4107, 0.5459, 0.4735,  ..., 0.5484, 0.4491, 0.5269],\n",
      "        [0.3431, 0.4304, 0.5071,  ..., 0.5619, 0.3742, 0.4616],\n",
      "        ...,\n",
      "        [0.4674, 0.5085, 0.5475,  ..., 0.6084, 0.4864, 0.5172],\n",
      "        [0.4467, 0.4529, 0.5481,  ..., 0.6148, 0.4442, 0.4864],\n",
      "        [0.4786, 0.5084, 0.3962,  ..., 0.6259, 0.4115, 0.5440]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 9, Loss: 1204.3955078125, Val Loss: 312.6966857910156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4244, 0.5522, 0.5464,  ..., 0.4699, 0.5658, 0.5482],\n",
      "        [0.4766, 0.4776, 0.4679,  ..., 0.4814, 0.3832, 0.5794],\n",
      "        [0.3815, 0.4725, 0.5176,  ..., 0.5435, 0.5161, 0.4683],\n",
      "        ...,\n",
      "        [0.4604, 0.4906, 0.5367,  ..., 0.5697, 0.4663, 0.5219],\n",
      "        [0.5890, 0.4422, 0.5785,  ..., 0.5539, 0.4370, 0.5529],\n",
      "        [0.4615, 0.4677, 0.4417,  ..., 0.6506, 0.4056, 0.6137]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 10, Loss: 1206.336669921875, Val Loss: 312.8360900878906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4176, 0.5188, 0.5406,  ..., 0.4526, 0.4579, 0.5282],\n",
      "        [0.4768, 0.6118, 0.5333,  ..., 0.4548, 0.3969, 0.5198],\n",
      "        [0.3606, 0.5010, 0.5638,  ..., 0.5393, 0.3392, 0.4599],\n",
      "        ...,\n",
      "        [0.4820, 0.4704, 0.5229,  ..., 0.4974, 0.4810, 0.5289],\n",
      "        [0.5345, 0.6160, 0.4902,  ..., 0.4003, 0.3876, 0.5888],\n",
      "        [0.4738, 0.6193, 0.5267,  ..., 0.6124, 0.4209, 0.6668]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 11, Loss: 1204.769287109375, Val Loss: 312.8531799316406\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4231, 0.5306, 0.4972,  ..., 0.5793, 0.4087, 0.5068],\n",
      "        [0.4564, 0.4800, 0.4916,  ..., 0.5501, 0.4810, 0.5129],\n",
      "        [0.3816, 0.4723, 0.5410,  ..., 0.6336, 0.4920, 0.4969],\n",
      "        ...,\n",
      "        [0.4865, 0.4889, 0.5354,  ..., 0.5011, 0.5077, 0.4845],\n",
      "        [0.3409, 0.4936, 0.5120,  ..., 0.5791, 0.3990, 0.5332],\n",
      "        [0.3341, 0.4217, 0.3977,  ..., 0.6449, 0.4108, 0.4824]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 12, Loss: 1206.6060791015625, Val Loss: 312.77191162109375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3593, 0.5587, 0.5221,  ..., 0.6055, 0.4137, 0.5482],\n",
      "        [0.5298, 0.4972, 0.5804,  ..., 0.4784, 0.4487, 0.4823],\n",
      "        [0.4079, 0.6053, 0.4943,  ..., 0.4807, 0.3670, 0.6304],\n",
      "        ...,\n",
      "        [0.4684, 0.5352, 0.5187,  ..., 0.5840, 0.4855, 0.5137],\n",
      "        [0.4840, 0.5149, 0.4359,  ..., 0.4372, 0.4131, 0.6357],\n",
      "        [0.4162, 0.4880, 0.4356,  ..., 0.5892, 0.4376, 0.5136]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 13, Loss: 1205.6351318359375, Val Loss: 312.6517028808594\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3804, 0.7118, 0.4319,  ..., 0.5278, 0.4004, 0.5017],\n",
      "        [0.4217, 0.5640, 0.4483,  ..., 0.5183, 0.3992, 0.5960],\n",
      "        [0.2693, 0.4908, 0.4952,  ..., 0.6153, 0.4171, 0.5163],\n",
      "        ...,\n",
      "        [0.5060, 0.5004, 0.5114,  ..., 0.4910, 0.4206, 0.5571],\n",
      "        [0.4784, 0.5037, 0.4169,  ..., 0.4639, 0.3535, 0.6269],\n",
      "        [0.4020, 0.5374, 0.4570,  ..., 0.5590, 0.4708, 0.5631]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 14, Loss: 1204.6419677734375, Val Loss: 312.5001525878906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4120, 0.5430, 0.4710,  ..., 0.5158, 0.4802, 0.4919],\n",
      "        [0.4198, 0.5541, 0.5304,  ..., 0.5430, 0.4039, 0.4645],\n",
      "        [0.3892, 0.4749, 0.4837,  ..., 0.5977, 0.3751, 0.5638],\n",
      "        ...,\n",
      "        [0.4180, 0.4596, 0.5018,  ..., 0.5341, 0.4612, 0.4730],\n",
      "        [0.4304, 0.4239, 0.5276,  ..., 0.5244, 0.5223, 0.5524],\n",
      "        [0.3672, 0.5371, 0.4319,  ..., 0.5904, 0.4401, 0.5322]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 15, Loss: 1205.7491455078125, Val Loss: 312.3808898925781\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4495, 0.5762, 0.4778,  ..., 0.5135, 0.4321, 0.5419],\n",
      "        [0.3984, 0.5423, 0.4306,  ..., 0.5946, 0.5011, 0.4570],\n",
      "        [0.4236, 0.4969, 0.4800,  ..., 0.5338, 0.4408, 0.4670],\n",
      "        ...,\n",
      "        [0.4804, 0.4910, 0.5186,  ..., 0.5236, 0.4438, 0.5473],\n",
      "        [0.5330, 0.5781, 0.4826,  ..., 0.5565, 0.4811, 0.5687],\n",
      "        [0.4827, 0.4042, 0.5643,  ..., 0.5472, 0.4716, 0.5187]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 16, Loss: 1205.135009765625, Val Loss: 312.3298034667969\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4565, 0.5650, 0.4911,  ..., 0.4810, 0.3892, 0.5778],\n",
      "        [0.4557, 0.5026, 0.5451,  ..., 0.5175, 0.4501, 0.5235],\n",
      "        [0.3808, 0.5253, 0.4907,  ..., 0.4979, 0.4024, 0.5257],\n",
      "        ...,\n",
      "        [0.4481, 0.4249, 0.5058,  ..., 0.5377, 0.4571, 0.5121],\n",
      "        [0.3871, 0.4554, 0.4583,  ..., 0.6125, 0.5428, 0.4495],\n",
      "        [0.4776, 0.5181, 0.5000,  ..., 0.5780, 0.4947, 0.5402]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 17, Loss: 1204.6552734375, Val Loss: 312.31939697265625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4010, 0.5366, 0.4708,  ..., 0.5427, 0.4652, 0.6209],\n",
      "        [0.4093, 0.5321, 0.5012,  ..., 0.5184, 0.3725, 0.5175],\n",
      "        [0.4043, 0.5034, 0.4840,  ..., 0.5879, 0.4463, 0.5285],\n",
      "        ...,\n",
      "        [0.5029, 0.5301, 0.4925,  ..., 0.5566, 0.4384, 0.5513],\n",
      "        [0.5095, 0.3751, 0.4659,  ..., 0.5141, 0.3666, 0.6332],\n",
      "        [0.4862, 0.5971, 0.4997,  ..., 0.5415, 0.4896, 0.5841]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 18, Loss: 1205.8240966796875, Val Loss: 312.3497314453125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3514, 0.3947, 0.5325,  ..., 0.6154, 0.4794, 0.5419],\n",
      "        [0.4296, 0.4700, 0.5545,  ..., 0.6364, 0.4436, 0.5796],\n",
      "        [0.3744, 0.4928, 0.4563,  ..., 0.5013, 0.4523, 0.5176],\n",
      "        ...,\n",
      "        [0.4738, 0.4984, 0.4917,  ..., 0.4871, 0.4233, 0.4986],\n",
      "        [0.4156, 0.5482, 0.4848,  ..., 0.5364, 0.4448, 0.4979],\n",
      "        [0.4906, 0.5111, 0.4738,  ..., 0.5280, 0.4614, 0.5462]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 19, Loss: 1205.555908203125, Val Loss: 312.34869384765625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3762, 0.5651, 0.4309,  ..., 0.5429, 0.4203, 0.5951],\n",
      "        [0.4453, 0.5462, 0.4748,  ..., 0.4807, 0.4095, 0.5766],\n",
      "        [0.3319, 0.5290, 0.5302,  ..., 0.4377, 0.3988, 0.6193],\n",
      "        ...,\n",
      "        [0.4177, 0.4868, 0.5112,  ..., 0.5676, 0.4386, 0.4911],\n",
      "        [0.4826, 0.5857, 0.3943,  ..., 0.4896, 0.3998, 0.6847],\n",
      "        [0.4469, 0.4846, 0.4775,  ..., 0.6498, 0.4299, 0.5805]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 20, Loss: 1206.1138916015625, Val Loss: 312.3252868652344\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3429, 0.4332, 0.4517,  ..., 0.6467, 0.4605, 0.5145],\n",
      "        [0.5071, 0.4438, 0.4992,  ..., 0.5230, 0.4481, 0.5326],\n",
      "        [0.3546, 0.5081, 0.4818,  ..., 0.5335, 0.4047, 0.4827],\n",
      "        ...,\n",
      "        [0.4491, 0.4780, 0.5614,  ..., 0.4872, 0.4710, 0.5332],\n",
      "        [0.3255, 0.5366, 0.4561,  ..., 0.5827, 0.4383, 0.5060],\n",
      "        [0.4935, 0.4618, 0.4766,  ..., 0.5776, 0.4485, 0.5579]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 21, Loss: 1205.84375, Val Loss: 312.2936096191406\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4423, 0.6243, 0.4622,  ..., 0.4556, 0.4098, 0.5569],\n",
      "        [0.4590, 0.5785, 0.4901,  ..., 0.7153, 0.4476, 0.5298],\n",
      "        [0.3663, 0.5630, 0.4885,  ..., 0.5682, 0.3859, 0.5441],\n",
      "        ...,\n",
      "        [0.4903, 0.5645, 0.5109,  ..., 0.4672, 0.4069, 0.5553],\n",
      "        [0.3698, 0.5493, 0.3904,  ..., 0.4495, 0.3540, 0.6419],\n",
      "        [0.4934, 0.5124, 0.4848,  ..., 0.5618, 0.4450, 0.5479]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 22, Loss: 1203.1102294921875, Val Loss: 312.2989807128906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3458, 0.6244, 0.4229,  ..., 0.4809, 0.4753, 0.5322],\n",
      "        [0.4130, 0.5157, 0.6161,  ..., 0.5922, 0.4236, 0.4661],\n",
      "        [0.3448, 0.5394, 0.4755,  ..., 0.4113, 0.3694, 0.5304],\n",
      "        ...,\n",
      "        [0.5022, 0.5008, 0.5251,  ..., 0.5600, 0.4205, 0.5587],\n",
      "        [0.2440, 0.3972, 0.3372,  ..., 0.6609, 0.3566, 0.6701],\n",
      "        [0.3829, 0.5184, 0.4584,  ..., 0.5565, 0.4512, 0.4959]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 23, Loss: 1204.5047607421875, Val Loss: 312.3133544921875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2898, 0.4523, 0.4996,  ..., 0.6782, 0.4673, 0.6125],\n",
      "        [0.4991, 0.4655, 0.5775,  ..., 0.5615, 0.4174, 0.5655],\n",
      "        [0.2326, 0.5085, 0.5048,  ..., 0.5682, 0.3978, 0.5462],\n",
      "        ...,\n",
      "        [0.4131, 0.4555, 0.4904,  ..., 0.5842, 0.4903, 0.4965],\n",
      "        [0.4711, 0.5412, 0.4557,  ..., 0.5683, 0.4115, 0.6193],\n",
      "        [0.5057, 0.5246, 0.4739,  ..., 0.5379, 0.4392, 0.5968]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 24, Loss: 1205.7830810546875, Val Loss: 312.3590087890625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3858, 0.5889, 0.4807,  ..., 0.4425, 0.4229, 0.5275],\n",
      "        [0.4105, 0.5033, 0.4247,  ..., 0.5363, 0.3584, 0.6750],\n",
      "        [0.4136, 0.5441, 0.4339,  ..., 0.5089, 0.4152, 0.4680],\n",
      "        ...,\n",
      "        [0.3174, 0.5416, 0.5099,  ..., 0.5594, 0.4621, 0.4607],\n",
      "        [0.4721, 0.5178, 0.5202,  ..., 0.4850, 0.4150, 0.5574],\n",
      "        [0.4442, 0.4798, 0.4669,  ..., 0.4773, 0.4011, 0.5639]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 25, Loss: 1204.6103515625, Val Loss: 312.39678955078125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4339, 0.4844, 0.5621,  ..., 0.5697, 0.4481, 0.4946],\n",
      "        [0.5001, 0.5080, 0.5827,  ..., 0.5182, 0.3773, 0.5842],\n",
      "        [0.3604, 0.5592, 0.5860,  ..., 0.4284, 0.4201, 0.5221],\n",
      "        ...,\n",
      "        [0.4179, 0.4915, 0.5127,  ..., 0.4814, 0.4631, 0.5214],\n",
      "        [0.4597, 0.5359, 0.4068,  ..., 0.4605, 0.4461, 0.5452],\n",
      "        [0.4207, 0.5232, 0.4933,  ..., 0.6331, 0.4597, 0.5863]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 26, Loss: 1205.0020751953125, Val Loss: 312.4136962890625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4006, 0.5602, 0.4204,  ..., 0.5436, 0.4010, 0.5822],\n",
      "        [0.4846, 0.5616, 0.4744,  ..., 0.5120, 0.3697, 0.5778],\n",
      "        [0.4595, 0.5474, 0.5883,  ..., 0.3636, 0.4364, 0.4987],\n",
      "        ...,\n",
      "        [0.4264, 0.4745, 0.4465,  ..., 0.5993, 0.4420, 0.5068],\n",
      "        [0.4522, 0.5321, 0.5739,  ..., 0.5658, 0.4836, 0.5284],\n",
      "        [0.4167, 0.5352, 0.5156,  ..., 0.5746, 0.5151, 0.5712]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 27, Loss: 1204.133056640625, Val Loss: 312.3959045410156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4076, 0.5285, 0.5032,  ..., 0.5414, 0.4839, 0.4779],\n",
      "        [0.5342, 0.5227, 0.5553,  ..., 0.5271, 0.4895, 0.5434],\n",
      "        [0.4681, 0.4505, 0.5324,  ..., 0.5192, 0.4268, 0.4213],\n",
      "        ...,\n",
      "        [0.3686, 0.4509, 0.4324,  ..., 0.6760, 0.4051, 0.5048],\n",
      "        [0.5409, 0.4795, 0.5977,  ..., 0.5933, 0.4423, 0.5920],\n",
      "        [0.4931, 0.5072, 0.5485,  ..., 0.5062, 0.4630, 0.4924]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 28, Loss: 1203.8734130859375, Val Loss: 312.3614196777344\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4865, 0.5536, 0.5175,  ..., 0.5185, 0.4457, 0.5602],\n",
      "        [0.4135, 0.5686, 0.4971,  ..., 0.6050, 0.4711, 0.5385],\n",
      "        [0.3387, 0.5852, 0.5561,  ..., 0.4962, 0.3915, 0.5623],\n",
      "        ...,\n",
      "        [0.4529, 0.5423, 0.5210,  ..., 0.5668, 0.3743, 0.5901],\n",
      "        [0.5233, 0.5310, 0.4989,  ..., 0.5769, 0.3826, 0.6427],\n",
      "        [0.3864, 0.4556, 0.4541,  ..., 0.6448, 0.4938, 0.4952]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 29, Loss: 1205.466552734375, Val Loss: 312.323974609375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3134, 0.4968, 0.3866,  ..., 0.5575, 0.4025, 0.5400],\n",
      "        [0.4026, 0.5025, 0.5013,  ..., 0.5399, 0.3702, 0.4987],\n",
      "        [0.3179, 0.5438, 0.4915,  ..., 0.4777, 0.4006, 0.6087],\n",
      "        ...,\n",
      "        [0.4410, 0.4368, 0.4971,  ..., 0.6280, 0.4639, 0.4505],\n",
      "        [0.4747, 0.5011, 0.5012,  ..., 0.4334, 0.4790, 0.5251],\n",
      "        [0.5039, 0.4902, 0.4660,  ..., 0.5651, 0.4446, 0.6296]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 30, Loss: 1203.825439453125, Val Loss: 312.2879943847656\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4553, 0.5912, 0.4884,  ..., 0.5256, 0.4101, 0.5682],\n",
      "        [0.5114, 0.4626, 0.6327,  ..., 0.5872, 0.4534, 0.4863],\n",
      "        [0.3907, 0.5553, 0.5295,  ..., 0.5228, 0.4354, 0.4885],\n",
      "        ...,\n",
      "        [0.4672, 0.5277, 0.5042,  ..., 0.5392, 0.4104, 0.5967],\n",
      "        [0.4647, 0.4848, 0.5211,  ..., 0.5110, 0.4356, 0.5317],\n",
      "        [0.3796, 0.4393, 0.4178,  ..., 0.6209, 0.4249, 0.5637]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 31, Loss: 1204.101806640625, Val Loss: 312.2743835449219\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3260, 0.4818, 0.4665,  ..., 0.5718, 0.3900, 0.5348],\n",
      "        [0.5172, 0.6246, 0.4818,  ..., 0.5124, 0.4173, 0.5560],\n",
      "        [0.4444, 0.5727, 0.5308,  ..., 0.5145, 0.3553, 0.5038],\n",
      "        ...,\n",
      "        [0.5060, 0.4705, 0.5940,  ..., 0.5099, 0.4415, 0.4850],\n",
      "        [0.4450, 0.4709, 0.6248,  ..., 0.6326, 0.5347, 0.4740],\n",
      "        [0.2889, 0.4591, 0.4832,  ..., 0.6417, 0.5079, 0.4297]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 32, Loss: 1205.5631103515625, Val Loss: 312.275390625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4385, 0.5654, 0.4726,  ..., 0.4690, 0.4993, 0.5911],\n",
      "        [0.3522, 0.6202, 0.5044,  ..., 0.5351, 0.3473, 0.4945],\n",
      "        [0.4056, 0.5242, 0.4782,  ..., 0.4601, 0.4216, 0.5452],\n",
      "        ...,\n",
      "        [0.4860, 0.4791, 0.5227,  ..., 0.5569, 0.4794, 0.5420],\n",
      "        [0.4425, 0.5186, 0.5800,  ..., 0.5628, 0.4418, 0.5421],\n",
      "        [0.4037, 0.5338, 0.4656,  ..., 0.5714, 0.4622, 0.4970]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 33, Loss: 1202.897705078125, Val Loss: 312.28240966796875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3508, 0.4650, 0.4197,  ..., 0.6264, 0.3868, 0.5850],\n",
      "        [0.4507, 0.5896, 0.4820,  ..., 0.5228, 0.4126, 0.5842],\n",
      "        [0.4332, 0.6030, 0.5250,  ..., 0.4981, 0.4150, 0.5661],\n",
      "        ...,\n",
      "        [0.4664, 0.4877, 0.4936,  ..., 0.5186, 0.4620, 0.5233],\n",
      "        [0.4051, 0.4827, 0.5075,  ..., 0.5585, 0.5145, 0.5560],\n",
      "        [0.5567, 0.5740, 0.4400,  ..., 0.5150, 0.4107, 0.6818]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 34, Loss: 1204.22412109375, Val Loss: 312.27264404296875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4642, 0.4953, 0.5224,  ..., 0.5362, 0.4828, 0.4725],\n",
      "        [0.5013, 0.4750, 0.5413,  ..., 0.5498, 0.4378, 0.6097],\n",
      "        [0.3658, 0.4692, 0.5584,  ..., 0.6163, 0.3996, 0.3931],\n",
      "        ...,\n",
      "        [0.4656, 0.4836, 0.5523,  ..., 0.5459, 0.4128, 0.4505],\n",
      "        [0.4114, 0.4732, 0.4916,  ..., 0.5817, 0.4501, 0.4968],\n",
      "        [0.3984, 0.5936, 0.4347,  ..., 0.5238, 0.3807, 0.5632]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 35, Loss: 1204.3819580078125, Val Loss: 312.2739562988281\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2698, 0.4136, 0.4517,  ..., 0.5870, 0.4218, 0.5390],\n",
      "        [0.3828, 0.6240, 0.6421,  ..., 0.6231, 0.4155, 0.4841],\n",
      "        [0.4005, 0.5465, 0.4156,  ..., 0.5239, 0.3555, 0.4998],\n",
      "        ...,\n",
      "        [0.4883, 0.5023, 0.5281,  ..., 0.5498, 0.4676, 0.5159],\n",
      "        [0.5805, 0.5025, 0.4854,  ..., 0.5088, 0.4062, 0.6443],\n",
      "        [0.5070, 0.5577, 0.4947,  ..., 0.6105, 0.5038, 0.5567]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 36, Loss: 1205.4017333984375, Val Loss: 312.275390625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4480, 0.6022, 0.4732,  ..., 0.4955, 0.5047, 0.5407],\n",
      "        [0.3203, 0.6752, 0.4588,  ..., 0.4977, 0.3253, 0.5445],\n",
      "        [0.4153, 0.5267, 0.5432,  ..., 0.5187, 0.4316, 0.4667],\n",
      "        ...,\n",
      "        [0.4524, 0.5572, 0.5156,  ..., 0.5658, 0.4362, 0.5908],\n",
      "        [0.4929, 0.4341, 0.4649,  ..., 0.5743, 0.3839, 0.6186],\n",
      "        [0.5442, 0.5143, 0.5454,  ..., 0.4528, 0.4705, 0.6128]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 37, Loss: 1204.2767333984375, Val Loss: 312.2863464355469\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3700, 0.4851, 0.4999,  ..., 0.5000, 0.4470, 0.4982],\n",
      "        [0.4759, 0.5133, 0.4757,  ..., 0.5266, 0.4119, 0.5506],\n",
      "        [0.3294, 0.4118, 0.5075,  ..., 0.5846, 0.4615, 0.5243],\n",
      "        ...,\n",
      "        [0.4398, 0.5022, 0.4526,  ..., 0.5235, 0.4155, 0.4975],\n",
      "        [0.4880, 0.5855, 0.5266,  ..., 0.5446, 0.4591, 0.6358],\n",
      "        [0.3817, 0.5225, 0.4081,  ..., 0.6400, 0.4592, 0.5069]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 38, Loss: 1204.1336669921875, Val Loss: 312.2902526855469\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2729, 0.4070, 0.4377,  ..., 0.6493, 0.4241, 0.5974],\n",
      "        [0.3339, 0.5576, 0.4610,  ..., 0.5142, 0.3821, 0.4581],\n",
      "        [0.2972, 0.4961, 0.4411,  ..., 0.5103, 0.3697, 0.5372],\n",
      "        ...,\n",
      "        [0.5252, 0.5534, 0.5030,  ..., 0.4881, 0.4035, 0.5009],\n",
      "        [0.3589, 0.4138, 0.5067,  ..., 0.6383, 0.4908, 0.5324],\n",
      "        [0.4077, 0.5350, 0.4805,  ..., 0.5997, 0.5144, 0.4987]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 39, Loss: 1205.1566162109375, Val Loss: 312.3068542480469\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3846, 0.4914, 0.5115,  ..., 0.6276, 0.3821, 0.5004],\n",
      "        [0.3863, 0.6608, 0.5306,  ..., 0.5332, 0.3547, 0.4951],\n",
      "        [0.3550, 0.5011, 0.5038,  ..., 0.4123, 0.3745, 0.4749],\n",
      "        ...,\n",
      "        [0.4858, 0.5323, 0.5659,  ..., 0.5045, 0.4895, 0.4854],\n",
      "        [0.3714, 0.6249, 0.3596,  ..., 0.5445, 0.3733, 0.6525],\n",
      "        [0.4866, 0.5713, 0.4657,  ..., 0.5927, 0.3944, 0.6648]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 40, Loss: 1204.3848876953125, Val Loss: 312.3146057128906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4284, 0.6225, 0.4708,  ..., 0.4928, 0.4530, 0.5300],\n",
      "        [0.3776, 0.5013, 0.5294,  ..., 0.6230, 0.4483, 0.4267],\n",
      "        [0.4502, 0.5837, 0.5158,  ..., 0.4773, 0.3574, 0.5016],\n",
      "        ...,\n",
      "        [0.4549, 0.4844, 0.5306,  ..., 0.5385, 0.4597, 0.4878],\n",
      "        [0.3878, 0.4194, 0.5700,  ..., 0.6437, 0.3896, 0.5524],\n",
      "        [0.4303, 0.4880, 0.4744,  ..., 0.5714, 0.4232, 0.6097]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 41, Loss: 1204.4755859375, Val Loss: 312.3087158203125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4773, 0.5085, 0.4765,  ..., 0.5980, 0.4888, 0.4926],\n",
      "        [0.5299, 0.5391, 0.5908,  ..., 0.6024, 0.3822, 0.6035],\n",
      "        [0.3156, 0.4752, 0.4359,  ..., 0.5863, 0.4687, 0.4641],\n",
      "        ...,\n",
      "        [0.4798, 0.5047, 0.4812,  ..., 0.4886, 0.4351, 0.5441],\n",
      "        [0.5200, 0.4773, 0.5126,  ..., 0.5824, 0.4301, 0.6578],\n",
      "        [0.4775, 0.4781, 0.4666,  ..., 0.5637, 0.5163, 0.5165]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 42, Loss: 1203.9150390625, Val Loss: 312.2999267578125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3319, 0.4824, 0.3975,  ..., 0.5455, 0.3971, 0.5138],\n",
      "        [0.5712, 0.5466, 0.5118,  ..., 0.5149, 0.4618, 0.5755],\n",
      "        [0.4297, 0.5364, 0.5386,  ..., 0.4387, 0.3172, 0.4633],\n",
      "        ...,\n",
      "        [0.4579, 0.5772, 0.5230,  ..., 0.5917, 0.4707, 0.5453],\n",
      "        [0.3905, 0.4149, 0.4924,  ..., 0.6685, 0.4532, 0.5525],\n",
      "        [0.5000, 0.4871, 0.4604,  ..., 0.5405, 0.4086, 0.5337]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 43, Loss: 1204.9659423828125, Val Loss: 312.2837219238281\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2487, 0.3939, 0.3309,  ..., 0.6146, 0.3533, 0.4643],\n",
      "        [0.4694, 0.5971, 0.4878,  ..., 0.5488, 0.4111, 0.5298],\n",
      "        [0.3610, 0.5341, 0.4538,  ..., 0.5080, 0.4172, 0.4415],\n",
      "        ...,\n",
      "        [0.3444, 0.5304, 0.4691,  ..., 0.5772, 0.4561, 0.5079],\n",
      "        [0.4359, 0.4897, 0.5081,  ..., 0.6099, 0.5099, 0.5125],\n",
      "        [0.4067, 0.5860, 0.4903,  ..., 0.5760, 0.4789, 0.5298]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 44, Loss: 1203.5115966796875, Val Loss: 312.2740173339844\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4617, 0.5797, 0.5042,  ..., 0.5346, 0.4082, 0.5199],\n",
      "        [0.3878, 0.4341, 0.4250,  ..., 0.5499, 0.3700, 0.5528],\n",
      "        [0.3941, 0.5853, 0.3909,  ..., 0.5237, 0.3433, 0.4892],\n",
      "        ...,\n",
      "        [0.4915, 0.5029, 0.5026,  ..., 0.4716, 0.4718, 0.5432],\n",
      "        [0.4364, 0.4783, 0.4920,  ..., 0.4914, 0.4208, 0.5515],\n",
      "        [0.4938, 0.5032, 0.5402,  ..., 0.5557, 0.4712, 0.5827]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 45, Loss: 1204.2235107421875, Val Loss: 312.2660217285156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4297, 0.4445, 0.4221,  ..., 0.5880, 0.4120, 0.3749],\n",
      "        [0.4558, 0.4931, 0.5394,  ..., 0.5481, 0.4087, 0.5533],\n",
      "        [0.2928, 0.4690, 0.4571,  ..., 0.5751, 0.3675, 0.4361],\n",
      "        ...,\n",
      "        [0.4363, 0.5851, 0.5026,  ..., 0.5178, 0.4413, 0.5101],\n",
      "        [0.3809, 0.4173, 0.4539,  ..., 0.5531, 0.4750, 0.5294],\n",
      "        [0.3860, 0.5392, 0.4983,  ..., 0.5534, 0.4839, 0.5190]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 46, Loss: 1205.6180419921875, Val Loss: 312.2685241699219\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3473, 0.4113, 0.4143,  ..., 0.6173, 0.4485, 0.4991],\n",
      "        [0.4195, 0.4657, 0.4344,  ..., 0.5135, 0.3334, 0.5651],\n",
      "        [0.4231, 0.3875, 0.5004,  ..., 0.5823, 0.4683, 0.4861],\n",
      "        ...,\n",
      "        [0.3355, 0.5358, 0.5150,  ..., 0.6272, 0.4634, 0.4556],\n",
      "        [0.3463, 0.3621, 0.4228,  ..., 0.6635, 0.4202, 0.5420],\n",
      "        [0.4042, 0.5294, 0.4803,  ..., 0.6877, 0.4648, 0.6282]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 47, Loss: 1204.17919921875, Val Loss: 312.26788330078125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4435, 0.5018, 0.4332,  ..., 0.6523, 0.3758, 0.6334],\n",
      "        [0.4907, 0.4495, 0.5084,  ..., 0.5648, 0.5141, 0.5236],\n",
      "        [0.3642, 0.5819, 0.4659,  ..., 0.4384, 0.4373, 0.5681],\n",
      "        ...,\n",
      "        [0.4885, 0.5560, 0.5624,  ..., 0.5435, 0.4350, 0.4840],\n",
      "        [0.4575, 0.5311, 0.5644,  ..., 0.4753, 0.4598, 0.4895],\n",
      "        [0.3657, 0.4781, 0.4524,  ..., 0.5971, 0.4442, 0.4971]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 48, Loss: 1205.0577392578125, Val Loss: 312.2680969238281\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4595, 0.5461, 0.6148,  ..., 0.4841, 0.5730, 0.5323],\n",
      "        [0.4885, 0.5428, 0.4530,  ..., 0.5015, 0.4041, 0.5193],\n",
      "        [0.4465, 0.5028, 0.5198,  ..., 0.4914, 0.4136, 0.4390],\n",
      "        ...,\n",
      "        [0.4412, 0.4987, 0.5639,  ..., 0.5911, 0.4451, 0.5038],\n",
      "        [0.4289, 0.5897, 0.4353,  ..., 0.4520, 0.3676, 0.5508],\n",
      "        [0.4310, 0.5406, 0.4723,  ..., 0.7135, 0.4784, 0.6056]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 49, Loss: 1205.4664306640625, Val Loss: 312.2699890136719\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4949, 0.6418, 0.4784,  ..., 0.5258, 0.4001, 0.5971],\n",
      "        [0.4933, 0.5630, 0.4569,  ..., 0.4705, 0.3970, 0.5823],\n",
      "        [0.2816, 0.4417, 0.3690,  ..., 0.5945, 0.3470, 0.5110],\n",
      "        ...,\n",
      "        [0.4652, 0.5386, 0.4861,  ..., 0.4830, 0.4430, 0.5746],\n",
      "        [0.5126, 0.5716, 0.5119,  ..., 0.5530, 0.4592, 0.6346],\n",
      "        [0.4013, 0.4216, 0.4765,  ..., 0.5925, 0.4240, 0.5342]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 50, Loss: 1203.9808349609375, Val Loss: 312.2689208984375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4087, 0.5470, 0.4964,  ..., 0.5576, 0.4565, 0.5551],\n",
      "        [0.3583, 0.5624, 0.4390,  ..., 0.5189, 0.3352, 0.4779],\n",
      "        [0.4026, 0.4925, 0.4700,  ..., 0.4271, 0.3730, 0.5110],\n",
      "        ...,\n",
      "        [0.4324, 0.4661, 0.5123,  ..., 0.5812, 0.4830, 0.4989],\n",
      "        [0.4768, 0.4322, 0.5307,  ..., 0.5950, 0.5589, 0.5083],\n",
      "        [0.3877, 0.4627, 0.5051,  ..., 0.6339, 0.4941, 0.5314]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 51, Loss: 1206.3653564453125, Val Loss: 312.27581787109375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3446, 0.5423, 0.4880,  ..., 0.5137, 0.4459, 0.5577],\n",
      "        [0.3382, 0.5731, 0.5827,  ..., 0.5854, 0.4085, 0.4552],\n",
      "        [0.3371, 0.5281, 0.5820,  ..., 0.4711, 0.3975, 0.4919],\n",
      "        ...,\n",
      "        [0.4239, 0.4896, 0.4750,  ..., 0.5315, 0.4919, 0.4929],\n",
      "        [0.3352, 0.5519, 0.5337,  ..., 0.6135, 0.5081, 0.4745],\n",
      "        [0.5272, 0.5104, 0.5500,  ..., 0.5737, 0.4311, 0.6398]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 52, Loss: 1205.210205078125, Val Loss: 312.28302001953125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4949, 0.6326, 0.4759,  ..., 0.5312, 0.4329, 0.5569],\n",
      "        [0.4368, 0.5338, 0.4924,  ..., 0.5739, 0.4159, 0.4777],\n",
      "        [0.3579, 0.5651, 0.5909,  ..., 0.5315, 0.4562, 0.5353],\n",
      "        ...,\n",
      "        [0.4689, 0.4922, 0.5652,  ..., 0.5072, 0.4116, 0.5761],\n",
      "        [0.4037, 0.3822, 0.4804,  ..., 0.5803, 0.4063, 0.5952],\n",
      "        [0.3559, 0.4766, 0.5287,  ..., 0.5511, 0.4445, 0.5185]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 53, Loss: 1203.775634765625, Val Loss: 312.2884826660156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4433, 0.5392, 0.5037,  ..., 0.6201, 0.5016, 0.5974],\n",
      "        [0.4086, 0.4573, 0.4451,  ..., 0.5142, 0.3938, 0.5391],\n",
      "        [0.4339, 0.5191, 0.5160,  ..., 0.5111, 0.4331, 0.4761],\n",
      "        ...,\n",
      "        [0.4746, 0.5199, 0.5139,  ..., 0.5429, 0.4666, 0.5205],\n",
      "        [0.3740, 0.6224, 0.4111,  ..., 0.4129, 0.3274, 0.5851],\n",
      "        [0.4083, 0.5421, 0.4835,  ..., 0.5227, 0.3854, 0.5246]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 54, Loss: 1204.4552001953125, Val Loss: 312.2848205566406\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4293, 0.5822, 0.4956,  ..., 0.5274, 0.4732, 0.5258],\n",
      "        [0.3439, 0.5187, 0.4577,  ..., 0.4866, 0.3787, 0.4446],\n",
      "        [0.4621, 0.5107, 0.4824,  ..., 0.4365, 0.3580, 0.4959],\n",
      "        ...,\n",
      "        [0.4434, 0.4852, 0.4806,  ..., 0.5135, 0.4623, 0.4960],\n",
      "        [0.3859, 0.4780, 0.4664,  ..., 0.5351, 0.4068, 0.5652],\n",
      "        [0.5023, 0.4403, 0.5060,  ..., 0.5866, 0.4808, 0.5216]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 55, Loss: 1204.6724853515625, Val Loss: 312.28363037109375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4130, 0.5817, 0.4949,  ..., 0.4844, 0.5272, 0.5094],\n",
      "        [0.4005, 0.5694, 0.4394,  ..., 0.5016, 0.3760, 0.5600],\n",
      "        [0.3643, 0.6200, 0.4749,  ..., 0.4592, 0.4192, 0.5791],\n",
      "        ...,\n",
      "        [0.4533, 0.4826, 0.4700,  ..., 0.6211, 0.4739, 0.5611],\n",
      "        [0.3584, 0.5561, 0.4169,  ..., 0.5799, 0.4058, 0.5169],\n",
      "        [0.4220, 0.4604, 0.5224,  ..., 0.5741, 0.4957, 0.5026]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 56, Loss: 1205.7049560546875, Val Loss: 312.2747802734375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4286, 0.4041, 0.5305,  ..., 0.6124, 0.4525, 0.4800],\n",
      "        [0.3385, 0.4492, 0.4970,  ..., 0.5610, 0.4024, 0.4437],\n",
      "        [0.4550, 0.4772, 0.5521,  ..., 0.4799, 0.4388, 0.4496],\n",
      "        ...,\n",
      "        [0.4435, 0.4459, 0.5456,  ..., 0.5145, 0.4887, 0.4521],\n",
      "        [0.3329, 0.2953, 0.4621,  ..., 0.6487, 0.3744, 0.4902],\n",
      "        [0.3667, 0.4229, 0.4134,  ..., 0.6018, 0.4519, 0.5197]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 57, Loss: 1204.29150390625, Val Loss: 312.2714538574219\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4463, 0.5068, 0.5382,  ..., 0.5452, 0.4594, 0.5028],\n",
      "        [0.4450, 0.4325, 0.5544,  ..., 0.5590, 0.4097, 0.5094],\n",
      "        [0.4195, 0.5041, 0.5133,  ..., 0.4787, 0.4245, 0.5211],\n",
      "        ...,\n",
      "        [0.3790, 0.4538, 0.4065,  ..., 0.5415, 0.4375, 0.4975],\n",
      "        [0.5352, 0.4840, 0.5280,  ..., 0.5747, 0.4604, 0.6297],\n",
      "        [0.5198, 0.5056, 0.5058,  ..., 0.4923, 0.4655, 0.4827]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 58, Loss: 1204.8916015625, Val Loss: 312.2701721191406\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3330, 0.5771, 0.4343,  ..., 0.5271, 0.4251, 0.5896],\n",
      "        [0.4243, 0.5706, 0.5403,  ..., 0.5716, 0.4142, 0.5333],\n",
      "        [0.2848, 0.5055, 0.5236,  ..., 0.4469, 0.3258, 0.6065],\n",
      "        ...,\n",
      "        [0.4546, 0.5285, 0.5521,  ..., 0.4479, 0.4061, 0.4814],\n",
      "        [0.4591, 0.4858, 0.5216,  ..., 0.5123, 0.4863, 0.5538],\n",
      "        [0.4228, 0.4232, 0.4290,  ..., 0.5271, 0.4249, 0.5253]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 59, Loss: 1204.822998046875, Val Loss: 312.2667541503906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4743, 0.5895, 0.4946,  ..., 0.5271, 0.4515, 0.5729],\n",
      "        [0.4456, 0.5239, 0.5841,  ..., 0.4014, 0.4318, 0.4762],\n",
      "        [0.3961, 0.5612, 0.5123,  ..., 0.4766, 0.4452, 0.4590],\n",
      "        ...,\n",
      "        [0.4866, 0.6443, 0.4735,  ..., 0.4432, 0.3539, 0.5221],\n",
      "        [0.4318, 0.5118, 0.5358,  ..., 0.4651, 0.4291, 0.5646],\n",
      "        [0.5051, 0.6499, 0.4564,  ..., 0.5619, 0.4214, 0.7225]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 60, Loss: 1204.65283203125, Val Loss: 312.26806640625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4015, 0.4696, 0.5335,  ..., 0.5551, 0.4625, 0.5043],\n",
      "        [0.4419, 0.4777, 0.4418,  ..., 0.6520, 0.4993, 0.5618],\n",
      "        [0.3502, 0.4698, 0.5133,  ..., 0.5840, 0.3919, 0.4559],\n",
      "        ...,\n",
      "        [0.4826, 0.5060, 0.5379,  ..., 0.4337, 0.4798, 0.4862],\n",
      "        [0.4613, 0.4586, 0.4056,  ..., 0.4530, 0.3410, 0.6143],\n",
      "        [0.4186, 0.4993, 0.5235,  ..., 0.5460, 0.4529, 0.5193]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 61, Loss: 1203.500732421875, Val Loss: 312.2690124511719\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4220, 0.4943, 0.4854,  ..., 0.6499, 0.4744, 0.4942],\n",
      "        [0.4259, 0.5854, 0.5310,  ..., 0.5565, 0.4529, 0.5747],\n",
      "        [0.4190, 0.5844, 0.5031,  ..., 0.5771, 0.3383, 0.6378],\n",
      "        ...,\n",
      "        [0.5138, 0.5308, 0.4941,  ..., 0.4629, 0.4037, 0.5702],\n",
      "        [0.4242, 0.4368, 0.4821,  ..., 0.5238, 0.4160, 0.5278],\n",
      "        [0.3232, 0.4205, 0.4073,  ..., 0.6280, 0.4431, 0.5050]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 62, Loss: 1204.47412109375, Val Loss: 312.27044677734375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2685, 0.5150, 0.3194,  ..., 0.5106, 0.3609, 0.4585],\n",
      "        [0.4297, 0.6540, 0.5011,  ..., 0.5283, 0.3922, 0.5071],\n",
      "        [0.3377, 0.5310, 0.5415,  ..., 0.4584, 0.3612, 0.5575],\n",
      "        ...,\n",
      "        [0.3985, 0.4912, 0.4660,  ..., 0.5183, 0.3978, 0.4965],\n",
      "        [0.4412, 0.4832, 0.4809,  ..., 0.5142, 0.3981, 0.5762],\n",
      "        [0.5202, 0.4851, 0.4905,  ..., 0.5072, 0.4030, 0.6485]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 63, Loss: 1203.7515869140625, Val Loss: 312.2696838378906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4813, 0.4921, 0.4911,  ..., 0.5080, 0.4690, 0.5380],\n",
      "        [0.4529, 0.5310, 0.5219,  ..., 0.4196, 0.4509, 0.5200],\n",
      "        [0.4328, 0.5409, 0.5007,  ..., 0.4955, 0.4075, 0.5329],\n",
      "        ...,\n",
      "        [0.4311, 0.5271, 0.5488,  ..., 0.5572, 0.4558, 0.4647],\n",
      "        [0.5039, 0.5264, 0.5563,  ..., 0.5961, 0.4653, 0.5432],\n",
      "        [0.4464, 0.4330, 0.4725,  ..., 0.5504, 0.4604, 0.5736]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 64, Loss: 1203.8472900390625, Val Loss: 312.27484130859375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4314, 0.4817, 0.4963,  ..., 0.5933, 0.5603, 0.5699],\n",
      "        [0.4084, 0.5811, 0.4303,  ..., 0.6273, 0.4430, 0.5314],\n",
      "        [0.2752, 0.4842, 0.4790,  ..., 0.7106, 0.4806, 0.5548],\n",
      "        ...,\n",
      "        [0.4486, 0.4799, 0.5392,  ..., 0.5572, 0.4552, 0.5148],\n",
      "        [0.2722, 0.6162, 0.4751,  ..., 0.6016, 0.4958, 0.4363],\n",
      "        [0.4440, 0.5534, 0.4927,  ..., 0.5528, 0.4213, 0.5696]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 65, Loss: 1204.3297119140625, Val Loss: 312.27777099609375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.5125, 0.5211, 0.4792,  ..., 0.5439, 0.4714, 0.5528],\n",
      "        [0.4082, 0.6305, 0.5568,  ..., 0.5093, 0.4168, 0.5268],\n",
      "        [0.2900, 0.4738, 0.3490,  ..., 0.4750, 0.3565, 0.5202],\n",
      "        ...,\n",
      "        [0.3682, 0.6259, 0.5919,  ..., 0.6178, 0.4388, 0.5125],\n",
      "        [0.3673, 0.4775, 0.4480,  ..., 0.5252, 0.4096, 0.4471],\n",
      "        [0.3167, 0.4237, 0.5238,  ..., 0.6430, 0.4281, 0.4714]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 66, Loss: 1204.92919921875, Val Loss: 312.2781677246094\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4391, 0.4950, 0.5290,  ..., 0.4679, 0.4987, 0.4094],\n",
      "        [0.3931, 0.4144, 0.4931,  ..., 0.5764, 0.4345, 0.4754],\n",
      "        [0.4659, 0.5158, 0.5081,  ..., 0.4557, 0.4684, 0.5109],\n",
      "        ...,\n",
      "        [0.3519, 0.4066, 0.4125,  ..., 0.5076, 0.3779, 0.4993],\n",
      "        [0.4407, 0.5462, 0.4814,  ..., 0.6166, 0.4737, 0.5695],\n",
      "        [0.4004, 0.4760, 0.4015,  ..., 0.6365, 0.4241, 0.5097]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 67, Loss: 1204.274658203125, Val Loss: 312.2742614746094\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4240, 0.6238, 0.4415,  ..., 0.6122, 0.4785, 0.5727],\n",
      "        [0.4332, 0.6379, 0.5083,  ..., 0.5973, 0.3955, 0.5420],\n",
      "        [0.4451, 0.4933, 0.6053,  ..., 0.5675, 0.5337, 0.4843],\n",
      "        ...,\n",
      "        [0.4274, 0.4904, 0.4846,  ..., 0.5303, 0.4452, 0.5121],\n",
      "        [0.4077, 0.4335, 0.5222,  ..., 0.5550, 0.5229, 0.5059],\n",
      "        [0.4309, 0.4672, 0.4541,  ..., 0.5075, 0.4105, 0.4872]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 68, Loss: 1204.349365234375, Val Loss: 312.2713623046875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3978, 0.6088, 0.3961,  ..., 0.5123, 0.4038, 0.5436],\n",
      "        [0.3894, 0.5164, 0.4446,  ..., 0.4719, 0.4312, 0.5219],\n",
      "        [0.4276, 0.4810, 0.4929,  ..., 0.4735, 0.4343, 0.4917],\n",
      "        ...,\n",
      "        [0.5113, 0.5250, 0.5530,  ..., 0.5164, 0.3710, 0.5933],\n",
      "        [0.3375, 0.4442, 0.5595,  ..., 0.5882, 0.4729, 0.4822],\n",
      "        [0.5468, 0.5275, 0.5139,  ..., 0.4542, 0.4390, 0.5493]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 69, Loss: 1205.0042724609375, Val Loss: 312.27044677734375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3504, 0.4667, 0.4574,  ..., 0.6118, 0.3918, 0.5363],\n",
      "        [0.4142, 0.5670, 0.5379,  ..., 0.5121, 0.3983, 0.4745],\n",
      "        [0.4778, 0.5085, 0.5883,  ..., 0.5363, 0.4604, 0.5009],\n",
      "        ...,\n",
      "        [0.3903, 0.5806, 0.4952,  ..., 0.5360, 0.4005, 0.5633],\n",
      "        [0.4149, 0.3989, 0.4677,  ..., 0.6360, 0.4458, 0.5841],\n",
      "        [0.4103, 0.4350, 0.4572,  ..., 0.6063, 0.4388, 0.4845]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 70, Loss: 1204.8592529296875, Val Loss: 312.2689514160156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4922, 0.5205, 0.5893,  ..., 0.5166, 0.4521, 0.4965],\n",
      "        [0.4814, 0.5179, 0.5206,  ..., 0.5106, 0.4108, 0.5660],\n",
      "        [0.3725, 0.6037, 0.4633,  ..., 0.5271, 0.4247, 0.5466],\n",
      "        ...,\n",
      "        [0.4608, 0.4743, 0.4851,  ..., 0.5324, 0.4314, 0.5920],\n",
      "        [0.4721, 0.4160, 0.4763,  ..., 0.5309, 0.3742, 0.6349],\n",
      "        [0.4851, 0.4703, 0.5154,  ..., 0.6457, 0.4363, 0.5660]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 71, Loss: 1204.8922119140625, Val Loss: 312.26739501953125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4575, 0.5442, 0.4893,  ..., 0.5646, 0.4574, 0.5717],\n",
      "        [0.3930, 0.4588, 0.4635,  ..., 0.5018, 0.4450, 0.5188],\n",
      "        [0.4561, 0.5591, 0.5968,  ..., 0.4178, 0.4020, 0.4736],\n",
      "        ...,\n",
      "        [0.4380, 0.5209, 0.5770,  ..., 0.5698, 0.4420, 0.4654],\n",
      "        [0.4528, 0.5297, 0.4607,  ..., 0.4946, 0.4337, 0.5628],\n",
      "        [0.5271, 0.5839, 0.4813,  ..., 0.5179, 0.4177, 0.5930]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 72, Loss: 1203.6793212890625, Val Loss: 312.2679748535156\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4016, 0.5536, 0.4602,  ..., 0.5197, 0.4130, 0.4824],\n",
      "        [0.4885, 0.5144, 0.4957,  ..., 0.4587, 0.4401, 0.5119],\n",
      "        [0.4655, 0.5010, 0.4974,  ..., 0.4786, 0.4279, 0.4360],\n",
      "        ...,\n",
      "        [0.4247, 0.4487, 0.5096,  ..., 0.6344, 0.4271, 0.5499],\n",
      "        [0.4667, 0.5107, 0.5554,  ..., 0.6771, 0.4716, 0.5686],\n",
      "        [0.4527, 0.5640, 0.5102,  ..., 0.5440, 0.4448, 0.5487]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 73, Loss: 1204.978515625, Val Loss: 312.2690734863281\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4852, 0.5368, 0.5076,  ..., 0.6296, 0.5134, 0.6134],\n",
      "        [0.4124, 0.5633, 0.5791,  ..., 0.5616, 0.3456, 0.4912],\n",
      "        [0.3964, 0.4382, 0.4490,  ..., 0.5483, 0.4032, 0.4444],\n",
      "        ...,\n",
      "        [0.4455, 0.4988, 0.4480,  ..., 0.5831, 0.4518, 0.5022],\n",
      "        [0.4275, 0.5409, 0.5375,  ..., 0.4824, 0.4806, 0.5344],\n",
      "        [0.3892, 0.5015, 0.4412,  ..., 0.6248, 0.4741, 0.5409]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 74, Loss: 1203.3748779296875, Val Loss: 312.2696838378906\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3701, 0.5324, 0.4306,  ..., 0.5636, 0.4619, 0.5457],\n",
      "        [0.4813, 0.5026, 0.5539,  ..., 0.4285, 0.4332, 0.5412],\n",
      "        [0.4556, 0.5164, 0.4913,  ..., 0.6003, 0.5223, 0.5276],\n",
      "        ...,\n",
      "        [0.4231, 0.5576, 0.4801,  ..., 0.4854, 0.4412, 0.5106],\n",
      "        [0.5174, 0.4260, 0.4793,  ..., 0.4901, 0.3920, 0.5801],\n",
      "        [0.4737, 0.5627, 0.4664,  ..., 0.5787, 0.4588, 0.5559]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 75, Loss: 1204.6197509765625, Val Loss: 312.2733459472656\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3495, 0.4463, 0.4345,  ..., 0.6484, 0.4442, 0.4726],\n",
      "        [0.4690, 0.4667, 0.5094,  ..., 0.5502, 0.4790, 0.5192],\n",
      "        [0.4133, 0.5557, 0.5170,  ..., 0.4374, 0.4481, 0.5548],\n",
      "        ...,\n",
      "        [0.4962, 0.5122, 0.4940,  ..., 0.5812, 0.4754, 0.5485],\n",
      "        [0.2717, 0.5331, 0.4606,  ..., 0.6447, 0.3940, 0.5808],\n",
      "        [0.4868, 0.5294, 0.5211,  ..., 0.4974, 0.4567, 0.5471]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 76, Loss: 1204.8050537109375, Val Loss: 312.2729797363281\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4198, 0.5123, 0.5179,  ..., 0.5630, 0.4674, 0.4866],\n",
      "        [0.4267, 0.5594, 0.5608,  ..., 0.4169, 0.3698, 0.5393],\n",
      "        [0.3592, 0.5107, 0.5773,  ..., 0.6248, 0.4785, 0.5355],\n",
      "        ...,\n",
      "        [0.4975, 0.5083, 0.4978,  ..., 0.5423, 0.4151, 0.5393],\n",
      "        [0.4731, 0.4469, 0.5316,  ..., 0.4903, 0.4550, 0.4979],\n",
      "        [0.3770, 0.5494, 0.5146,  ..., 0.6016, 0.4790, 0.5100]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 77, Loss: 1203.63134765625, Val Loss: 312.272705078125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4827, 0.5015, 0.4518,  ..., 0.6002, 0.5030, 0.5301],\n",
      "        [0.4572, 0.4419, 0.5064,  ..., 0.5472, 0.4729, 0.5379],\n",
      "        [0.4764, 0.5157, 0.4897,  ..., 0.5232, 0.4103, 0.5319],\n",
      "        ...,\n",
      "        [0.5274, 0.4634, 0.5358,  ..., 0.5426, 0.4742, 0.5907],\n",
      "        [0.4630, 0.6111, 0.4722,  ..., 0.4750, 0.4710, 0.5625],\n",
      "        [0.4560, 0.5395, 0.4508,  ..., 0.5807, 0.3972, 0.5801]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 78, Loss: 1204.240234375, Val Loss: 312.2735595703125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3631, 0.5263, 0.4980,  ..., 0.5993, 0.4536, 0.5409],\n",
      "        [0.3984, 0.5506, 0.5184,  ..., 0.5042, 0.4787, 0.4645],\n",
      "        [0.3763, 0.4867, 0.4487,  ..., 0.5472, 0.4547, 0.4431],\n",
      "        ...,\n",
      "        [0.5620, 0.4449, 0.5508,  ..., 0.5584, 0.4487, 0.6090],\n",
      "        [0.3080, 0.4890, 0.4476,  ..., 0.5703, 0.4517, 0.4751],\n",
      "        [0.3740, 0.4939, 0.4390,  ..., 0.6801, 0.4303, 0.6191]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 79, Loss: 1203.8856201171875, Val Loss: 312.27337646484375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4051, 0.4784, 0.4600,  ..., 0.5014, 0.4608, 0.4678],\n",
      "        [0.4517, 0.5255, 0.5945,  ..., 0.5555, 0.4665, 0.4987],\n",
      "        [0.4320, 0.5842, 0.5342,  ..., 0.5006, 0.4322, 0.5751],\n",
      "        ...,\n",
      "        [0.4590, 0.6070, 0.5130,  ..., 0.5946, 0.4212, 0.6154],\n",
      "        [0.5248, 0.4931, 0.5312,  ..., 0.5468, 0.4566, 0.6075],\n",
      "        [0.4285, 0.4911, 0.4607,  ..., 0.5877, 0.4522, 0.5095]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 80, Loss: 1205.2177734375, Val Loss: 312.2685241699219\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3099, 0.5325, 0.4165,  ..., 0.5923, 0.4731, 0.5728],\n",
      "        [0.4144, 0.5207, 0.5367,  ..., 0.5730, 0.4553, 0.5682],\n",
      "        [0.3258, 0.5176, 0.4555,  ..., 0.5401, 0.3789, 0.5108],\n",
      "        ...,\n",
      "        [0.4027, 0.5742, 0.5471,  ..., 0.5753, 0.4383, 0.4664],\n",
      "        [0.4898, 0.4392, 0.4858,  ..., 0.5986, 0.5140, 0.5885],\n",
      "        [0.4101, 0.4968, 0.4821,  ..., 0.5320, 0.4404, 0.4920]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 81, Loss: 1204.776611328125, Val Loss: 312.27081298828125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3549, 0.4076, 0.4412,  ..., 0.6144, 0.4019, 0.5087],\n",
      "        [0.5304, 0.5583, 0.4992,  ..., 0.4430, 0.4315, 0.5230],\n",
      "        [0.3649, 0.4631, 0.5176,  ..., 0.5041, 0.3466, 0.5435],\n",
      "        ...,\n",
      "        [0.3824, 0.5155, 0.4703,  ..., 0.5972, 0.3978, 0.4923],\n",
      "        [0.4205, 0.5858, 0.4503,  ..., 0.5906, 0.4255, 0.6566],\n",
      "        [0.4118, 0.5536, 0.4672,  ..., 0.5216, 0.4414, 0.5041]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 82, Loss: 1203.7388916015625, Val Loss: 312.26788330078125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4411, 0.5642, 0.5122,  ..., 0.5399, 0.4577, 0.5834],\n",
      "        [0.4668, 0.5376, 0.4782,  ..., 0.5240, 0.4589, 0.5006],\n",
      "        [0.4291, 0.5334, 0.5194,  ..., 0.4465, 0.4413, 0.4991],\n",
      "        ...,\n",
      "        [0.3923, 0.5356, 0.5152,  ..., 0.5252, 0.4772, 0.5086],\n",
      "        [0.3957, 0.4561, 0.5043,  ..., 0.6003, 0.4237, 0.5251],\n",
      "        [0.4304, 0.5227, 0.4689,  ..., 0.5222, 0.3954, 0.5851]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 83, Loss: 1205.559326171875, Val Loss: 312.2662353515625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4337, 0.5083, 0.5229,  ..., 0.5768, 0.5446, 0.5362],\n",
      "        [0.4180, 0.5840, 0.5029,  ..., 0.5509, 0.3818, 0.5392],\n",
      "        [0.3818, 0.4156, 0.4864,  ..., 0.4868, 0.3421, 0.4198],\n",
      "        ...,\n",
      "        [0.4934, 0.4637, 0.5114,  ..., 0.5542, 0.4230, 0.5189],\n",
      "        [0.3831, 0.6202, 0.5052,  ..., 0.5152, 0.4196, 0.5468],\n",
      "        [0.4611, 0.4615, 0.4818,  ..., 0.6953, 0.5001, 0.5452]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 84, Loss: 1205.20361328125, Val Loss: 312.26513671875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2350, 0.4236, 0.4502,  ..., 0.6284, 0.4543, 0.5121],\n",
      "        [0.4374, 0.5164, 0.5622,  ..., 0.3675, 0.3712, 0.5006],\n",
      "        [0.4546, 0.5000, 0.5111,  ..., 0.4640, 0.4696, 0.5043],\n",
      "        ...,\n",
      "        [0.3861, 0.5546, 0.4502,  ..., 0.5806, 0.3628, 0.5733],\n",
      "        [0.5263, 0.4597, 0.5634,  ..., 0.5175, 0.4636, 0.6050],\n",
      "        [0.4081, 0.6011, 0.3854,  ..., 0.6524, 0.4516, 0.5794]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 85, Loss: 1204.0440673828125, Val Loss: 312.26409912109375\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4448, 0.5181, 0.5117,  ..., 0.6296, 0.5299, 0.5060],\n",
      "        [0.4786, 0.5741, 0.4845,  ..., 0.5240, 0.4129, 0.5558],\n",
      "        [0.3596, 0.6100, 0.4148,  ..., 0.5592, 0.4198, 0.6529],\n",
      "        ...,\n",
      "        [0.4572, 0.4736, 0.4853,  ..., 0.5122, 0.4171, 0.5786],\n",
      "        [0.4363, 0.4466, 0.4830,  ..., 0.5160, 0.3574, 0.5597],\n",
      "        [0.4016, 0.5685, 0.4830,  ..., 0.6038, 0.4628, 0.5597]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 86, Loss: 1206.0267333984375, Val Loss: 312.2691650390625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4427, 0.6035, 0.5169,  ..., 0.5232, 0.4171, 0.4737],\n",
      "        [0.4177, 0.4810, 0.5048,  ..., 0.5659, 0.3892, 0.4828],\n",
      "        [0.3275, 0.5063, 0.4455,  ..., 0.5793, 0.4342, 0.5547],\n",
      "        ...,\n",
      "        [0.3375, 0.3992, 0.4914,  ..., 0.6409, 0.4115, 0.5042],\n",
      "        [0.5043, 0.5341, 0.5947,  ..., 0.5330, 0.5255, 0.5378],\n",
      "        [0.3288, 0.4196, 0.4237,  ..., 0.6904, 0.4909, 0.5162]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 87, Loss: 1203.6636962890625, Val Loss: 312.2695617675781\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4904, 0.4848, 0.4707,  ..., 0.5518, 0.4531, 0.5886],\n",
      "        [0.3614, 0.5192, 0.4906,  ..., 0.5084, 0.3659, 0.4616],\n",
      "        [0.3214, 0.4945, 0.4595,  ..., 0.4519, 0.3168, 0.4824],\n",
      "        ...,\n",
      "        [0.3784, 0.5405, 0.4845,  ..., 0.5871, 0.4705, 0.5152],\n",
      "        [0.3605, 0.3882, 0.4388,  ..., 0.6782, 0.5175, 0.4845],\n",
      "        [0.3233, 0.5727, 0.3920,  ..., 0.5944, 0.3697, 0.5668]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 88, Loss: 1204.0753173828125, Val Loss: 312.2711486816406\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4660, 0.6192, 0.5164,  ..., 0.5258, 0.4915, 0.5325],\n",
      "        [0.4736, 0.4065, 0.4589,  ..., 0.5812, 0.4743, 0.5101],\n",
      "        [0.4079, 0.4563, 0.4980,  ..., 0.5421, 0.4567, 0.4339],\n",
      "        ...,\n",
      "        [0.5167, 0.5275, 0.5731,  ..., 0.5685, 0.4789, 0.5433],\n",
      "        [0.3411, 0.5125, 0.3903,  ..., 0.5347, 0.3289, 0.5659],\n",
      "        [0.3936, 0.4213, 0.4632,  ..., 0.6030, 0.4443, 0.4983]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 89, Loss: 1203.248779296875, Val Loss: 312.27227783203125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3053, 0.4579, 0.4732,  ..., 0.5335, 0.4216, 0.4510],\n",
      "        [0.4221, 0.4991, 0.4844,  ..., 0.5234, 0.5183, 0.5114],\n",
      "        [0.2916, 0.4184, 0.5028,  ..., 0.5601, 0.3184, 0.5313],\n",
      "        ...,\n",
      "        [0.4468, 0.4783, 0.4335,  ..., 0.6151, 0.4505, 0.5455],\n",
      "        [0.3140, 0.3593, 0.3947,  ..., 0.6069, 0.3403, 0.5198],\n",
      "        [0.3306, 0.4788, 0.3639,  ..., 0.5921, 0.3719, 0.6036]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 90, Loss: 1203.607666015625, Val Loss: 312.27081298828125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3852, 0.4689, 0.5279,  ..., 0.4991, 0.4515, 0.5526],\n",
      "        [0.4568, 0.4875, 0.5361,  ..., 0.5310, 0.4144, 0.5376],\n",
      "        [0.2648, 0.3487, 0.3431,  ..., 0.6639, 0.3878, 0.4250],\n",
      "        ...,\n",
      "        [0.4716, 0.4993, 0.5377,  ..., 0.5013, 0.4145, 0.4792],\n",
      "        [0.4419, 0.5538, 0.4301,  ..., 0.5063, 0.4617, 0.5329],\n",
      "        [0.3766, 0.4687, 0.4414,  ..., 0.6430, 0.4613, 0.4991]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 91, Loss: 1204.902099609375, Val Loss: 312.2712097167969\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3951, 0.5528, 0.3883,  ..., 0.5486, 0.4153, 0.5307],\n",
      "        [0.3607, 0.6970, 0.4948,  ..., 0.5512, 0.3711, 0.5724],\n",
      "        [0.4606, 0.5487, 0.4996,  ..., 0.5053, 0.4700, 0.5876],\n",
      "        ...,\n",
      "        [0.4483, 0.4850, 0.5582,  ..., 0.5826, 0.5339, 0.4542],\n",
      "        [0.3772, 0.5398, 0.4176,  ..., 0.5610, 0.4148, 0.5572],\n",
      "        [0.4808, 0.5402, 0.5753,  ..., 0.5558, 0.4875, 0.5632]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 92, Loss: 1204.49267578125, Val Loss: 312.2691955566406\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3927, 0.5184, 0.4623,  ..., 0.5526, 0.4684, 0.4754],\n",
      "        [0.4994, 0.5552, 0.5021,  ..., 0.4768, 0.4385, 0.5132],\n",
      "        [0.4285, 0.5228, 0.4717,  ..., 0.4790, 0.4115, 0.5503],\n",
      "        ...,\n",
      "        [0.4487, 0.4876, 0.4818,  ..., 0.5231, 0.4285, 0.5408],\n",
      "        [0.4897, 0.5049, 0.5404,  ..., 0.5815, 0.3827, 0.5354],\n",
      "        [0.4491, 0.4824, 0.5428,  ..., 0.5589, 0.4565, 0.5211]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 93, Loss: 1203.90087890625, Val Loss: 312.2673645019531\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4744, 0.5691, 0.5185,  ..., 0.5736, 0.4452, 0.5993],\n",
      "        [0.4827, 0.4371, 0.4931,  ..., 0.6169, 0.3445, 0.6725],\n",
      "        [0.2800, 0.4648, 0.4930,  ..., 0.4254, 0.2776, 0.4920],\n",
      "        ...,\n",
      "        [0.3513, 0.6264, 0.5189,  ..., 0.5735, 0.3952, 0.4905],\n",
      "        [0.3355, 0.4086, 0.4124,  ..., 0.5655, 0.3824, 0.5585],\n",
      "        [0.4165, 0.6354, 0.4981,  ..., 0.5756, 0.4249, 0.5846]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 94, Loss: 1204.278564453125, Val Loss: 312.26824951171875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4563, 0.6041, 0.4672,  ..., 0.4498, 0.4179, 0.5151],\n",
      "        [0.5101, 0.4828, 0.5183,  ..., 0.4732, 0.3683, 0.6144],\n",
      "        [0.4625, 0.6137, 0.4440,  ..., 0.4557, 0.3685, 0.5749],\n",
      "        ...,\n",
      "        [0.4709, 0.5056, 0.5278,  ..., 0.5495, 0.4888, 0.5536],\n",
      "        [0.4146, 0.4442, 0.4780,  ..., 0.6579, 0.4123, 0.5618],\n",
      "        [0.5471, 0.5077, 0.5326,  ..., 0.5765, 0.4421, 0.5370]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 95, Loss: 1204.30517578125, Val Loss: 312.2657470703125\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4010, 0.6489, 0.4365,  ..., 0.5312, 0.3989, 0.4980],\n",
      "        [0.4990, 0.5408, 0.4733,  ..., 0.5705, 0.4306, 0.6161],\n",
      "        [0.4263, 0.4925, 0.4488,  ..., 0.5468, 0.4638, 0.5560],\n",
      "        ...,\n",
      "        [0.4814, 0.4794, 0.4868,  ..., 0.5630, 0.4098, 0.6037],\n",
      "        [0.4548, 0.6698, 0.4585,  ..., 0.5182, 0.3732, 0.6116],\n",
      "        [0.5008, 0.5082, 0.4784,  ..., 0.5039, 0.4298, 0.5515]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 96, Loss: 1204.5626220703125, Val Loss: 312.27056884765625\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2234, 0.4837, 0.4457,  ..., 0.5593, 0.3840, 0.4896],\n",
      "        [0.3750, 0.4962, 0.4801,  ..., 0.5857, 0.4027, 0.5084],\n",
      "        [0.3420, 0.5284, 0.5078,  ..., 0.4179, 0.3839, 0.5196],\n",
      "        ...,\n",
      "        [0.4683, 0.4758, 0.5671,  ..., 0.5449, 0.4861, 0.4726],\n",
      "        [0.3831, 0.4721, 0.4477,  ..., 0.5276, 0.4398, 0.5457],\n",
      "        [0.4689, 0.5417, 0.5176,  ..., 0.5434, 0.4300, 0.5753]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 97, Loss: 1204.5966796875, Val Loss: 312.2702331542969\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.2886, 0.5268, 0.4247,  ..., 0.6051, 0.4257, 0.5881],\n",
      "        [0.3862, 0.5188, 0.4297,  ..., 0.5225, 0.3958, 0.5388],\n",
      "        [0.3913, 0.3641, 0.5721,  ..., 0.3922, 0.3667, 0.5551],\n",
      "        ...,\n",
      "        [0.4235, 0.5070, 0.5174,  ..., 0.5954, 0.4316, 0.5375],\n",
      "        [0.4232, 0.4244, 0.4336,  ..., 0.6421, 0.4644, 0.5802],\n",
      "        [0.4081, 0.4886, 0.4698,  ..., 0.5565, 0.4343, 0.4988]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 98, Loss: 1204.3270263671875, Val Loss: 312.270263671875\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3983, 0.5541, 0.4827,  ..., 0.5830, 0.4937, 0.5319],\n",
      "        [0.4154, 0.5870, 0.5192,  ..., 0.5901, 0.4014, 0.5905],\n",
      "        [0.3451, 0.4734, 0.4231,  ..., 0.5580, 0.4069, 0.5462],\n",
      "        ...,\n",
      "        [0.4481, 0.5348, 0.5371,  ..., 0.4892, 0.4136, 0.5487],\n",
      "        [0.4366, 0.4907, 0.3945,  ..., 0.5236, 0.3952, 0.5797],\n",
      "        [0.4633, 0.5367, 0.4812,  ..., 0.4692, 0.4391, 0.5861]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 99, Loss: 1204.6995849609375, Val Loss: 312.2696228027344\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.3431, 0.5497, 0.3701,  ..., 0.5660, 0.3904, 0.5919],\n",
      "        [0.3794, 0.4682, 0.5370,  ..., 0.6549, 0.3933, 0.4776],\n",
      "        [0.3430, 0.5675, 0.5388,  ..., 0.5633, 0.4234, 0.6617],\n",
      "        ...,\n",
      "        [0.5035, 0.4622, 0.5204,  ..., 0.5974, 0.4757, 0.5388],\n",
      "        [0.4704, 0.6081, 0.5160,  ..., 0.4608, 0.4308, 0.5509],\n",
      "        [0.4494, 0.4688, 0.5175,  ..., 0.4925, 0.4823, 0.4741]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "Epoch 100, Loss: 1205.433837890625, Val Loss: 312.2685546875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Define the Hypernetwork\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.dropout1 = nn.Dropout(0.4)   # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.dropout2 = nn.Dropout(0.4)   # Another Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        xavier_uniform_(self.fc3.weight)  # Xavier initialization\n",
    "        self.fc4 = nn.Linear(32, input_dim)\n",
    "        xavier_uniform_(self.fc4.weight)  # Xavier initialization\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))  # Sigmoid to ensure output is a probability\n",
    "        return x\n",
    "\n",
    "# Define the Prediction Network\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.dropout2 = nn.Dropout(0.5)   # Another Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(64, 1) # final layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Applying dropout after activation\n",
    "        x = self.fc3(x)  # No activation function, suitable for regression\n",
    "        return x\n",
    "    \n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the custom loss function incorporating both MSE and regularization term\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg, sigma):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.sigma = sigma\n",
    "        self.normal_dist = Normal(torch.tensor([0.0]), torch.tensor([sigma]))\n",
    "\n",
    "    def forward(self, y_pred, y_true, selection_probs):\n",
    "        # Calculate the MSE part\n",
    "        mse = self.mse_loss(y_pred, y_true)\n",
    "        \n",
    "        # Calculate the regularization term\n",
    "        # Use the CDF of the standard Gaussian distribution\n",
    "        regularization_term = torch.sum(self.normal_dist.cdf(selection_probs))/ self.sigma\n",
    "        \n",
    "        # Combine the MSE and regularization term\n",
    "        total_loss = mse + self.lambda_reg * regularization_term\n",
    "        return total_loss\n",
    "\n",
    "# Usage of CustomLoss\n",
    "# Set lambda_reg and sigma based on your requirements\n",
    "lambda_regu = 0.01  # Example value; needs to be tuned\n",
    "sigma = 1.0  # Assuming noise standard deviation is 1\n",
    "\n",
    "# Initialize the custom loss\n",
    "custom_loss = CustomLoss(lambda_regu, sigma)\n",
    "\n",
    "# During training, you would use this custom_loss function like so:\n",
    "# custom_loss(y_pred, y_true, selection_probs)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, hypernet, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hypernet.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass through the hypernetwork to get selection probabilities\n",
    "        selection_prob = hypernet(X_train)\n",
    "        # Sample from the Bernoulli distribution to get the feature mask\n",
    "        selection_mask = torch.bernoulli(selection_prob)\n",
    "        print(\"##\"*90)\n",
    "        print(selection_prob)\n",
    "        print(\"##\"*90)\n",
    "        print(\"##\"*90)\n",
    "        print(selection_mask)\n",
    "        print(\"##\"*90)\n",
    "        # Apply mask to the input features\n",
    "        selected_features = X_train * selection_mask\n",
    "        # Make predictions with the masked features\n",
    "        y_pred = model(selected_features)\n",
    "        loss = custom_loss(y_pred, y_train, selection_mask) # TODO: Better loss function in progress\n",
    "        # Backpropagation\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        hypernet.eval()\n",
    "        with torch.no_grad():\n",
    "            val_selection_prob = hypernet(X_val)\n",
    "            val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "            val_selected_features = X_val * val_selection_mask\n",
    "            val_y_pred = model(val_selected_features)\n",
    "            # val_loss = criterion(val_y_pred, y_val)\n",
    "            val_loss = custom_loss(val_y_pred, y_val, val_selection_prob)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "hypernet = HyperNet(input_dim=X_train_tensor.shape[1])\n",
    "model = PredNet(feature_dim=X_train_tensor.shape[1])\n",
    "optimizer = Adam(list(hypernet.parameters()) + list(model.parameters()) , lr=0.01)\n",
    "\n",
    "# Training\n",
    "epochs = 100  # Adjust as necessary based on convergence and performance\n",
    "train(model, hypernet, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to train and evaluate models using STG algorithm\n",
    "def train_evaluate_stg(X_train, y_train, X_test, y_test, context_dim):\n",
    "    # Instantiate HyperNetwork and PredictionNetwork for STG\n",
    "    hypernetwork_stg = HyperNetwork(context_dim, X_train.shape[1] - context_dim)\n",
    "    prediction_network_stg = PredictionNetwork(X_train.shape[1] - context_dim)\n",
    "\n",
    "    # Train the model\n",
    "    stg_losses_train, stg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "                                                    hypernetwork_stg, prediction_network_stg)\n",
    "    return stg_losses_train, stg_losses_val\n",
    "\n",
    "# Define a function to train and evaluate models using CSTG algorithm\n",
    "def train_evaluate_cstg(X_train, y_train, X_test, y_test, context_dim):\n",
    "    # Instantiate ConditionalStochasticGates model\n",
    "    cstg_model = ConditionalStochasticGates(context_dim, X_train.shape[1] - context_dim)\n",
    "\n",
    "    # Train the model\n",
    "    cstg_losses_train, cstg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "                                                      cstg_model, prediction_network)\n",
    "    return cstg_losses_train, cstg_losses_val\n",
    "\n",
    "# Define a function to train the model and return training and validation losses\n",
    "def train_model(X_train, y_train, X_test, y_test, hypernetwork, prediction_network):\n",
    "    optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    num_epochs = 100\n",
    "    stg_losses_train, stg_losses_val = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training step\n",
    "        hypernetwork.train()\n",
    "        prediction_network.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        gates_train = hypernetwork(X_train[:, :context_dim])\n",
    "        selected_features_train = X_train[:, context_dim:] * stochastic_gates(gates_train)\n",
    "        predictions_train = prediction_network(selected_features_train)\n",
    "        loss_train = loss_fn(predictions_train, y_train)\n",
    "\n",
    "        # Backward pass\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            hypernetwork.eval()\n",
    "            prediction_network.eval()\n",
    "\n",
    "            gates_val = hypernetwork(X_test[:, :context_dim])\n",
    "            selected_features_val = X_test[:, context_dim:] * stochastic_gates(gates_val)\n",
    "            predictions_val = prediction_network(selected_features_val)\n",
    "            loss_val = loss_fn(predictions_val, y_test)\n",
    "\n",
    "        stg_losses_train.append(loss_train.item())\n",
    "        stg_losses_val.append(loss_val.item())\n",
    "\n",
    "    return stg_losses_train, stg_losses_val\n",
    "\n",
    "# Define the context dimension\n",
    "context_dim = 3  # Assuming the first 3 features are used as context\n",
    "\n",
    "# Train and evaluate models using STG algorithm\n",
    "stg_losses_train, stg_losses_val = train_evaluate_stg(X_train_tensor, y_train_tensor, \n",
    "                                                      X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# Train and evaluate models using CSTG algorithm\n",
    "cstg_losses_train, cstg_losses_val = train_evaluate_cstg(X_train_tensor, y_train_tensor, \n",
    "                                                          X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# Plot training and validation losses for STG and CSTG algorithms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), stg_losses_train, label='STG Train Loss', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), stg_losses_val, label='STG Validation Loss', linestyle='--', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), cstg_losses_train, label='CSTG Train Loss', color='orange')\n",
    "plt.plot(range(1, num_epochs + 1), cstg_losses_val, label='CSTG Validation Loss', linestyle='--', color='orange')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Assuming X_train_tensor and X_test_tensor are already defined and properly shaped\n",
    "# input_dim = X_train_tensor.shape[1]  # Number of features in the input data\n",
    "# feature_dim = input_dim  # The hypernetwork output and prediction input dimensions must match\n",
    "\n",
    "# # Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "# hypernet = HyperNet(input_dim)\n",
    "# model = PredNet(feature_dim)\n",
    "# optimizer = Adam(list(hypernet.parameters()) + list(model.parameters()) , lr=0.01)\n",
    "\n",
    "# # Training\n",
    "# epochs = 2  # Adjust as necessary based on convergence and performance\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "AveOccup: 0.44150832295417786\n",
      "MedInc: 0.5098904967308044\n",
      "Uniform_Noise: 0.474697470664978\n",
      "Cosine_Noise: 0.4606289863586426\n",
      "AveRooms: 0.47779104113578796\n",
      "HouseAge: 0.461354523897171\n",
      "Gaussian_Noise: 0.5596177577972412\n",
      "Population: 0.407277375459671\n",
      "Longitude: 0.5244668126106262\n",
      "AveBedrms: 0.4461166560649872\n",
      "Latitude: 0.5383712649345398\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with the trained models\n",
    "def make_inference(model, hypernet, X_test):\n",
    "    model.eval()\n",
    "    hypernet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the feature selection probabilities from the hypernetwork\n",
    "        selection_prob = hypernet(X_test)\n",
    "        # For inference use the expected values (probabilities) instead of sampling\n",
    "        selected_features = X_test * selection_prob\n",
    "        # Get the model predictions\n",
    "        predictions = model(selected_features)\n",
    "    return predictions, selection_prob\n",
    "\n",
    "# Load the test data (replace this with your actual test data)\n",
    "X_test = X_test_tensor  # Assuming X_test_tensor is your test data\n",
    "\n",
    "# Make inference\n",
    "predictions, feature_importance_probabilities = make_inference(model, hypernet, X_test)\n",
    "\n",
    "\n",
    "# Print the conditional probability of each feature being significant\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    print(f\"{feature_name}: {feature_importance_probabilities[:, i].mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "Gaussian_Noise: 0.5596177577972412\n",
      "Latitude: 0.5383712649345398\n",
      "Longitude: 0.5244668126106262\n",
      "MedInc: 0.5098904967308044\n",
      "AveRooms: 0.47779104113578796\n",
      "Uniform_Noise: 0.474697470664978\n",
      "HouseAge: 0.461354523897171\n",
      "Cosine_Noise: 0.4606289863586426\n",
      "AveBedrms: 0.4461166560649872\n",
      "AveOccup: 0.44150832295417786\n",
      "Population: 0.407277375459671\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples (feature_name, probability)\n",
    "feature_probabilities = [(feature_name, feature_importance_probabilities[:, i].mean().item()) for i, feature_name in enumerate(feature_names)]\n",
    "\n",
    "# Sort the list by probability in descending order\n",
    "sorted_feature_probabilities = sorted(feature_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted list\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for feature, probability in sorted_feature_probabilities:\n",
    "    print(f\"{feature}: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mean_feature_importance_probabilities \u001b[38;5;241m=\u001b[39m feature_importance_probabilities\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Determine which features have a mean selection probability above the threshold\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m important_features_indices \u001b[38;5;241m=\u001b[39m mean_feature_importance_probabilities \u001b[38;5;241m>\u001b[39m \u001b[43mthreshold\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean feature selection probabilities from the hypernetwork (higher means more important):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_feature_importance_probabilities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the mean feature importance probability across all examples\n",
    "mean_feature_importance_probabilities = feature_importance_probabilities.mean(dim=0)\n",
    "\n",
    "# Determine which features have a mean selection probability above the threshold\n",
    "important_features_indices = mean_feature_importance_probabilities > threshold\n",
    "\n",
    "print(\"Mean feature selection probabilities from the hypernetwork (higher means more important):\")\n",
    "print(mean_feature_importance_probabilities)\n",
    "\n",
    "print(\"\\nFeatures with mean selection probability higher than threshold:\")\n",
    "print(important_features_indices)\n",
    "# Extract the names of the important features\n",
    "important_feature_names = [name for i, name in enumerate(feature_names) if important_features_indices[i]]\n",
    "\n",
    "print(\"\\nImportant feature names:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters of HyperNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1502, -0.1104, -0.0366,  ...,  0.1718,  0.0637, -0.0697],\n",
      "        [-0.1804,  0.0159, -0.1112,  ...,  0.1196, -0.0227, -0.0770],\n",
      "        [ 0.1346,  0.1256,  0.1248,  ..., -0.0737, -0.1655, -0.1717],\n",
      "        ...,\n",
      "        [-0.0565,  0.0891,  0.0982,  ..., -0.1358, -0.1359,  0.0080],\n",
      "        [-0.0074, -0.0703,  0.0896,  ...,  0.2032, -0.2027, -0.1844],\n",
      "        [-0.0535,  0.0381,  0.0786,  ..., -0.1920, -0.0853, -0.0579]])\n",
      "fc1.bias:\n",
      "tensor([ 0.1512,  0.2043,  0.1095, -0.1715, -0.0402, -0.1496, -0.2718,  0.2931,\n",
      "         0.1333,  0.1599,  0.0045, -0.1857, -0.2328,  0.0870,  0.1531, -0.2914,\n",
      "         0.0040,  0.2220, -0.0121, -0.1919,  0.0696, -0.0186,  0.0074,  0.0958,\n",
      "        -0.0544,  0.2858, -0.1110, -0.0218,  0.0621, -0.1147, -0.2755,  0.1071,\n",
      "         0.2763,  0.1478, -0.2083,  0.2526,  0.0627, -0.0947, -0.2117, -0.2135,\n",
      "         0.1645, -0.2223,  0.1970, -0.1847, -0.0560, -0.2816, -0.2027, -0.0012,\n",
      "         0.2529,  0.2172, -0.0118, -0.2407,  0.2348, -0.2373, -0.1128,  0.0839,\n",
      "        -0.1592,  0.2953,  0.1130,  0.0883,  0.2779, -0.0367,  0.1327, -0.0183,\n",
      "        -0.0516, -0.2265, -0.0095, -0.2309,  0.1992, -0.2779, -0.0978, -0.0260,\n",
      "        -0.2853,  0.1937,  0.2322,  0.2955, -0.0263, -0.2422,  0.2143, -0.1737,\n",
      "        -0.0019,  0.0418, -0.2932,  0.1785,  0.2519,  0.1678,  0.2759, -0.2777,\n",
      "        -0.1369, -0.0138,  0.2617, -0.2685, -0.0044,  0.1171, -0.0095,  0.1542,\n",
      "        -0.1603, -0.1790,  0.1629,  0.0338, -0.1578, -0.2427,  0.0014, -0.1207,\n",
      "        -0.1155, -0.0885, -0.1136, -0.1674, -0.1247, -0.2480, -0.1872, -0.1625,\n",
      "        -0.1832,  0.2756, -0.2429,  0.0740, -0.1579,  0.0388, -0.2921, -0.1844,\n",
      "        -0.0605, -0.0986, -0.1727, -0.1732, -0.0819,  0.0992,  0.0258,  0.1645])\n",
      "fc2.weight:\n",
      "tensor([[-0.0785, -0.0891,  0.1382,  ..., -0.0968, -0.0798,  0.0083],\n",
      "        [ 0.1183, -0.1070, -0.1937,  ..., -0.0883, -0.1277, -0.0018],\n",
      "        [-0.1392, -0.2076, -0.0042,  ...,  0.0864, -0.1327, -0.0715],\n",
      "        ...,\n",
      "        [ 0.1834,  0.0056,  0.1638,  ...,  0.0071, -0.0697,  0.1988],\n",
      "        [-0.1742,  0.1368,  0.1087,  ..., -0.1041,  0.0640, -0.1027],\n",
      "        [-0.0524, -0.1429,  0.0062,  ..., -0.1828, -0.0921,  0.1280]])\n",
      "fc2.bias:\n",
      "tensor([-0.0545, -0.0791,  0.0383,  0.0282,  0.0775,  0.0259, -0.0079, -0.0723,\n",
      "         0.0538, -0.0139, -0.0712])\n",
      "\n",
      "Final parameters of PredNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1951,  0.1289,  0.0336,  ...,  0.0818,  0.1140, -0.0908],\n",
      "        [-0.1356,  0.0355,  0.2034,  ...,  0.0818,  0.1826, -0.0745],\n",
      "        [ 0.1945,  0.0366,  0.1083,  ...,  0.1903,  0.1251,  0.1758],\n",
      "        ...,\n",
      "        [-0.0572, -0.0295, -0.1143,  ...,  0.0377,  0.0034,  0.0813],\n",
      "        [ 0.0804,  0.0441,  0.0599,  ...,  0.0587, -0.0560, -0.1215],\n",
      "        [-0.1221, -0.2030,  0.0446,  ..., -0.1810, -0.0804, -0.0874]])\n",
      "fc1.bias:\n",
      "tensor([ 0.0847,  0.2508,  0.0543, -0.1693, -0.2101,  0.0284, -0.1087, -0.0365,\n",
      "        -0.1931, -0.0249,  0.1657, -0.0997,  0.0687, -0.1125, -0.2778,  0.0163,\n",
      "        -0.2127, -0.2116, -0.1091,  0.1054,  0.2549,  0.2965,  0.0145,  0.0660,\n",
      "        -0.1624,  0.2841,  0.0210, -0.0436, -0.1952,  0.2271,  0.2290, -0.2897,\n",
      "         0.2864, -0.2730,  0.2922,  0.0786,  0.2531,  0.1794, -0.1115, -0.2798,\n",
      "         0.0356, -0.1551,  0.0831,  0.2643, -0.1746,  0.2181,  0.0653,  0.0814,\n",
      "        -0.0537,  0.0539,  0.2520,  0.0199,  0.1531,  0.0614,  0.2748, -0.3002,\n",
      "         0.1219, -0.2122, -0.1631, -0.1681, -0.0185,  0.0782, -0.1613, -0.2044,\n",
      "        -0.0142,  0.0375, -0.1722, -0.1976, -0.0613, -0.0207,  0.0363, -0.1575,\n",
      "        -0.0928,  0.1729,  0.2558, -0.1558,  0.2242,  0.2890,  0.2902,  0.0747,\n",
      "        -0.0839, -0.0538, -0.0766, -0.2373,  0.0298, -0.0279,  0.1766,  0.1254,\n",
      "        -0.0312, -0.1889,  0.2507,  0.1399, -0.2916, -0.0606,  0.2620,  0.2371,\n",
      "         0.0271,  0.2982,  0.0881, -0.1257, -0.1925,  0.1016,  0.2289,  0.2053,\n",
      "         0.1121, -0.1936,  0.0434, -0.0033,  0.0369,  0.2812, -0.0046, -0.2101,\n",
      "         0.1726,  0.1283,  0.1392, -0.2868,  0.0306,  0.2251, -0.0042,  0.2264,\n",
      "         0.0210, -0.2270,  0.0823,  0.1144,  0.1638,  0.2373, -0.0120,  0.1358])\n",
      "fc2.weight:\n",
      "tensor([[-0.0340, -0.0058, -0.0467,  0.0282, -0.0404, -0.0728,  0.0139, -0.0756,\n",
      "          0.0479,  0.0861,  0.0223, -0.0131, -0.0110, -0.0324, -0.0306, -0.0440,\n",
      "         -0.0814,  0.0595,  0.0548, -0.0188,  0.0324, -0.0526, -0.0685,  0.0643,\n",
      "          0.0150,  0.0643, -0.0713,  0.0403,  0.0188,  0.0580,  0.0643, -0.0183,\n",
      "         -0.0688,  0.0253, -0.0434,  0.0338, -0.0089,  0.0365, -0.0424, -0.0007,\n",
      "         -0.0031,  0.0186,  0.0339, -0.0069, -0.0783, -0.0002,  0.0574, -0.0139,\n",
      "         -0.0260,  0.0144,  0.0535,  0.0597,  0.0617,  0.0504,  0.0844,  0.0623,\n",
      "         -0.0482,  0.0821, -0.0063,  0.0366, -0.0527, -0.0601,  0.0478,  0.0693,\n",
      "         -0.0706, -0.0636, -0.0599,  0.0776,  0.0848,  0.0028,  0.0122, -0.0097,\n",
      "          0.0428, -0.0329, -0.0786, -0.0789, -0.0787, -0.0558, -0.0111,  0.0623,\n",
      "          0.0209,  0.0124, -0.0683, -0.0567, -0.0454, -0.0577, -0.0457, -0.0199,\n",
      "         -0.0357,  0.0092, -0.0231, -0.0800,  0.0424, -0.0070, -0.0331, -0.0531,\n",
      "         -0.0198, -0.0406, -0.0228,  0.0601, -0.0557, -0.0754,  0.0691,  0.0722,\n",
      "          0.0143,  0.0155,  0.0684, -0.0660, -0.0323,  0.0408,  0.0286, -0.0627,\n",
      "          0.0069,  0.0855, -0.0040, -0.0363,  0.0698,  0.0618,  0.0673,  0.0820,\n",
      "          0.0318,  0.0011, -0.0143, -0.0545,  0.0625,  0.0356, -0.0055, -0.0379]])\n",
      "fc2.bias:\n",
      "tensor([0.0553])\n"
     ]
    }
   ],
   "source": [
    "# ... (after the training loop)\n",
    "\n",
    "# Retrieve the final parameters of HyperNet\n",
    "hypernet_params = hypernet.state_dict()\n",
    "print(\"Final parameters of HyperNet:\")\n",
    "for param_name, param in hypernet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n",
    "\n",
    "# Retrieve the final parameters of PredNet\n",
    "prednet_params = model.state_dict()\n",
    "print(\"\\nFinal parameters of PredNet:\")\n",
    "for param_name, param in prednet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise error\n",
    "raise ValueError(\"This is a custom error message. You can replace this with an actual error message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     53\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not DataFrame"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, input_dim)  # The output should match the number of features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = X_train_tensor.shape[1]  # This should be the number of features in your input\n",
    "feature_dim = input_dim  # This should match the number of input features\n",
    "hypernet = HyperNet(input_dim)\n",
    "model = PredNet(feature_dim)\n",
    "\n",
    "# Rest of the initialization and training code...\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(list(model.parameters()) + list(hypernet.parameters()), lr=0.001)\n",
    "\n",
    "hypernet.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass through hypernetwork to get selection probabilities\n",
    "selection_prob = hypernet(X_train)\n",
    "# Sample from Bernoulli distribution to get feature mask\n",
    "selection_mask = torch.bernoulli(selection_prob)\n",
    "# Apply mask to input features\n",
    "selected_features = X_train * selection_mask\n",
    "\n",
    "# Make predictions with masked features\n",
    "y_pred = model(selected_features)\n",
    "loss = criterion(y_pred, y_train)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Validation phase\n",
    "model.eval()\n",
    "hypernet.eval()\n",
    "# with torch.no_grad():\n",
    "#     val_selection_prob = hypernet(X_val)\n",
    "#     val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "#     val_selected_features = X_val * val_selection_mask\n",
    "#     val_y_pred = model(val_selected_features)\n",
    "#     val_loss = criterion(val_y_pred, y_val)\n",
    "\n",
    "# print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Model initialization\n",
    "\n",
    "\n",
    "# Training\n",
    "# epochs = 100 # Adjust as necessary\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    119\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Adjust as necessary\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     86\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the Hypernetwork\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         # Initialize weights using Xavier initialization\n",
    "#         xavier_uniform_(self.fc1.weight)\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         xavier_uniform_(self.fc2.weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # Final layer with sigmoid to ensure outputs are between 0 and 1\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, feature_dim)  # feature_dim should match the number of input features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the Prediction Network\n",
    "# class PredNet(nn.Module):\n",
    "#     def __init__(self, feature_dim):\n",
    "#         super(PredNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(feature_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, 1) # Assuming a single output\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)  # feature_dim should match the number of input features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = 11\n",
    "feature_dim = input_dim  # Make sure this matches the number of input features\n",
    "hypernet = HyperNet(input_dim, feature_dim)\n",
    "model = PredNet(feature_dim)\n",
    "def train(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.bernoulli import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Hypernetwork, self).__init__()\n",
    "        # Define the architecture of the hypernetwork.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer mapping to Bernoulli parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = torch.sigmoid(self.fc2(x)) # Sigmoid to ensure output is in [0,1], representing probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        # Define the architecture of the prediction network.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer for the response variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = self.fc2(x) # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hypernet, criterion, optimizer, data_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for x, z, y in data_loader: # Assuming x is the feature, z is the context, y is the target\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate probabilities from the Hypernetwork\n",
    "            probs = hypernet(z)\n",
    "            \n",
    "            # Sample from Bernoulli to get the feature selection mask\n",
    "            m = Bernoulli(probs)\n",
    "            mask = m.sample()\n",
    "            \n",
    "            # Apply mask and predict\n",
    "            x_masked = x * mask\n",
    "            predictions = model(x_masked)\n",
    "            \n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
