{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Uniform_Noise</th>\n",
       "      <th>Cosine_Noise</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Gaussian_Noise</th>\n",
       "      <th>Population</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.247299</td>\n",
       "      <td>4.5625</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>-0.958842</td>\n",
       "      <td>4.845138</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-0.706843</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>-122.59</td>\n",
       "      <td>1.027611</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.267930</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>-0.134879</td>\n",
       "      <td>0.105413</td>\n",
       "      <td>5.262517</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.869742</td>\n",
       "      <td>2415.0</td>\n",
       "      <td>-119.19</td>\n",
       "      <td>1.012179</td>\n",
       "      <td>34.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.445217</td>\n",
       "      <td>5.2174</td>\n",
       "      <td>0.784740</td>\n",
       "      <td>0.261862</td>\n",
       "      <td>7.306957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.116566</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>-117.21</td>\n",
       "      <td>1.078261</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.345733</td>\n",
       "      <td>2.3083</td>\n",
       "      <td>-0.577435</td>\n",
       "      <td>-0.877524</td>\n",
       "      <td>5.485777</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.198482</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-122.63</td>\n",
       "      <td>1.262582</td>\n",
       "      <td>38.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.496000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.833185</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>5.442667</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.133279</td>\n",
       "      <td>936.0</td>\n",
       "      <td>-117.24</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>34.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveOccup  MedInc  Uniform_Noise  Cosine_Noise  AveRooms  HouseAge  \\\n",
       "0  2.247299  4.5625       0.760274     -0.958842  4.845138      46.0   \n",
       "1  3.267930  4.5000      -0.134879      0.105413  5.262517      17.0   \n",
       "2  3.445217  5.2174       0.784740      0.261862  7.306957       5.0   \n",
       "3  2.345733  2.3083      -0.577435     -0.877524  5.485777      20.0   \n",
       "4  2.496000  6.0000       0.833185      0.042673  5.442667      26.0   \n",
       "\n",
       "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
       "0       -0.706843      1872.0    -122.59   1.027611     37.97  \n",
       "1       -1.869742      2415.0    -119.19   1.012179     34.23  \n",
       "2        0.116566      3962.0    -117.21   1.078261     33.95  \n",
       "3        1.198482      1072.0    -122.63   1.262582     38.96  \n",
       "4       -0.133279       936.0    -117.24   0.781333     34.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Noise'] = cosine_values\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "display(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Define the Hypernetwork\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.dropout2 = nn.Dropout(0.5)   # Another Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        xavier_uniform_(self.fc3.weight)  # Xavier initialization\n",
    "        self.fc4 = nn.Linear(32, input_dim)\n",
    "        xavier_uniform_(self.fc4.weight)  # Xavier initialization\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))  # Sigmoid to ensure output is a probability\n",
    "        return x\n",
    "\n",
    "# Define the Prediction Network\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim,selection_mask_hÏ•):\n",
    "        super(PredNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.dropout2 = nn.Dropout(0.5)   # Another Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(64, 1) # final layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Applying dropout after activation\n",
    "        x = self.fc3(x)  # No activation function, suitable for regression\n",
    "        return x\n",
    "    \n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the custom loss function incorporating both MSE and regularization term\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self, lambda_reg, sigma):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "#         self.mse_loss = nn.MSELoss()\n",
    "#         self.lambda_reg = lambda_reg\n",
    "#         self.sigma = sigma\n",
    "#         self.normal_dist = Normal(torch.tensor([0.0]), torch.tensor([sigma]))\n",
    "\n",
    "#     def forward(self, y_pred, y_true, selection_probs):\n",
    "#         # Calculate the MSE part\n",
    "#         mse = self.mse_loss(y_pred, y_true)\n",
    "        \n",
    "#         # Calculate the regularization term\n",
    "#         # Use the CDF of the standard Gaussian distribution\n",
    "#         regularization_term = torch.sum(self.normal_dist.cdf(selection_probs))/ self.sigma\n",
    "        \n",
    "#         # Combine the MSE and regularization term\n",
    "#         total_loss = mse + self.lambda_reg * regularization_term\n",
    "#         return total_loss\n",
    "\n",
    "# Usage of CustomLoss\n",
    "# Set lambda_reg and sigma based on your requirements\n",
    "# lambda_regu = 0.01  # Example value; needs to be tuned\n",
    "# sigma = 1.0  # Assuming noise standard deviation is 1\n",
    "\n",
    "# # Initialize the custom loss\n",
    "# custom_loss = CustomLoss(lambda_regu, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function incorporating both MSE and regularization term\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg, sigma):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.sigma = sigma\n",
    "        self.normal_dist = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([sigma]))\n",
    "\n",
    "    def forward(self, y_pred, y_true, selection_mask):\n",
    "        # Calculate the L0 loss\n",
    "        # l0_loss = torch.sum((y_pred - y_true).abs() > 1e-6).float() #TODO: change to l0\n",
    "        \n",
    "        hÏ• = selection_mask\n",
    "        self.epsilon = self.normal_dist.sample() # sample from standard normal distribution\n",
    "        self.z = torch.clamp( self.epsilon + hÏ•, 0, 1)\n",
    "        print(\"meu sampled from standart normal dist\",self.normal_dist.sample())\n",
    "        # Calculate the regularization term\n",
    "        # Use the CDF of the standard Gaussian distribution\n",
    "        regularization_term = torch.sum(self.normal_dist.cdf(selection_mask)) / self.sigma\n",
    "\n",
    "        # Combine the L0 loss and regularization term\n",
    "        total_loss = l0_loss + self.lambda_reg * regularization_term\n",
    "        return total_loss\n",
    "\n",
    "lambda_regu = 0.01  # Example value; needs to be tuned\n",
    "sigma = 1.0  # Assuming noise standard deviation is 1\n",
    "\n",
    "# Initialize the custom loss\n",
    "custom_loss = CustomLoss(lambda_regu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llength of selection_prob:  16512\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.5084, 0.4533, 0.4831,  ..., 0.4571, 0.4007, 0.3496],\n",
      "        [0.3494, 0.3931, 0.5503,  ..., 0.4934, 0.4316, 0.3371],\n",
      "        [0.5726, 0.3713, 0.4706,  ..., 0.4974, 0.4594, 0.3306],\n",
      "        ...,\n",
      "        [0.5381, 0.3314, 0.5887,  ..., 0.5256, 0.4927, 0.4660],\n",
      "        [0.4136, 0.5647, 0.4638,  ..., 0.4182, 0.3463, 0.2645],\n",
      "        [0.4244, 0.4730, 0.5469,  ..., 0.3817, 0.4000, 0.3921]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "meu sampled from standart normal dist tensor([0.1407])\n",
      "meu sampled from standart normal dist tensor([0.1010])\n",
      "Epoch 1, Loss: 17711.853515625, Val Loss: 4438.6328125\n",
      "llength of selection_prob:  16512\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0.4683, 0.4860, 0.4296,  ..., 0.5081, 0.4545, 0.3207],\n",
      "        [0.5761, 0.4206, 0.4306,  ..., 0.4188, 0.5039, 0.5582],\n",
      "        [0.5068, 0.4143, 0.4202,  ..., 0.5255, 0.5304, 0.3696],\n",
      "        ...,\n",
      "        [0.3895, 0.4852, 0.5027,  ..., 0.4995, 0.5097, 0.3457],\n",
      "        [0.4252, 0.5347, 0.4775,  ..., 0.4075, 0.4234, 0.3630],\n",
      "        [0.4319, 0.5379, 0.4814,  ..., 0.5268, 0.4232, 0.4477]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "####################################################################################################################################################################################\n",
      "tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 1.]], grad_fn=<BernoulliBackward0>)\n",
      "####################################################################################################################################################################################\n",
      "meu sampled from standart normal dist tensor([1.5406])\n",
      "meu sampled from standart normal dist tensor([0.6613])\n",
      "Epoch 2, Loss: 17711.453125, Val Loss: 4438.6328125\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# During training, you would use this custom_loss function like so:\n",
    "# custom_loss(y_pred, y_true, selection_probs)\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, hypernet, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hypernet.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass through the hypernetwork to get selection probabilities\n",
    "        selection_prob = hypernet(X_train)\n",
    "        mu = selection_prob\n",
    "        sigma = [] # theese are the gates\n",
    "        print(\"llength of mu: \",len(selection_prob))\n",
    "        # selection_mask_hÏ•\n",
    "        normal_dist = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0])) # standard normal distribution\n",
    "\n",
    "        # Assuming mu is a tensor of shape [K, D]\n",
    "        for d in range(mu.size(1)):  # D is the second dimension size\n",
    "            epsilon_d = torch.normal(0, 1, size=(mu.size(0),), device=mu.device)\n",
    "            sigma_d = torch.clamp(mu[:, d] + epsilon_d, min=0, max=1)\n",
    "            sigma.append(sigma_d)\n",
    "\n",
    "        sigma = torch.stack(sigma, dim=1)\n",
    "\n",
    "        # Sample from the Bernoulli distribution to get the feature mask\n",
    "        selection_mask = torch.bernoulli(selection_prob)\n",
    "        epsilon = normal_dist.sample() # sample from standard normal distribution\n",
    "        z = torch.clamp( epsilon + selection_mask, 0, 1) # \n",
    "\n",
    "        # stochastic_gates = \n",
    "        print(\"##\"*90)\n",
    "        print(selection_prob)\n",
    "        print(\"##\"*90)\n",
    "        print(\"##\"*90)\n",
    "        print(selection_mask)\n",
    "        print(\"##\"*90)\n",
    "        # Apply mask to the input features\n",
    "        selected_features = X_train * selection_mask\n",
    "        s_tilda = sigma * mu  # Element-wise multiplication (Hadamard product)\n",
    "        # Make predictions with the masked features\n",
    "        y_pred = model(selected_features)\n",
    "        loss = custom_loss(y_pred, y_train, selection_mask) # TODO: Better loss function in progress\n",
    "        # Backpropagation\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        hypernet.eval()\n",
    "        with torch.no_grad():\n",
    "            val_selection_prob = hypernet(X_val)\n",
    "            val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "            val_selected_features = X_val * val_selection_mask\n",
    "            val_y_pred = model(val_selected_features)\n",
    "            # val_loss = criterion(val_y_pred, y_val)\n",
    "            val_loss = custom_loss(val_y_pred, y_val, val_selection_prob)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "hypernet = HyperNet(input_dim=X_train_tensor.shape[1])\n",
    "model = PredNet(feature_dim=X_train_tensor.shape[1])\n",
    "optimizer = Adam(list(hypernet.parameters()) + list(model.parameters()) , lr=0.01)\n",
    "\n",
    "# Training\n",
    "epochs = 2  # Adjust as necessary based on convergence and performance\n",
    "train(model, hypernet, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to train and evaluate models using STG algorithm\n",
    "def train_evaluate_stg(X_train, y_train, X_test, y_test, context_dim):\n",
    "    # Instantiate HyperNetwork and PredictionNetwork for STG\n",
    "    hypernetwork_stg = HyperNetwork(context_dim, X_train.shape[1] - context_dim)\n",
    "    prediction_network_stg = PredictionNetwork(X_train.shape[1] - context_dim)\n",
    "\n",
    "    # Train the model\n",
    "    stg_losses_train, stg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "                                                    hypernetwork_stg, prediction_network_stg)\n",
    "    return stg_losses_train, stg_losses_val\n",
    "\n",
    "# Define a function to train and evaluate models using CSTG algorithm\n",
    "def train_evaluate_cstg(X_train, y_train, X_test, y_test, context_dim):\n",
    "    # Instantiate ConditionalStochasticGates model\n",
    "    cstg_model = ConditionalStochasticGates(context_dim, X_train.shape[1] - context_dim)\n",
    "\n",
    "    # Train the model\n",
    "    cstg_losses_train, cstg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "                                                      cstg_model, prediction_network)\n",
    "    return cstg_losses_train, cstg_losses_val\n",
    "\n",
    "# Define a function to train the model and return training and validation losses\n",
    "def train_model(X_train, y_train, X_test, y_test, hypernetwork, prediction_network):\n",
    "    optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    num_epochs = 100\n",
    "    stg_losses_train, stg_losses_val = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training step\n",
    "        hypernetwork.train()\n",
    "        prediction_network.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        gates_train = hypernetwork(X_train[:, :context_dim])\n",
    "        selected_features_train = X_train[:, context_dim:] * stochastic_gates(gates_train)\n",
    "        predictions_train = prediction_network(selected_features_train)\n",
    "        loss_train = loss_fn(predictions_train, y_train)\n",
    "\n",
    "        # Backward pass\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            hypernetwork.eval()\n",
    "            prediction_network.eval()\n",
    "\n",
    "            gates_val = hypernetwork(X_test[:, :context_dim])\n",
    "            selected_features_val = X_test[:, context_dim:] * stochastic_gates(gates_val)\n",
    "            predictions_val = prediction_network(selected_features_val)\n",
    "            loss_val = loss_fn(predictions_val, y_test)\n",
    "\n",
    "        stg_losses_train.append(loss_train.item())\n",
    "        stg_losses_val.append(loss_val.item())\n",
    "\n",
    "    return stg_losses_train, stg_losses_val\n",
    "\n",
    "# Define the context dimension\n",
    "context_dim = 3  # Assuming the first 3 features are used as context\n",
    "\n",
    "# Train and evaluate models using STG algorithm\n",
    "stg_losses_train, stg_losses_val = train_evaluate_stg(X_train_tensor, y_train_tensor, \n",
    "                                                      X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# Train and evaluate models using CSTG algorithm\n",
    "cstg_losses_train, cstg_losses_val = train_evaluate_cstg(X_train_tensor, y_train_tensor, \n",
    "                                                          X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# Plot training and validation losses for STG and CSTG algorithms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), stg_losses_train, label='STG Train Loss', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), stg_losses_val, label='STG Validation Loss', linestyle='--', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), cstg_losses_train, label='CSTG Train Loss', color='orange')\n",
    "plt.plot(range(1, num_epochs + 1), cstg_losses_val, label='CSTG Validation Loss', linestyle='--', color='orange')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Assuming X_train_tensor and X_test_tensor are already defined and properly shaped\n",
    "# input_dim = X_train_tensor.shape[1]  # Number of features in the input data\n",
    "# feature_dim = input_dim  # The hypernetwork output and prediction input dimensions must match\n",
    "\n",
    "# # Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "# hypernet = HyperNet(input_dim)\n",
    "# model = PredNet(feature_dim)\n",
    "# optimizer = Adam(list(hypernet.parameters()) + list(model.parameters()) , lr=0.01)\n",
    "\n",
    "# # Training\n",
    "# epochs = 2  # Adjust as necessary based on convergence and performance\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "AveOccup: 0.44150832295417786\n",
      "MedInc: 0.5098904967308044\n",
      "Uniform_Noise: 0.474697470664978\n",
      "Cosine_Noise: 0.4606289863586426\n",
      "AveRooms: 0.47779104113578796\n",
      "HouseAge: 0.461354523897171\n",
      "Gaussian_Noise: 0.5596177577972412\n",
      "Population: 0.407277375459671\n",
      "Longitude: 0.5244668126106262\n",
      "AveBedrms: 0.4461166560649872\n",
      "Latitude: 0.5383712649345398\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with the trained models\n",
    "def make_inference(model, hypernet, X_test):\n",
    "    model.eval()\n",
    "    hypernet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the feature selection probabilities from the hypernetwork\n",
    "        selection_prob = hypernet(X_test)\n",
    "        # For inference use the expected values (probabilities) instead of sampling\n",
    "        selected_features = X_test * selection_prob\n",
    "        # Get the model predictions\n",
    "        predictions = model(selected_features)\n",
    "    return predictions, selection_prob\n",
    "\n",
    "# Load the test data (replace this with your actual test data)\n",
    "X_test = X_test_tensor  # Assuming X_test_tensor is your test data\n",
    "\n",
    "# Make inference\n",
    "predictions, feature_importance_probabilities = make_inference(model, hypernet, X_test)\n",
    "\n",
    "\n",
    "# Print the conditional probability of each feature being significant\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    print(f\"{feature_name}: {feature_importance_probabilities[:, i].mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "Gaussian_Noise: 0.5596177577972412\n",
      "Latitude: 0.5383712649345398\n",
      "Longitude: 0.5244668126106262\n",
      "MedInc: 0.5098904967308044\n",
      "AveRooms: 0.47779104113578796\n",
      "Uniform_Noise: 0.474697470664978\n",
      "HouseAge: 0.461354523897171\n",
      "Cosine_Noise: 0.4606289863586426\n",
      "AveBedrms: 0.4461166560649872\n",
      "AveOccup: 0.44150832295417786\n",
      "Population: 0.407277375459671\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples (feature_name, probability)\n",
    "feature_probabilities = [(feature_name, feature_importance_probabilities[:, i].mean().item()) for i, feature_name in enumerate(feature_names)]\n",
    "\n",
    "# Sort the list by probability in descending order\n",
    "sorted_feature_probabilities = sorted(feature_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted list\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for feature, probability in sorted_feature_probabilities:\n",
    "    print(f\"{feature}: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mean_feature_importance_probabilities \u001b[38;5;241m=\u001b[39m feature_importance_probabilities\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Determine which features have a mean selection probability above the threshold\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m important_features_indices \u001b[38;5;241m=\u001b[39m mean_feature_importance_probabilities \u001b[38;5;241m>\u001b[39m \u001b[43mthreshold\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean feature selection probabilities from the hypernetwork (higher means more important):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_feature_importance_probabilities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the mean feature importance probability across all examples\n",
    "mean_feature_importance_probabilities = feature_importance_probabilities.mean(dim=0)\n",
    "\n",
    "# Determine which features have a mean selection probability above the threshold\n",
    "important_features_indices = mean_feature_importance_probabilities > threshold\n",
    "\n",
    "print(\"Mean feature selection probabilities from the hypernetwork (higher means more important):\")\n",
    "print(mean_feature_importance_probabilities)\n",
    "\n",
    "print(\"\\nFeatures with mean selection probability higher than threshold:\")\n",
    "print(important_features_indices)\n",
    "# Extract the names of the important features\n",
    "important_feature_names = [name for i, name in enumerate(feature_names) if important_features_indices[i]]\n",
    "\n",
    "print(\"\\nImportant feature names:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters of HyperNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1502, -0.1104, -0.0366,  ...,  0.1718,  0.0637, -0.0697],\n",
      "        [-0.1804,  0.0159, -0.1112,  ...,  0.1196, -0.0227, -0.0770],\n",
      "        [ 0.1346,  0.1256,  0.1248,  ..., -0.0737, -0.1655, -0.1717],\n",
      "        ...,\n",
      "        [-0.0565,  0.0891,  0.0982,  ..., -0.1358, -0.1359,  0.0080],\n",
      "        [-0.0074, -0.0703,  0.0896,  ...,  0.2032, -0.2027, -0.1844],\n",
      "        [-0.0535,  0.0381,  0.0786,  ..., -0.1920, -0.0853, -0.0579]])\n",
      "fc1.bias:\n",
      "tensor([ 0.1512,  0.2043,  0.1095, -0.1715, -0.0402, -0.1496, -0.2718,  0.2931,\n",
      "         0.1333,  0.1599,  0.0045, -0.1857, -0.2328,  0.0870,  0.1531, -0.2914,\n",
      "         0.0040,  0.2220, -0.0121, -0.1919,  0.0696, -0.0186,  0.0074,  0.0958,\n",
      "        -0.0544,  0.2858, -0.1110, -0.0218,  0.0621, -0.1147, -0.2755,  0.1071,\n",
      "         0.2763,  0.1478, -0.2083,  0.2526,  0.0627, -0.0947, -0.2117, -0.2135,\n",
      "         0.1645, -0.2223,  0.1970, -0.1847, -0.0560, -0.2816, -0.2027, -0.0012,\n",
      "         0.2529,  0.2172, -0.0118, -0.2407,  0.2348, -0.2373, -0.1128,  0.0839,\n",
      "        -0.1592,  0.2953,  0.1130,  0.0883,  0.2779, -0.0367,  0.1327, -0.0183,\n",
      "        -0.0516, -0.2265, -0.0095, -0.2309,  0.1992, -0.2779, -0.0978, -0.0260,\n",
      "        -0.2853,  0.1937,  0.2322,  0.2955, -0.0263, -0.2422,  0.2143, -0.1737,\n",
      "        -0.0019,  0.0418, -0.2932,  0.1785,  0.2519,  0.1678,  0.2759, -0.2777,\n",
      "        -0.1369, -0.0138,  0.2617, -0.2685, -0.0044,  0.1171, -0.0095,  0.1542,\n",
      "        -0.1603, -0.1790,  0.1629,  0.0338, -0.1578, -0.2427,  0.0014, -0.1207,\n",
      "        -0.1155, -0.0885, -0.1136, -0.1674, -0.1247, -0.2480, -0.1872, -0.1625,\n",
      "        -0.1832,  0.2756, -0.2429,  0.0740, -0.1579,  0.0388, -0.2921, -0.1844,\n",
      "        -0.0605, -0.0986, -0.1727, -0.1732, -0.0819,  0.0992,  0.0258,  0.1645])\n",
      "fc2.weight:\n",
      "tensor([[-0.0785, -0.0891,  0.1382,  ..., -0.0968, -0.0798,  0.0083],\n",
      "        [ 0.1183, -0.1070, -0.1937,  ..., -0.0883, -0.1277, -0.0018],\n",
      "        [-0.1392, -0.2076, -0.0042,  ...,  0.0864, -0.1327, -0.0715],\n",
      "        ...,\n",
      "        [ 0.1834,  0.0056,  0.1638,  ...,  0.0071, -0.0697,  0.1988],\n",
      "        [-0.1742,  0.1368,  0.1087,  ..., -0.1041,  0.0640, -0.1027],\n",
      "        [-0.0524, -0.1429,  0.0062,  ..., -0.1828, -0.0921,  0.1280]])\n",
      "fc2.bias:\n",
      "tensor([-0.0545, -0.0791,  0.0383,  0.0282,  0.0775,  0.0259, -0.0079, -0.0723,\n",
      "         0.0538, -0.0139, -0.0712])\n",
      "\n",
      "Final parameters of PredNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1951,  0.1289,  0.0336,  ...,  0.0818,  0.1140, -0.0908],\n",
      "        [-0.1356,  0.0355,  0.2034,  ...,  0.0818,  0.1826, -0.0745],\n",
      "        [ 0.1945,  0.0366,  0.1083,  ...,  0.1903,  0.1251,  0.1758],\n",
      "        ...,\n",
      "        [-0.0572, -0.0295, -0.1143,  ...,  0.0377,  0.0034,  0.0813],\n",
      "        [ 0.0804,  0.0441,  0.0599,  ...,  0.0587, -0.0560, -0.1215],\n",
      "        [-0.1221, -0.2030,  0.0446,  ..., -0.1810, -0.0804, -0.0874]])\n",
      "fc1.bias:\n",
      "tensor([ 0.0847,  0.2508,  0.0543, -0.1693, -0.2101,  0.0284, -0.1087, -0.0365,\n",
      "        -0.1931, -0.0249,  0.1657, -0.0997,  0.0687, -0.1125, -0.2778,  0.0163,\n",
      "        -0.2127, -0.2116, -0.1091,  0.1054,  0.2549,  0.2965,  0.0145,  0.0660,\n",
      "        -0.1624,  0.2841,  0.0210, -0.0436, -0.1952,  0.2271,  0.2290, -0.2897,\n",
      "         0.2864, -0.2730,  0.2922,  0.0786,  0.2531,  0.1794, -0.1115, -0.2798,\n",
      "         0.0356, -0.1551,  0.0831,  0.2643, -0.1746,  0.2181,  0.0653,  0.0814,\n",
      "        -0.0537,  0.0539,  0.2520,  0.0199,  0.1531,  0.0614,  0.2748, -0.3002,\n",
      "         0.1219, -0.2122, -0.1631, -0.1681, -0.0185,  0.0782, -0.1613, -0.2044,\n",
      "        -0.0142,  0.0375, -0.1722, -0.1976, -0.0613, -0.0207,  0.0363, -0.1575,\n",
      "        -0.0928,  0.1729,  0.2558, -0.1558,  0.2242,  0.2890,  0.2902,  0.0747,\n",
      "        -0.0839, -0.0538, -0.0766, -0.2373,  0.0298, -0.0279,  0.1766,  0.1254,\n",
      "        -0.0312, -0.1889,  0.2507,  0.1399, -0.2916, -0.0606,  0.2620,  0.2371,\n",
      "         0.0271,  0.2982,  0.0881, -0.1257, -0.1925,  0.1016,  0.2289,  0.2053,\n",
      "         0.1121, -0.1936,  0.0434, -0.0033,  0.0369,  0.2812, -0.0046, -0.2101,\n",
      "         0.1726,  0.1283,  0.1392, -0.2868,  0.0306,  0.2251, -0.0042,  0.2264,\n",
      "         0.0210, -0.2270,  0.0823,  0.1144,  0.1638,  0.2373, -0.0120,  0.1358])\n",
      "fc2.weight:\n",
      "tensor([[-0.0340, -0.0058, -0.0467,  0.0282, -0.0404, -0.0728,  0.0139, -0.0756,\n",
      "          0.0479,  0.0861,  0.0223, -0.0131, -0.0110, -0.0324, -0.0306, -0.0440,\n",
      "         -0.0814,  0.0595,  0.0548, -0.0188,  0.0324, -0.0526, -0.0685,  0.0643,\n",
      "          0.0150,  0.0643, -0.0713,  0.0403,  0.0188,  0.0580,  0.0643, -0.0183,\n",
      "         -0.0688,  0.0253, -0.0434,  0.0338, -0.0089,  0.0365, -0.0424, -0.0007,\n",
      "         -0.0031,  0.0186,  0.0339, -0.0069, -0.0783, -0.0002,  0.0574, -0.0139,\n",
      "         -0.0260,  0.0144,  0.0535,  0.0597,  0.0617,  0.0504,  0.0844,  0.0623,\n",
      "         -0.0482,  0.0821, -0.0063,  0.0366, -0.0527, -0.0601,  0.0478,  0.0693,\n",
      "         -0.0706, -0.0636, -0.0599,  0.0776,  0.0848,  0.0028,  0.0122, -0.0097,\n",
      "          0.0428, -0.0329, -0.0786, -0.0789, -0.0787, -0.0558, -0.0111,  0.0623,\n",
      "          0.0209,  0.0124, -0.0683, -0.0567, -0.0454, -0.0577, -0.0457, -0.0199,\n",
      "         -0.0357,  0.0092, -0.0231, -0.0800,  0.0424, -0.0070, -0.0331, -0.0531,\n",
      "         -0.0198, -0.0406, -0.0228,  0.0601, -0.0557, -0.0754,  0.0691,  0.0722,\n",
      "          0.0143,  0.0155,  0.0684, -0.0660, -0.0323,  0.0408,  0.0286, -0.0627,\n",
      "          0.0069,  0.0855, -0.0040, -0.0363,  0.0698,  0.0618,  0.0673,  0.0820,\n",
      "          0.0318,  0.0011, -0.0143, -0.0545,  0.0625,  0.0356, -0.0055, -0.0379]])\n",
      "fc2.bias:\n",
      "tensor([0.0553])\n"
     ]
    }
   ],
   "source": [
    "# ... (after the training loop)\n",
    "\n",
    "# Retrieve the final parameters of HyperNet\n",
    "hypernet_params = hypernet.state_dict()\n",
    "print(\"Final parameters of HyperNet:\")\n",
    "for param_name, param in hypernet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n",
    "\n",
    "# Retrieve the final parameters of PredNet\n",
    "prednet_params = model.state_dict()\n",
    "print(\"\\nFinal parameters of PredNet:\")\n",
    "for param_name, param in prednet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise error\n",
    "raise ValueError(\"This is a custom error message. You can replace this with an actual error message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     53\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not DataFrame"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, input_dim)  # The output should match the number of features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = X_train_tensor.shape[1]  # This should be the number of features in your input\n",
    "feature_dim = input_dim  # This should match the number of input features\n",
    "hypernet = HyperNet(input_dim)\n",
    "model = PredNet(feature_dim)\n",
    "\n",
    "# Rest of the initialization and training code...\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(list(model.parameters()) + list(hypernet.parameters()), lr=0.001)\n",
    "\n",
    "hypernet.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass through hypernetwork to get selection probabilities\n",
    "selection_prob = hypernet(X_train)\n",
    "# Sample from Bernoulli distribution to get feature mask\n",
    "selection_mask = torch.bernoulli(selection_prob)\n",
    "# Apply mask to input features\n",
    "selected_features = X_train * selection_mask\n",
    "\n",
    "# Make predictions with masked features\n",
    "y_pred = model(selected_features)\n",
    "loss = criterion(y_pred, y_train)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Validation phase\n",
    "model.eval()\n",
    "hypernet.eval()\n",
    "# with torch.no_grad():\n",
    "#     val_selection_prob = hypernet(X_val)\n",
    "#     val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "#     val_selected_features = X_val * val_selection_mask\n",
    "#     val_y_pred = model(val_selected_features)\n",
    "#     val_loss = criterion(val_y_pred, y_val)\n",
    "\n",
    "# print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Model initialization\n",
    "\n",
    "\n",
    "# Training\n",
    "# epochs = 100 # Adjust as necessary\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    119\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Adjust as necessary\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     86\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the Hypernetwork\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         # Initialize weights using Xavier initialization\n",
    "#         xavier_uniform_(self.fc1.weight)\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         xavier_uniform_(self.fc2.weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # Final layer with sigmoid to ensure outputs are between 0 and 1\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, feature_dim)  # feature_dim should match the number of input features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the Prediction Network\n",
    "# class PredNet(nn.Module):\n",
    "#     def __init__(self, feature_dim):\n",
    "#         super(PredNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(feature_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, 1) # Assuming a single output\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)  # feature_dim should match the number of input features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = 11\n",
    "feature_dim = input_dim  # Make sure this matches the number of input features\n",
    "hypernet = HyperNet(input_dim, feature_dim)\n",
    "model = PredNet(feature_dim)\n",
    "def train(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.bernoulli import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Hypernetwork, self).__init__()\n",
    "        # Define the architecture of the hypernetwork.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer mapping to Bernoulli parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = torch.sigmoid(self.fc2(x)) # Sigmoid to ensure output is in [0,1], representing probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        # Define the architecture of the prediction network.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer for the response variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = self.fc2(x) # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hypernet, criterion, optimizer, data_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for x, z, y in data_loader: # Assuming x is the feature, z is the context, y is the target\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate probabilities from the Hypernetwork\n",
    "            probs = hypernet(z)\n",
    "            \n",
    "            # Sample from Bernoulli to get the feature selection mask\n",
    "            m = Bernoulli(probs)\n",
    "            mask = m.sample()\n",
    "            \n",
    "            # Apply mask and predict\n",
    "            x_masked = x * mask\n",
    "            predictions = model(x_masked)\n",
    "            \n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
