{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Uniform_Noise</th>\n",
       "      <th>Cosine_Values</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Gaussian_Noise</th>\n",
       "      <th>Population</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.247299</td>\n",
       "      <td>4.5625</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>-0.958842</td>\n",
       "      <td>4.845138</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-0.706843</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>-122.59</td>\n",
       "      <td>1.027611</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.267930</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>-0.134879</td>\n",
       "      <td>0.105413</td>\n",
       "      <td>5.262517</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.869742</td>\n",
       "      <td>2415.0</td>\n",
       "      <td>-119.19</td>\n",
       "      <td>1.012179</td>\n",
       "      <td>34.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.445217</td>\n",
       "      <td>5.2174</td>\n",
       "      <td>0.784740</td>\n",
       "      <td>0.261862</td>\n",
       "      <td>7.306957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.116566</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>-117.21</td>\n",
       "      <td>1.078261</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.345733</td>\n",
       "      <td>2.3083</td>\n",
       "      <td>-0.577435</td>\n",
       "      <td>-0.877524</td>\n",
       "      <td>5.485777</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.198482</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-122.63</td>\n",
       "      <td>1.262582</td>\n",
       "      <td>38.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.496000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.833185</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>5.442667</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.133279</td>\n",
       "      <td>936.0</td>\n",
       "      <td>-117.24</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>34.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveOccup  MedInc  Uniform_Noise  Cosine_Values  AveRooms  HouseAge  \\\n",
       "0  2.247299  4.5625       0.760274      -0.958842  4.845138      46.0   \n",
       "1  3.267930  4.5000      -0.134879       0.105413  5.262517      17.0   \n",
       "2  3.445217  5.2174       0.784740       0.261862  7.306957       5.0   \n",
       "3  2.345733  2.3083      -0.577435      -0.877524  5.485777      20.0   \n",
       "4  2.496000  6.0000       0.833185       0.042673  5.442667      26.0   \n",
       "\n",
       "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
       "0       -0.706843      1872.0    -122.59   1.027611     37.97  \n",
       "1       -1.869742      2415.0    -119.19   1.012179     34.23  \n",
       "2        0.116566      3962.0    -117.21   1.078261     33.95  \n",
       "3        1.198482      1072.0    -122.63   1.262582     38.96  \n",
       "4       -0.133279       936.0    -117.24   0.781333     34.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Values'] = cosine_values\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "display(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Define the Hypernetwork\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.dropout1 = nn.Dropout(0.4)   # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.dropout2 = nn.Dropout(0.4)   # Another Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        xavier_uniform_(self.fc3.weight)  # Xavier initialization\n",
    "        self.fc4 = nn.Linear(32, input_dim)\n",
    "        xavier_uniform_(self.fc4.weight)  # Xavier initialization\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))  # Sigmoid to ensure output is a probability\n",
    "        return x\n",
    "\n",
    "# Define the Prediction Network\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        # xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.dropout2 = nn.Dropout(0.5)   # Another Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Applying dropout after activation\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the custom loss function incorporating both MSE and regularization term\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg, sigma):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.sigma = sigma\n",
    "        self.normal_dist = Normal(torch.tensor([0.0]), torch.tensor([sigma]))\n",
    "\n",
    "    def forward(self, y_pred, y_true, selection_probs):\n",
    "        # Calculate the MSE part\n",
    "        mse = self.mse_loss(y_pred, y_true)\n",
    "        \n",
    "        # Calculate the regularization term\n",
    "        # Use the CDF of the standard Gaussian distribution\n",
    "        regularization_term = torch.sum(self.normal_dist.cdf(selection_probs)) / self.sigma\n",
    "        \n",
    "        # Combine the MSE and regularization term\n",
    "        total_loss = mse + self.lambda_reg * regularization_term\n",
    "        return total_loss\n",
    "\n",
    "# Usage of CustomLoss\n",
    "# Set lambda_reg and sigma based on your requirements\n",
    "lambda_reg = 0.01  # Example value; needs to be tuned\n",
    "sigma = 1.0  # Assuming noise standard deviation is 1\n",
    "\n",
    "# Initialize the custom loss\n",
    "custom_loss = CustomLoss(lambda_reg, sigma)\n",
    "\n",
    "# During training, you would use this custom_loss function like so:\n",
    "# custom_loss(y_pred, y_true, selection_probs)\n",
    "\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hypernet.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the hypernetwork to get selection probabilities\n",
    "        selection_prob = hypernet(X_train)\n",
    "        # Sample from the Bernoulli distribution to get the feature mask\n",
    "        selection_mask = torch.bernoulli(selection_prob)\n",
    "        # Apply mask to the input features\n",
    "        selected_features = X_train * selection_mask\n",
    "        \n",
    "        # Make predictions with the masked features\n",
    "        y_pred = model(selected_features)\n",
    "        # loss = criterion(y_pred, y_train)\n",
    "        loss = custom_loss(y_pred, y_train, selection_prob)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward() # TODO: MAKE AN ACCURATELOSS function\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        hypernet.eval()\n",
    "        with torch.no_grad():\n",
    "            val_selection_prob = hypernet(X_val)\n",
    "            val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "            val_selected_features = X_val * val_selection_mask\n",
    "            val_y_pred = model(val_selected_features)\n",
    "            # val_loss = criterion(val_y_pred, y_val)\n",
    "            val_loss =custom_loss(val_y_pred, y_val, val_selection_prob)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1254.8494873046875, Val Loss: 317.346435546875\n",
      "Epoch 2, Loss: 1249.74853515625, Val Loss: 315.94317626953125\n",
      "Epoch 3, Loss: 1244.4898681640625, Val Loss: 314.4925537109375\n",
      "Epoch 4, Loss: 1239.2120361328125, Val Loss: 312.98828125\n",
      "Epoch 5, Loss: 1233.3814697265625, Val Loss: 311.4148864746094\n",
      "Epoch 6, Loss: 1227.45751953125, Val Loss: 309.800537109375\n",
      "Epoch 7, Loss: 1221.283203125, Val Loss: 308.1207275390625\n",
      "Epoch 8, Loss: 1214.66796875, Val Loss: 306.35546875\n",
      "Epoch 9, Loss: 1207.836181640625, Val Loss: 304.5390930175781\n",
      "Epoch 10, Loss: 1200.76123046875, Val Loss: 302.6092224121094\n",
      "Epoch 11, Loss: 1193.238525390625, Val Loss: 300.62664794921875\n",
      "Epoch 12, Loss: 1185.23193359375, Val Loss: 298.5159912109375\n",
      "Epoch 13, Loss: 1177.3048095703125, Val Loss: 296.3395690917969\n",
      "Epoch 14, Loss: 1168.626953125, Val Loss: 294.0638122558594\n",
      "Epoch 15, Loss: 1160.218994140625, Val Loss: 291.6771240234375\n",
      "Epoch 16, Loss: 1151.322509765625, Val Loss: 289.1874084472656\n",
      "Epoch 17, Loss: 1141.9217529296875, Val Loss: 286.65789794921875\n",
      "Epoch 18, Loss: 1132.5533447265625, Val Loss: 284.05303955078125\n",
      "Epoch 19, Loss: 1122.6802978515625, Val Loss: 281.3468322753906\n",
      "Epoch 20, Loss: 1112.8900146484375, Val Loss: 278.5611572265625\n",
      "Epoch 21, Loss: 1102.8685302734375, Val Loss: 275.7934265136719\n",
      "Epoch 22, Loss: 1093.1375732421875, Val Loss: 272.9911804199219\n",
      "Epoch 23, Loss: 1082.7493896484375, Val Loss: 270.17352294921875\n",
      "Epoch 24, Loss: 1072.93896484375, Val Loss: 267.39794921875\n",
      "Epoch 25, Loss: 1063.1328125, Val Loss: 264.6401062011719\n",
      "Epoch 26, Loss: 1052.8609619140625, Val Loss: 261.92828369140625\n",
      "Epoch 27, Loss: 1043.4327392578125, Val Loss: 259.280029296875\n",
      "Epoch 28, Loss: 1033.6343994140625, Val Loss: 256.6793212890625\n",
      "Epoch 29, Loss: 1024.34619140625, Val Loss: 254.18141174316406\n",
      "Epoch 30, Loss: 1014.7964477539062, Val Loss: 251.76699829101562\n",
      "Epoch 31, Loss: 1006.3051147460938, Val Loss: 249.47073364257812\n",
      "Epoch 32, Loss: 997.505615234375, Val Loss: 247.23915100097656\n",
      "Epoch 33, Loss: 989.6510620117188, Val Loss: 245.19078063964844\n",
      "Epoch 34, Loss: 981.645751953125, Val Loss: 243.21617126464844\n",
      "Epoch 35, Loss: 973.9301147460938, Val Loss: 241.40341186523438\n",
      "Epoch 36, Loss: 967.1646118164062, Val Loss: 239.71322631835938\n",
      "Epoch 37, Loss: 960.5865478515625, Val Loss: 238.19268798828125\n",
      "Epoch 38, Loss: 954.6184692382812, Val Loss: 236.81097412109375\n",
      "Epoch 39, Loss: 949.0509643554688, Val Loss: 235.56854248046875\n",
      "Epoch 40, Loss: 943.9148559570312, Val Loss: 234.4723358154297\n",
      "Epoch 41, Loss: 939.3576049804688, Val Loss: 233.5189971923828\n",
      "Epoch 42, Loss: 935.4938354492188, Val Loss: 232.68707275390625\n",
      "Epoch 43, Loss: 931.963134765625, Val Loss: 231.9859161376953\n",
      "Epoch 44, Loss: 928.57177734375, Val Loss: 231.39907836914062\n",
      "Epoch 45, Loss: 926.0992431640625, Val Loss: 230.90162658691406\n",
      "Epoch 46, Loss: 923.7373046875, Val Loss: 230.47836303710938\n",
      "Epoch 47, Loss: 921.7010498046875, Val Loss: 230.1251220703125\n",
      "Epoch 48, Loss: 920.0784912109375, Val Loss: 229.82752990722656\n",
      "Epoch 49, Loss: 918.650146484375, Val Loss: 229.591064453125\n",
      "Epoch 50, Loss: 917.4490966796875, Val Loss: 229.39981079101562\n",
      "Epoch 51, Loss: 916.4572143554688, Val Loss: 229.2309112548828\n",
      "Epoch 52, Loss: 915.5601806640625, Val Loss: 229.09896850585938\n",
      "Epoch 53, Loss: 914.790283203125, Val Loss: 228.9871826171875\n",
      "Epoch 54, Loss: 914.2269287109375, Val Loss: 228.89476013183594\n",
      "Epoch 55, Loss: 913.7552490234375, Val Loss: 228.81646728515625\n",
      "Epoch 56, Loss: 913.2999877929688, Val Loss: 228.75563049316406\n",
      "Epoch 57, Loss: 912.8812866210938, Val Loss: 228.70263671875\n",
      "Epoch 58, Loss: 912.6485595703125, Val Loss: 228.65879821777344\n",
      "Epoch 59, Loss: 912.3236083984375, Val Loss: 228.6241455078125\n",
      "Epoch 60, Loss: 912.1224365234375, Val Loss: 228.5957794189453\n",
      "Epoch 61, Loss: 911.8651123046875, Val Loss: 228.56686401367188\n",
      "Epoch 62, Loss: 911.7168579101562, Val Loss: 228.54600524902344\n",
      "Epoch 63, Loss: 911.5829467773438, Val Loss: 228.52638244628906\n",
      "Epoch 64, Loss: 911.4169921875, Val Loss: 228.51048278808594\n",
      "Epoch 65, Loss: 911.3031005859375, Val Loss: 228.49456787109375\n",
      "Epoch 66, Loss: 911.1427612304688, Val Loss: 228.48089599609375\n",
      "Epoch 67, Loss: 911.0765991210938, Val Loss: 228.46987915039062\n",
      "Epoch 68, Loss: 910.9906616210938, Val Loss: 228.46327209472656\n",
      "Epoch 69, Loss: 910.9244384765625, Val Loss: 228.45443725585938\n",
      "Epoch 70, Loss: 910.8599853515625, Val Loss: 228.44805908203125\n",
      "Epoch 71, Loss: 910.7938842773438, Val Loss: 228.4440460205078\n",
      "Epoch 72, Loss: 910.76025390625, Val Loss: 228.4376678466797\n",
      "Epoch 73, Loss: 910.7019653320312, Val Loss: 228.4339599609375\n",
      "Epoch 74, Loss: 910.6268920898438, Val Loss: 228.43295288085938\n",
      "Epoch 75, Loss: 910.615234375, Val Loss: 228.43072509765625\n",
      "Epoch 76, Loss: 910.578857421875, Val Loss: 228.42837524414062\n",
      "Epoch 77, Loss: 910.598388671875, Val Loss: 228.42698669433594\n",
      "Epoch 78, Loss: 910.5452880859375, Val Loss: 228.42501831054688\n",
      "Epoch 79, Loss: 910.4592895507812, Val Loss: 228.42384338378906\n",
      "Epoch 80, Loss: 910.4636840820312, Val Loss: 228.42276000976562\n",
      "Epoch 81, Loss: 910.4528198242188, Val Loss: 228.42138671875\n",
      "Epoch 82, Loss: 910.4203491210938, Val Loss: 228.4202880859375\n",
      "Epoch 83, Loss: 910.4058227539062, Val Loss: 228.41871643066406\n",
      "Epoch 84, Loss: 910.3723754882812, Val Loss: 228.41668701171875\n",
      "Epoch 85, Loss: 910.3798217773438, Val Loss: 228.41448974609375\n",
      "Epoch 86, Loss: 910.356201171875, Val Loss: 228.4126434326172\n",
      "Epoch 87, Loss: 910.322021484375, Val Loss: 228.4108123779297\n",
      "Epoch 88, Loss: 910.3182373046875, Val Loss: 228.40951538085938\n",
      "Epoch 89, Loss: 910.310302734375, Val Loss: 228.40606689453125\n",
      "Epoch 90, Loss: 910.2835693359375, Val Loss: 228.40557861328125\n",
      "Epoch 91, Loss: 910.2702026367188, Val Loss: 228.40379333496094\n",
      "Epoch 92, Loss: 910.2738037109375, Val Loss: 228.40206909179688\n",
      "Epoch 93, Loss: 910.2694702148438, Val Loss: 228.39971923828125\n",
      "Epoch 94, Loss: 910.2291870117188, Val Loss: 228.39816284179688\n",
      "Epoch 95, Loss: 910.2077026367188, Val Loss: 228.39678955078125\n",
      "Epoch 96, Loss: 910.2172241210938, Val Loss: 228.3950958251953\n",
      "Epoch 97, Loss: 910.2091064453125, Val Loss: 228.39373779296875\n",
      "Epoch 98, Loss: 910.1871948242188, Val Loss: 228.39210510253906\n",
      "Epoch 99, Loss: 910.2025756835938, Val Loss: 228.39134216308594\n",
      "Epoch 100, Loss: 910.1974487304688, Val Loss: 228.3881072998047\n",
      "Epoch 101, Loss: 910.1823120117188, Val Loss: 228.3884735107422\n",
      "Epoch 102, Loss: 910.1593627929688, Val Loss: 228.38685607910156\n",
      "Epoch 103, Loss: 910.174560546875, Val Loss: 228.3863525390625\n",
      "Epoch 104, Loss: 910.1592407226562, Val Loss: 228.3855438232422\n",
      "Epoch 105, Loss: 910.165283203125, Val Loss: 228.38424682617188\n",
      "Epoch 106, Loss: 910.1557006835938, Val Loss: 228.38446044921875\n",
      "Epoch 107, Loss: 910.1506958007812, Val Loss: 228.38287353515625\n",
      "Epoch 108, Loss: 910.1306762695312, Val Loss: 228.38230895996094\n",
      "Epoch 109, Loss: 910.11083984375, Val Loss: 228.38214111328125\n",
      "Epoch 110, Loss: 910.1248779296875, Val Loss: 228.38169860839844\n",
      "Epoch 111, Loss: 910.11962890625, Val Loss: 228.38124084472656\n",
      "Epoch 112, Loss: 910.1033935546875, Val Loss: 228.38072204589844\n",
      "Epoch 113, Loss: 910.1141967773438, Val Loss: 228.38095092773438\n",
      "Epoch 114, Loss: 910.094970703125, Val Loss: 228.37884521484375\n",
      "Epoch 115, Loss: 910.1150512695312, Val Loss: 228.38075256347656\n",
      "Epoch 116, Loss: 910.0767211914062, Val Loss: 228.3799591064453\n",
      "Epoch 117, Loss: 910.0911865234375, Val Loss: 228.38052368164062\n",
      "Epoch 118, Loss: 910.0899658203125, Val Loss: 228.3807830810547\n",
      "Epoch 119, Loss: 910.082763671875, Val Loss: 228.38063049316406\n",
      "Epoch 120, Loss: 910.064208984375, Val Loss: 228.37940979003906\n",
      "Epoch 121, Loss: 910.0697631835938, Val Loss: 228.37973022460938\n",
      "Epoch 122, Loss: 910.0512084960938, Val Loss: 228.37986755371094\n",
      "Epoch 123, Loss: 910.0535888671875, Val Loss: 228.37948608398438\n",
      "Epoch 124, Loss: 910.046142578125, Val Loss: 228.37911987304688\n",
      "Epoch 125, Loss: 910.045654296875, Val Loss: 228.37864685058594\n",
      "Epoch 126, Loss: 910.0419921875, Val Loss: 228.37818908691406\n",
      "Epoch 127, Loss: 910.0211791992188, Val Loss: 228.37815856933594\n",
      "Epoch 128, Loss: 910.0577392578125, Val Loss: 228.37771606445312\n",
      "Epoch 129, Loss: 910.0302734375, Val Loss: 228.37721252441406\n",
      "Epoch 130, Loss: 910.0155029296875, Val Loss: 228.37681579589844\n",
      "Epoch 131, Loss: 910.0294799804688, Val Loss: 228.37655639648438\n",
      "Epoch 132, Loss: 910.0122680664062, Val Loss: 228.3759307861328\n",
      "Epoch 133, Loss: 910.0109252929688, Val Loss: 228.3756561279297\n",
      "Epoch 134, Loss: 910.0194091796875, Val Loss: 228.3754119873047\n",
      "Epoch 135, Loss: 909.9961547851562, Val Loss: 228.37503051757812\n",
      "Epoch 136, Loss: 910.014892578125, Val Loss: 228.37429809570312\n",
      "Epoch 137, Loss: 910.0031127929688, Val Loss: 228.3739013671875\n",
      "Epoch 138, Loss: 910.0075073242188, Val Loss: 228.37405395507812\n",
      "Epoch 139, Loss: 910.0038452148438, Val Loss: 228.37513732910156\n",
      "Epoch 140, Loss: 909.9873046875, Val Loss: 228.37347412109375\n",
      "Epoch 141, Loss: 910.00927734375, Val Loss: 228.3732452392578\n",
      "Epoch 142, Loss: 909.9851684570312, Val Loss: 228.37315368652344\n",
      "Epoch 143, Loss: 909.988525390625, Val Loss: 228.3729248046875\n",
      "Epoch 144, Loss: 909.97802734375, Val Loss: 228.3730926513672\n",
      "Epoch 145, Loss: 909.986572265625, Val Loss: 228.37274169921875\n",
      "Epoch 146, Loss: 909.9708251953125, Val Loss: 228.37205505371094\n",
      "Epoch 147, Loss: 909.9933471679688, Val Loss: 228.37246704101562\n",
      "Epoch 148, Loss: 909.9673461914062, Val Loss: 228.3723907470703\n",
      "Epoch 149, Loss: 909.9775390625, Val Loss: 228.3724822998047\n",
      "Epoch 150, Loss: 909.9488525390625, Val Loss: 228.37203979492188\n",
      "Epoch 151, Loss: 909.9757080078125, Val Loss: 228.3721160888672\n",
      "Epoch 152, Loss: 909.9683837890625, Val Loss: 228.37188720703125\n",
      "Epoch 153, Loss: 909.9650268554688, Val Loss: 228.37179565429688\n",
      "Epoch 154, Loss: 909.9735717773438, Val Loss: 228.37159729003906\n",
      "Epoch 155, Loss: 909.9586181640625, Val Loss: 228.37132263183594\n",
      "Epoch 156, Loss: 909.963134765625, Val Loss: 228.3717498779297\n",
      "Epoch 157, Loss: 909.9352416992188, Val Loss: 228.3710479736328\n",
      "Epoch 158, Loss: 909.9476318359375, Val Loss: 228.37095642089844\n",
      "Epoch 159, Loss: 909.9280395507812, Val Loss: 228.37069702148438\n",
      "Epoch 160, Loss: 909.9382934570312, Val Loss: 228.37054443359375\n",
      "Epoch 161, Loss: 909.9314575195312, Val Loss: 228.37014770507812\n",
      "Epoch 162, Loss: 909.9473266601562, Val Loss: 228.37039184570312\n",
      "Epoch 163, Loss: 909.9335327148438, Val Loss: 228.36949157714844\n",
      "Epoch 164, Loss: 909.9425048828125, Val Loss: 228.36956787109375\n",
      "Epoch 165, Loss: 909.9309692382812, Val Loss: 228.36947631835938\n",
      "Epoch 166, Loss: 909.9224243164062, Val Loss: 228.36912536621094\n",
      "Epoch 167, Loss: 909.9136962890625, Val Loss: 228.36904907226562\n",
      "Epoch 168, Loss: 909.92724609375, Val Loss: 228.3692626953125\n",
      "Epoch 169, Loss: 909.91650390625, Val Loss: 228.3687286376953\n",
      "Epoch 170, Loss: 909.9240112304688, Val Loss: 228.36978149414062\n",
      "Epoch 171, Loss: 909.9107055664062, Val Loss: 228.36785888671875\n",
      "Epoch 172, Loss: 909.9232177734375, Val Loss: 228.36813354492188\n",
      "Epoch 173, Loss: 909.9090576171875, Val Loss: 228.367919921875\n",
      "Epoch 174, Loss: 909.9169311523438, Val Loss: 228.36769104003906\n",
      "Epoch 175, Loss: 909.912353515625, Val Loss: 228.3675079345703\n",
      "Epoch 176, Loss: 909.9302368164062, Val Loss: 228.36805725097656\n",
      "Epoch 177, Loss: 909.9152221679688, Val Loss: 228.367431640625\n",
      "Epoch 178, Loss: 909.8959350585938, Val Loss: 228.36685180664062\n",
      "Epoch 179, Loss: 909.9009399414062, Val Loss: 228.3673095703125\n",
      "Epoch 180, Loss: 909.9022827148438, Val Loss: 228.36778259277344\n",
      "Epoch 181, Loss: 909.9159545898438, Val Loss: 228.36761474609375\n",
      "Epoch 182, Loss: 909.8958129882812, Val Loss: 228.3672332763672\n",
      "Epoch 183, Loss: 909.90234375, Val Loss: 228.36717224121094\n",
      "Epoch 184, Loss: 909.8745727539062, Val Loss: 228.36709594726562\n",
      "Epoch 185, Loss: 909.8596801757812, Val Loss: 228.3669891357422\n",
      "Epoch 186, Loss: 909.877197265625, Val Loss: 228.36705017089844\n",
      "Epoch 187, Loss: 909.8829956054688, Val Loss: 228.36676025390625\n",
      "Epoch 188, Loss: 909.8914184570312, Val Loss: 228.36618041992188\n",
      "Epoch 189, Loss: 909.8834228515625, Val Loss: 228.3665008544922\n",
      "Epoch 190, Loss: 909.8941650390625, Val Loss: 228.36642456054688\n",
      "Epoch 191, Loss: 909.8681030273438, Val Loss: 228.36618041992188\n",
      "Epoch 192, Loss: 909.8760375976562, Val Loss: 228.36602783203125\n",
      "Epoch 193, Loss: 909.8709106445312, Val Loss: 228.3658905029297\n",
      "Epoch 194, Loss: 909.8816528320312, Val Loss: 228.36569213867188\n",
      "Epoch 195, Loss: 909.8871459960938, Val Loss: 228.36566162109375\n",
      "Epoch 196, Loss: 909.8925170898438, Val Loss: 228.36566162109375\n",
      "Epoch 197, Loss: 909.8656005859375, Val Loss: 228.365478515625\n",
      "Epoch 198, Loss: 909.8541259765625, Val Loss: 228.36544799804688\n",
      "Epoch 199, Loss: 909.8799438476562, Val Loss: 228.36529541015625\n",
      "Epoch 200, Loss: 909.8683471679688, Val Loss: 228.36549377441406\n",
      "Epoch 201, Loss: 909.8704223632812, Val Loss: 228.36500549316406\n",
      "Epoch 202, Loss: 909.8633422851562, Val Loss: 228.36483764648438\n",
      "Epoch 203, Loss: 909.8590698242188, Val Loss: 228.3651123046875\n",
      "Epoch 204, Loss: 909.8490600585938, Val Loss: 228.36532592773438\n",
      "Epoch 205, Loss: 909.86474609375, Val Loss: 228.3645477294922\n",
      "Epoch 206, Loss: 909.8629760742188, Val Loss: 228.36444091796875\n",
      "Epoch 207, Loss: 909.84326171875, Val Loss: 228.3643341064453\n",
      "Epoch 208, Loss: 909.8602294921875, Val Loss: 228.3645477294922\n",
      "Epoch 209, Loss: 909.8732299804688, Val Loss: 228.36434936523438\n",
      "Epoch 210, Loss: 909.8688354492188, Val Loss: 228.36422729492188\n",
      "Epoch 211, Loss: 909.8524780273438, Val Loss: 228.36451721191406\n",
      "Epoch 212, Loss: 909.8529052734375, Val Loss: 228.36444091796875\n",
      "Epoch 213, Loss: 909.857666015625, Val Loss: 228.36410522460938\n",
      "Epoch 214, Loss: 909.8265991210938, Val Loss: 228.36441040039062\n",
      "Epoch 215, Loss: 909.8695068359375, Val Loss: 228.3644561767578\n",
      "Epoch 216, Loss: 909.8486938476562, Val Loss: 228.36419677734375\n",
      "Epoch 217, Loss: 909.8458251953125, Val Loss: 228.36428833007812\n",
      "Epoch 218, Loss: 909.8409423828125, Val Loss: 228.3642120361328\n",
      "Epoch 219, Loss: 909.8383178710938, Val Loss: 228.3640899658203\n",
      "Epoch 220, Loss: 909.8441772460938, Val Loss: 228.36402893066406\n",
      "Epoch 221, Loss: 909.837890625, Val Loss: 228.3639678955078\n",
      "Epoch 222, Loss: 909.829833984375, Val Loss: 228.3639678955078\n",
      "Epoch 223, Loss: 909.8339233398438, Val Loss: 228.3636932373047\n",
      "Epoch 224, Loss: 909.8428344726562, Val Loss: 228.36361694335938\n",
      "Epoch 225, Loss: 909.8419189453125, Val Loss: 228.3634796142578\n",
      "Epoch 226, Loss: 909.8447265625, Val Loss: 228.3633270263672\n",
      "Epoch 227, Loss: 909.8336791992188, Val Loss: 228.3631591796875\n",
      "Epoch 228, Loss: 909.8352661132812, Val Loss: 228.36331176757812\n",
      "Epoch 229, Loss: 909.8509521484375, Val Loss: 228.36289978027344\n",
      "Epoch 230, Loss: 909.8624267578125, Val Loss: 228.36305236816406\n",
      "Epoch 231, Loss: 909.8283081054688, Val Loss: 228.36305236816406\n",
      "Epoch 232, Loss: 909.8462524414062, Val Loss: 228.3630828857422\n",
      "Epoch 233, Loss: 909.8359985351562, Val Loss: 228.363037109375\n",
      "Epoch 234, Loss: 909.8304443359375, Val Loss: 228.36302185058594\n",
      "Epoch 235, Loss: 909.8428955078125, Val Loss: 228.3634490966797\n",
      "Epoch 236, Loss: 909.824951171875, Val Loss: 228.36306762695312\n",
      "Epoch 237, Loss: 909.8286743164062, Val Loss: 228.36300659179688\n",
      "Epoch 238, Loss: 909.829833984375, Val Loss: 228.3631134033203\n",
      "Epoch 239, Loss: 909.8283081054688, Val Loss: 228.3632354736328\n",
      "Epoch 240, Loss: 909.8234252929688, Val Loss: 228.3631134033203\n",
      "Epoch 241, Loss: 909.829345703125, Val Loss: 228.3631134033203\n",
      "Epoch 242, Loss: 909.8358764648438, Val Loss: 228.36314392089844\n",
      "Epoch 243, Loss: 909.82275390625, Val Loss: 228.36317443847656\n",
      "Epoch 244, Loss: 909.8251953125, Val Loss: 228.3631591796875\n",
      "Epoch 245, Loss: 909.8164672851562, Val Loss: 228.36343383789062\n",
      "Epoch 246, Loss: 909.8402099609375, Val Loss: 228.363037109375\n",
      "Epoch 247, Loss: 909.8150024414062, Val Loss: 228.36297607421875\n",
      "Epoch 248, Loss: 909.8277587890625, Val Loss: 228.36300659179688\n",
      "Epoch 249, Loss: 909.8043823242188, Val Loss: 228.36294555664062\n",
      "Epoch 250, Loss: 909.807373046875, Val Loss: 228.36285400390625\n",
      "Epoch 251, Loss: 909.814697265625, Val Loss: 228.36305236816406\n",
      "Epoch 252, Loss: 909.8154296875, Val Loss: 228.3627166748047\n",
      "Epoch 253, Loss: 909.8262329101562, Val Loss: 228.36253356933594\n",
      "Epoch 254, Loss: 909.8095703125, Val Loss: 228.362548828125\n",
      "Epoch 255, Loss: 909.8065185546875, Val Loss: 228.36233520507812\n",
      "Epoch 256, Loss: 909.8069458007812, Val Loss: 228.3622283935547\n",
      "Epoch 257, Loss: 909.8057250976562, Val Loss: 228.3618621826172\n",
      "Epoch 258, Loss: 909.8106079101562, Val Loss: 228.36203002929688\n",
      "Epoch 259, Loss: 909.8046875, Val Loss: 228.36155700683594\n",
      "Epoch 260, Loss: 909.7994384765625, Val Loss: 228.36141967773438\n",
      "Epoch 261, Loss: 909.815673828125, Val Loss: 228.3613739013672\n",
      "Epoch 262, Loss: 909.8187255859375, Val Loss: 228.36184692382812\n",
      "Epoch 263, Loss: 909.7998657226562, Val Loss: 228.36119079589844\n",
      "Epoch 264, Loss: 909.8076171875, Val Loss: 228.3604278564453\n",
      "Epoch 265, Loss: 909.8042602539062, Val Loss: 228.36122131347656\n",
      "Epoch 266, Loss: 909.7962646484375, Val Loss: 228.361083984375\n",
      "Epoch 267, Loss: 909.8117065429688, Val Loss: 228.36166381835938\n",
      "Epoch 268, Loss: 909.7987670898438, Val Loss: 228.3613739013672\n",
      "Epoch 269, Loss: 909.8037109375, Val Loss: 228.3611602783203\n",
      "Epoch 270, Loss: 909.8187255859375, Val Loss: 228.3612823486328\n",
      "Epoch 271, Loss: 909.8176879882812, Val Loss: 228.36135864257812\n",
      "Epoch 272, Loss: 909.8123168945312, Val Loss: 228.36151123046875\n",
      "Epoch 273, Loss: 909.8005981445312, Val Loss: 228.36195373535156\n",
      "Epoch 274, Loss: 909.8076782226562, Val Loss: 228.36175537109375\n",
      "Epoch 275, Loss: 909.8002319335938, Val Loss: 228.36207580566406\n",
      "Epoch 276, Loss: 909.7891845703125, Val Loss: 228.36192321777344\n",
      "Epoch 277, Loss: 909.8114624023438, Val Loss: 228.3619842529297\n",
      "Epoch 278, Loss: 909.7993774414062, Val Loss: 228.3619384765625\n",
      "Epoch 279, Loss: 909.7858276367188, Val Loss: 228.36181640625\n",
      "Epoch 280, Loss: 909.7840576171875, Val Loss: 228.3616943359375\n",
      "Epoch 281, Loss: 909.7898559570312, Val Loss: 228.36163330078125\n",
      "Epoch 282, Loss: 909.7926025390625, Val Loss: 228.36151123046875\n",
      "Epoch 283, Loss: 909.8012084960938, Val Loss: 228.36158752441406\n",
      "Epoch 284, Loss: 909.7793579101562, Val Loss: 228.3612823486328\n",
      "Epoch 285, Loss: 909.7843017578125, Val Loss: 228.3612060546875\n",
      "Epoch 286, Loss: 909.7816772460938, Val Loss: 228.3610076904297\n",
      "Epoch 287, Loss: 909.7959594726562, Val Loss: 228.36087036132812\n",
      "Epoch 288, Loss: 909.7724609375, Val Loss: 228.3606719970703\n",
      "Epoch 289, Loss: 909.7819213867188, Val Loss: 228.36038208007812\n",
      "Epoch 290, Loss: 909.7764892578125, Val Loss: 228.36033630371094\n",
      "Epoch 291, Loss: 909.7840576171875, Val Loss: 228.36019897460938\n",
      "Epoch 292, Loss: 909.7979736328125, Val Loss: 228.36026000976562\n",
      "Epoch 293, Loss: 909.7952880859375, Val Loss: 228.3600311279297\n",
      "Epoch 294, Loss: 909.7864379882812, Val Loss: 228.36001586914062\n",
      "Epoch 295, Loss: 909.7920532226562, Val Loss: 228.36000061035156\n",
      "Epoch 296, Loss: 909.77001953125, Val Loss: 228.3595428466797\n",
      "Epoch 297, Loss: 909.7841796875, Val Loss: 228.35995483398438\n",
      "Epoch 298, Loss: 909.7892456054688, Val Loss: 228.36009216308594\n",
      "Epoch 299, Loss: 909.7886352539062, Val Loss: 228.3601837158203\n",
      "Epoch 300, Loss: 909.7994995117188, Val Loss: 228.36036682128906\n",
      "Epoch 301, Loss: 909.7752685546875, Val Loss: 228.36045837402344\n",
      "Epoch 302, Loss: 909.77880859375, Val Loss: 228.36065673828125\n",
      "Epoch 303, Loss: 909.78564453125, Val Loss: 228.36062622070312\n",
      "Epoch 304, Loss: 909.77294921875, Val Loss: 228.3606414794922\n",
      "Epoch 305, Loss: 909.7653198242188, Val Loss: 228.36062622070312\n",
      "Epoch 306, Loss: 909.7775268554688, Val Loss: 228.360595703125\n",
      "Epoch 307, Loss: 909.7760009765625, Val Loss: 228.36058044433594\n",
      "Epoch 308, Loss: 909.7734375, Val Loss: 228.3605499267578\n",
      "Epoch 309, Loss: 909.7713623046875, Val Loss: 228.3604736328125\n",
      "Epoch 310, Loss: 909.7857666015625, Val Loss: 228.3604278564453\n",
      "Epoch 311, Loss: 909.7764892578125, Val Loss: 228.36036682128906\n",
      "Epoch 312, Loss: 909.77783203125, Val Loss: 228.36032104492188\n",
      "Epoch 313, Loss: 909.7818603515625, Val Loss: 228.36024475097656\n",
      "Epoch 314, Loss: 909.755615234375, Val Loss: 228.36021423339844\n",
      "Epoch 315, Loss: 909.7837524414062, Val Loss: 228.36029052734375\n",
      "Epoch 316, Loss: 909.774169921875, Val Loss: 228.3602294921875\n",
      "Epoch 317, Loss: 909.783935546875, Val Loss: 228.36026000976562\n",
      "Epoch 318, Loss: 909.775146484375, Val Loss: 228.36024475097656\n",
      "Epoch 319, Loss: 909.76904296875, Val Loss: 228.3601531982422\n",
      "Epoch 320, Loss: 909.7852172851562, Val Loss: 228.36038208007812\n",
      "Epoch 321, Loss: 909.7896118164062, Val Loss: 228.36044311523438\n",
      "Epoch 322, Loss: 909.7672729492188, Val Loss: 228.360595703125\n",
      "Epoch 323, Loss: 909.7614135742188, Val Loss: 228.36033630371094\n",
      "Epoch 324, Loss: 909.7807006835938, Val Loss: 228.3605194091797\n",
      "Epoch 325, Loss: 909.76416015625, Val Loss: 228.3604736328125\n",
      "Epoch 326, Loss: 909.7734375, Val Loss: 228.36048889160156\n",
      "Epoch 327, Loss: 909.770751953125, Val Loss: 228.36013793945312\n",
      "Epoch 328, Loss: 909.7688598632812, Val Loss: 228.3604278564453\n",
      "Epoch 329, Loss: 909.780517578125, Val Loss: 228.3603973388672\n",
      "Epoch 330, Loss: 909.7752075195312, Val Loss: 228.3603973388672\n",
      "Epoch 331, Loss: 909.7643432617188, Val Loss: 228.36033630371094\n",
      "Epoch 332, Loss: 909.7470703125, Val Loss: 228.36041259765625\n",
      "Epoch 333, Loss: 909.7619018554688, Val Loss: 228.36009216308594\n",
      "Epoch 334, Loss: 909.7630615234375, Val Loss: 228.3599090576172\n",
      "Epoch 335, Loss: 909.76806640625, Val Loss: 228.3597869873047\n",
      "Epoch 336, Loss: 909.770751953125, Val Loss: 228.35972595214844\n",
      "Epoch 337, Loss: 909.7783813476562, Val Loss: 228.3595428466797\n",
      "Epoch 338, Loss: 909.7769165039062, Val Loss: 228.35943603515625\n",
      "Epoch 339, Loss: 909.7726440429688, Val Loss: 228.3594207763672\n",
      "Epoch 340, Loss: 909.7796630859375, Val Loss: 228.35935974121094\n",
      "Epoch 341, Loss: 909.7720947265625, Val Loss: 228.35939025878906\n",
      "Epoch 342, Loss: 909.75439453125, Val Loss: 228.359375\n",
      "Epoch 343, Loss: 909.7606201171875, Val Loss: 228.3594207763672\n",
      "Epoch 344, Loss: 909.7538452148438, Val Loss: 228.35943603515625\n",
      "Epoch 345, Loss: 909.7529907226562, Val Loss: 228.35948181152344\n",
      "Epoch 346, Loss: 909.761474609375, Val Loss: 228.35951232910156\n",
      "Epoch 347, Loss: 909.7708740234375, Val Loss: 228.3595428466797\n",
      "Epoch 348, Loss: 909.762451171875, Val Loss: 228.3597412109375\n",
      "Epoch 349, Loss: 909.7592163085938, Val Loss: 228.35960388183594\n",
      "Epoch 350, Loss: 909.7584838867188, Val Loss: 228.35975646972656\n",
      "Epoch 351, Loss: 909.749755859375, Val Loss: 228.3596954345703\n",
      "Epoch 352, Loss: 909.7645874023438, Val Loss: 228.35971069335938\n",
      "Epoch 353, Loss: 909.7549438476562, Val Loss: 228.3597412109375\n",
      "Epoch 354, Loss: 909.739013671875, Val Loss: 228.35971069335938\n",
      "Epoch 355, Loss: 909.7675170898438, Val Loss: 228.35971069335938\n",
      "Epoch 356, Loss: 909.77099609375, Val Loss: 228.3597412109375\n",
      "Epoch 357, Loss: 909.763671875, Val Loss: 228.35971069335938\n",
      "Epoch 358, Loss: 909.759521484375, Val Loss: 228.3596954345703\n",
      "Epoch 359, Loss: 909.7703247070312, Val Loss: 228.35964965820312\n",
      "Epoch 360, Loss: 909.7527465820312, Val Loss: 228.359619140625\n",
      "Epoch 361, Loss: 909.7471923828125, Val Loss: 228.35946655273438\n",
      "Epoch 362, Loss: 909.7633056640625, Val Loss: 228.35934448242188\n",
      "Epoch 363, Loss: 909.741943359375, Val Loss: 228.35922241210938\n",
      "Epoch 364, Loss: 909.7530517578125, Val Loss: 228.3592071533203\n",
      "Epoch 365, Loss: 909.7601928710938, Val Loss: 228.3590087890625\n",
      "Epoch 366, Loss: 909.7481079101562, Val Loss: 228.3589324951172\n",
      "Epoch 367, Loss: 909.7266845703125, Val Loss: 228.3588409423828\n",
      "Epoch 368, Loss: 909.7407836914062, Val Loss: 228.3586883544922\n",
      "Epoch 369, Loss: 909.7450561523438, Val Loss: 228.35858154296875\n",
      "Epoch 370, Loss: 909.7540283203125, Val Loss: 228.35861206054688\n",
      "Epoch 371, Loss: 909.7447509765625, Val Loss: 228.35850524902344\n",
      "Epoch 372, Loss: 909.766357421875, Val Loss: 228.3583526611328\n",
      "Epoch 373, Loss: 909.7337036132812, Val Loss: 228.35826110839844\n",
      "Epoch 374, Loss: 909.7457275390625, Val Loss: 228.3584442138672\n",
      "Epoch 375, Loss: 909.75341796875, Val Loss: 228.35850524902344\n",
      "Epoch 376, Loss: 909.7509765625, Val Loss: 228.35862731933594\n",
      "Epoch 377, Loss: 909.7501220703125, Val Loss: 228.3587188720703\n",
      "Epoch 378, Loss: 909.7572631835938, Val Loss: 228.3589324951172\n",
      "Epoch 379, Loss: 909.7464599609375, Val Loss: 228.359130859375\n",
      "Epoch 380, Loss: 909.7435913085938, Val Loss: 228.3592987060547\n",
      "Epoch 381, Loss: 909.745361328125, Val Loss: 228.3594207763672\n",
      "Epoch 382, Loss: 909.7516479492188, Val Loss: 228.35955810546875\n",
      "Epoch 383, Loss: 909.7446899414062, Val Loss: 228.3596649169922\n",
      "Epoch 384, Loss: 909.7427978515625, Val Loss: 228.35958862304688\n",
      "Epoch 385, Loss: 909.7542114257812, Val Loss: 228.3596649169922\n",
      "Epoch 386, Loss: 909.7504272460938, Val Loss: 228.3596649169922\n",
      "Epoch 387, Loss: 909.7618408203125, Val Loss: 228.35964965820312\n",
      "Epoch 388, Loss: 909.74365234375, Val Loss: 228.3595733642578\n",
      "Epoch 389, Loss: 909.7423095703125, Val Loss: 228.35955810546875\n",
      "Epoch 390, Loss: 909.7648315429688, Val Loss: 228.3595428466797\n",
      "Epoch 391, Loss: 909.7605590820312, Val Loss: 228.35955810546875\n",
      "Epoch 392, Loss: 909.7439575195312, Val Loss: 228.3595428466797\n",
      "Epoch 393, Loss: 909.747802734375, Val Loss: 228.3595428466797\n",
      "Epoch 394, Loss: 909.7526245117188, Val Loss: 228.3595428466797\n",
      "Epoch 395, Loss: 909.7505493164062, Val Loss: 228.3587188720703\n",
      "Epoch 396, Loss: 909.7421264648438, Val Loss: 228.35943603515625\n",
      "Epoch 397, Loss: 909.7405395507812, Val Loss: 228.3592987060547\n",
      "Epoch 398, Loss: 909.7589111328125, Val Loss: 228.3591766357422\n",
      "Epoch 399, Loss: 909.736572265625, Val Loss: 228.3590087890625\n",
      "Epoch 400, Loss: 909.7499389648438, Val Loss: 228.3585968017578\n",
      "Epoch 401, Loss: 909.7340087890625, Val Loss: 228.3587188720703\n",
      "Epoch 402, Loss: 909.7406616210938, Val Loss: 228.3585662841797\n",
      "Epoch 403, Loss: 909.7428588867188, Val Loss: 228.35848999023438\n",
      "Epoch 404, Loss: 909.7515869140625, Val Loss: 228.3584442138672\n",
      "Epoch 405, Loss: 909.7467651367188, Val Loss: 228.35826110839844\n",
      "Epoch 406, Loss: 909.7410278320312, Val Loss: 228.35845947265625\n",
      "Epoch 407, Loss: 909.71923828125, Val Loss: 228.3584747314453\n",
      "Epoch 408, Loss: 909.7378540039062, Val Loss: 228.3584442138672\n",
      "Epoch 409, Loss: 909.7435302734375, Val Loss: 228.35850524902344\n",
      "Epoch 410, Loss: 909.7457885742188, Val Loss: 228.35850524902344\n",
      "Epoch 411, Loss: 909.7312622070312, Val Loss: 228.35850524902344\n",
      "Epoch 412, Loss: 909.7327270507812, Val Loss: 228.3585205078125\n",
      "Epoch 413, Loss: 909.7357177734375, Val Loss: 228.35842895507812\n",
      "Epoch 414, Loss: 909.7317504882812, Val Loss: 228.35842895507812\n",
      "Epoch 415, Loss: 909.7382202148438, Val Loss: 228.35842895507812\n",
      "Epoch 416, Loss: 909.7608032226562, Val Loss: 228.3585205078125\n",
      "Epoch 417, Loss: 909.732666015625, Val Loss: 228.35865783691406\n",
      "Epoch 418, Loss: 909.7425537109375, Val Loss: 228.35867309570312\n",
      "Epoch 419, Loss: 909.7479858398438, Val Loss: 228.35870361328125\n",
      "Epoch 420, Loss: 909.7366943359375, Val Loss: 228.3587646484375\n",
      "Epoch 421, Loss: 909.7373657226562, Val Loss: 228.35879516601562\n",
      "Epoch 422, Loss: 909.7399291992188, Val Loss: 228.35882568359375\n",
      "Epoch 423, Loss: 909.7232666015625, Val Loss: 228.35879516601562\n",
      "Epoch 424, Loss: 909.7383422851562, Val Loss: 228.3587646484375\n",
      "Epoch 425, Loss: 909.7255249023438, Val Loss: 228.35870361328125\n",
      "Epoch 426, Loss: 909.7266845703125, Val Loss: 228.3585662841797\n",
      "Epoch 427, Loss: 909.7426147460938, Val Loss: 228.35842895507812\n",
      "Epoch 428, Loss: 909.74658203125, Val Loss: 228.3582763671875\n",
      "Epoch 429, Loss: 909.73583984375, Val Loss: 228.35816955566406\n",
      "Epoch 430, Loss: 909.7343139648438, Val Loss: 228.3581085205078\n",
      "Epoch 431, Loss: 909.7263793945312, Val Loss: 228.35804748535156\n",
      "Epoch 432, Loss: 909.738525390625, Val Loss: 228.35800170898438\n",
      "Epoch 433, Loss: 909.729248046875, Val Loss: 228.35792541503906\n",
      "Epoch 434, Loss: 909.7311401367188, Val Loss: 228.3579559326172\n",
      "Epoch 435, Loss: 909.73193359375, Val Loss: 228.35733032226562\n",
      "Epoch 436, Loss: 909.7421264648438, Val Loss: 228.35806274414062\n",
      "Epoch 437, Loss: 909.7161865234375, Val Loss: 228.3580780029297\n",
      "Epoch 438, Loss: 909.7335815429688, Val Loss: 228.35813903808594\n",
      "Epoch 439, Loss: 909.7341918945312, Val Loss: 228.3582000732422\n",
      "Epoch 440, Loss: 909.726318359375, Val Loss: 228.35812377929688\n",
      "Epoch 441, Loss: 909.7293090820312, Val Loss: 228.35821533203125\n",
      "Epoch 442, Loss: 909.7332763671875, Val Loss: 228.3582305908203\n",
      "Epoch 443, Loss: 909.7242431640625, Val Loss: 228.35818481445312\n",
      "Epoch 444, Loss: 909.7421264648438, Val Loss: 228.35818481445312\n",
      "Epoch 445, Loss: 909.7465209960938, Val Loss: 228.3582000732422\n",
      "Epoch 446, Loss: 909.7318115234375, Val Loss: 228.35818481445312\n",
      "Epoch 447, Loss: 909.7384643554688, Val Loss: 228.35818481445312\n",
      "Epoch 448, Loss: 909.7321166992188, Val Loss: 228.3582000732422\n",
      "Epoch 449, Loss: 909.7423706054688, Val Loss: 228.3582000732422\n",
      "Epoch 450, Loss: 909.744140625, Val Loss: 228.3582305908203\n",
      "Epoch 451, Loss: 909.7202758789062, Val Loss: 228.35818481445312\n",
      "Epoch 452, Loss: 909.7196044921875, Val Loss: 228.35813903808594\n",
      "Epoch 453, Loss: 909.7385864257812, Val Loss: 228.3580322265625\n",
      "Epoch 454, Loss: 909.7369384765625, Val Loss: 228.35792541503906\n",
      "Epoch 455, Loss: 909.7301025390625, Val Loss: 228.3577880859375\n",
      "Epoch 456, Loss: 909.7321166992188, Val Loss: 228.35728454589844\n",
      "Epoch 457, Loss: 909.7080078125, Val Loss: 228.35745239257812\n",
      "Epoch 458, Loss: 909.7130126953125, Val Loss: 228.35726928710938\n",
      "Epoch 459, Loss: 909.7342529296875, Val Loss: 228.35708618164062\n",
      "Epoch 460, Loss: 909.7412719726562, Val Loss: 228.35693359375\n",
      "Epoch 461, Loss: 909.7142333984375, Val Loss: 228.3567657470703\n",
      "Epoch 462, Loss: 909.720703125, Val Loss: 228.35665893554688\n",
      "Epoch 463, Loss: 909.7310791015625, Val Loss: 228.35659790039062\n",
      "Epoch 464, Loss: 909.7364501953125, Val Loss: 228.35655212402344\n",
      "Epoch 465, Loss: 909.7258911132812, Val Loss: 228.35659790039062\n",
      "Epoch 466, Loss: 909.718994140625, Val Loss: 228.3566436767578\n",
      "Epoch 467, Loss: 909.7197875976562, Val Loss: 228.356689453125\n",
      "Epoch 468, Loss: 909.7096557617188, Val Loss: 228.35671997070312\n",
      "Epoch 469, Loss: 909.71630859375, Val Loss: 228.35671997070312\n",
      "Epoch 470, Loss: 909.7050170898438, Val Loss: 228.356689453125\n",
      "Epoch 471, Loss: 909.72314453125, Val Loss: 228.35671997070312\n",
      "Epoch 472, Loss: 909.7094116210938, Val Loss: 228.35671997070312\n",
      "Epoch 473, Loss: 909.727294921875, Val Loss: 228.35675048828125\n",
      "Epoch 474, Loss: 909.725341796875, Val Loss: 228.3568115234375\n",
      "Epoch 475, Loss: 909.724853515625, Val Loss: 228.35684204101562\n",
      "Epoch 476, Loss: 909.718505859375, Val Loss: 228.3568115234375\n",
      "Epoch 477, Loss: 909.7177124023438, Val Loss: 228.35682678222656\n",
      "Epoch 478, Loss: 909.7112426757812, Val Loss: 228.35684204101562\n",
      "Epoch 479, Loss: 909.7193603515625, Val Loss: 228.35696411132812\n",
      "Epoch 480, Loss: 909.716796875, Val Loss: 228.35711669921875\n",
      "Epoch 481, Loss: 909.709716796875, Val Loss: 228.3572540283203\n",
      "Epoch 482, Loss: 909.71435546875, Val Loss: 228.35736083984375\n",
      "Epoch 483, Loss: 909.719970703125, Val Loss: 228.357421875\n",
      "Epoch 484, Loss: 909.7020874023438, Val Loss: 228.35745239257812\n",
      "Epoch 485, Loss: 909.7152709960938, Val Loss: 228.3574676513672\n",
      "Epoch 486, Loss: 909.7154541015625, Val Loss: 228.3574981689453\n",
      "Epoch 487, Loss: 909.7128295898438, Val Loss: 228.35763549804688\n",
      "Epoch 488, Loss: 909.7124633789062, Val Loss: 228.35757446289062\n",
      "Epoch 489, Loss: 909.7169799804688, Val Loss: 228.35769653320312\n",
      "Epoch 490, Loss: 909.7141723632812, Val Loss: 228.35787963867188\n",
      "Epoch 491, Loss: 909.7090454101562, Val Loss: 228.35801696777344\n",
      "Epoch 492, Loss: 909.7169799804688, Val Loss: 228.35809326171875\n",
      "Epoch 493, Loss: 909.71044921875, Val Loss: 228.35824584960938\n",
      "Epoch 494, Loss: 909.7102661132812, Val Loss: 228.35816955566406\n",
      "Epoch 495, Loss: 909.7035522460938, Val Loss: 228.358154296875\n",
      "Epoch 496, Loss: 909.7210083007812, Val Loss: 228.3581085205078\n",
      "Epoch 497, Loss: 909.703125, Val Loss: 228.35800170898438\n",
      "Epoch 498, Loss: 909.7175903320312, Val Loss: 228.35781860351562\n",
      "Epoch 499, Loss: 909.6990356445312, Val Loss: 228.35760498046875\n",
      "Epoch 500, Loss: 909.705078125, Val Loss: 228.357421875\n",
      "Epoch 501, Loss: 909.7120971679688, Val Loss: 228.35723876953125\n",
      "Epoch 502, Loss: 909.7168579101562, Val Loss: 228.3571319580078\n",
      "Epoch 503, Loss: 909.7172241210938, Val Loss: 228.35711669921875\n",
      "Epoch 504, Loss: 909.7110595703125, Val Loss: 228.3570098876953\n",
      "Epoch 505, Loss: 909.7217407226562, Val Loss: 228.35696411132812\n",
      "Epoch 506, Loss: 909.7031860351562, Val Loss: 228.35693359375\n",
      "Epoch 507, Loss: 909.7167358398438, Val Loss: 228.35694885253906\n",
      "Epoch 508, Loss: 909.7091674804688, Val Loss: 228.35691833496094\n",
      "Epoch 509, Loss: 909.7003784179688, Val Loss: 228.3568878173828\n",
      "Epoch 510, Loss: 909.7133178710938, Val Loss: 228.35691833496094\n",
      "Epoch 511, Loss: 909.716796875, Val Loss: 228.35704040527344\n",
      "Epoch 512, Loss: 909.7039794921875, Val Loss: 228.3571319580078\n",
      "Epoch 513, Loss: 909.71484375, Val Loss: 228.357177734375\n",
      "Epoch 514, Loss: 909.7000122070312, Val Loss: 228.35720825195312\n",
      "Epoch 515, Loss: 909.7073364257812, Val Loss: 228.35714721679688\n",
      "Epoch 516, Loss: 909.7074584960938, Val Loss: 228.3571319580078\n",
      "Epoch 517, Loss: 909.702392578125, Val Loss: 228.35714721679688\n",
      "Epoch 518, Loss: 909.6997680664062, Val Loss: 228.35711669921875\n",
      "Epoch 519, Loss: 909.7247924804688, Val Loss: 228.35720825195312\n",
      "Epoch 520, Loss: 909.7072143554688, Val Loss: 228.35720825195312\n",
      "Epoch 521, Loss: 909.710205078125, Val Loss: 228.357177734375\n",
      "Epoch 522, Loss: 909.7197875976562, Val Loss: 228.35716247558594\n",
      "Epoch 523, Loss: 909.7179565429688, Val Loss: 228.35716247558594\n",
      "Epoch 524, Loss: 909.7092895507812, Val Loss: 228.3571014404297\n",
      "Epoch 525, Loss: 909.7083129882812, Val Loss: 228.35716247558594\n",
      "Epoch 526, Loss: 909.7073974609375, Val Loss: 228.35693359375\n",
      "Epoch 527, Loss: 909.70751953125, Val Loss: 228.35682678222656\n",
      "Epoch 528, Loss: 909.70166015625, Val Loss: 228.35678100585938\n",
      "Epoch 529, Loss: 909.6959228515625, Val Loss: 228.3567352294922\n",
      "Epoch 530, Loss: 909.7108764648438, Val Loss: 228.35671997070312\n",
      "Epoch 531, Loss: 909.7220458984375, Val Loss: 228.3567352294922\n",
      "Epoch 532, Loss: 909.6974487304688, Val Loss: 228.3567352294922\n",
      "Epoch 533, Loss: 909.71337890625, Val Loss: 228.35670471191406\n",
      "Epoch 534, Loss: 909.69580078125, Val Loss: 228.356689453125\n",
      "Epoch 535, Loss: 909.7186889648438, Val Loss: 228.3567352294922\n",
      "Epoch 536, Loss: 909.7046508789062, Val Loss: 228.35684204101562\n",
      "Epoch 537, Loss: 909.7070922851562, Val Loss: 228.35679626464844\n",
      "Epoch 538, Loss: 909.70458984375, Val Loss: 228.3568572998047\n",
      "Epoch 539, Loss: 909.705810546875, Val Loss: 228.3568878173828\n",
      "Epoch 540, Loss: 909.711669921875, Val Loss: 228.3568878173828\n",
      "Epoch 541, Loss: 909.703125, Val Loss: 228.35693359375\n",
      "Epoch 542, Loss: 909.6970825195312, Val Loss: 228.35691833496094\n",
      "Epoch 543, Loss: 909.7069091796875, Val Loss: 228.35691833496094\n",
      "Epoch 544, Loss: 909.7041015625, Val Loss: 228.3569793701172\n",
      "Epoch 545, Loss: 909.7144775390625, Val Loss: 228.35696411132812\n",
      "Epoch 546, Loss: 909.6990356445312, Val Loss: 228.357177734375\n",
      "Epoch 547, Loss: 909.7162475585938, Val Loss: 228.3572235107422\n",
      "Epoch 548, Loss: 909.6981201171875, Val Loss: 228.3572540283203\n",
      "Epoch 549, Loss: 909.7015991210938, Val Loss: 228.35726928710938\n",
      "Epoch 550, Loss: 909.7102661132812, Val Loss: 228.35723876953125\n",
      "Epoch 551, Loss: 909.6946411132812, Val Loss: 228.3571319580078\n",
      "Epoch 552, Loss: 909.6972045898438, Val Loss: 228.35699462890625\n",
      "Epoch 553, Loss: 909.6908569335938, Val Loss: 228.35678100585938\n",
      "Epoch 554, Loss: 909.71728515625, Val Loss: 228.3566436767578\n",
      "Epoch 555, Loss: 909.699462890625, Val Loss: 228.35653686523438\n",
      "Epoch 556, Loss: 909.7076416015625, Val Loss: 228.35610961914062\n",
      "Epoch 557, Loss: 909.7134399414062, Val Loss: 228.3565673828125\n",
      "Epoch 558, Loss: 909.6966552734375, Val Loss: 228.35653686523438\n",
      "Epoch 559, Loss: 909.6988525390625, Val Loss: 228.356689453125\n",
      "Epoch 560, Loss: 909.7149658203125, Val Loss: 228.3567657470703\n",
      "Epoch 561, Loss: 909.7101440429688, Val Loss: 228.3568572998047\n",
      "Epoch 562, Loss: 909.6980590820312, Val Loss: 228.3570098876953\n",
      "Epoch 563, Loss: 909.703125, Val Loss: 228.35714721679688\n",
      "Epoch 564, Loss: 909.697021484375, Val Loss: 228.35731506347656\n",
      "Epoch 565, Loss: 909.6974487304688, Val Loss: 228.35740661621094\n",
      "Epoch 566, Loss: 909.6959838867188, Val Loss: 228.35745239257812\n",
      "Epoch 567, Loss: 909.6957397460938, Val Loss: 228.35736083984375\n",
      "Epoch 568, Loss: 909.692138671875, Val Loss: 228.35728454589844\n",
      "Epoch 569, Loss: 909.6871337890625, Val Loss: 228.35711669921875\n",
      "Epoch 570, Loss: 909.7117309570312, Val Loss: 228.3569793701172\n",
      "Epoch 571, Loss: 909.7036743164062, Val Loss: 228.35690307617188\n",
      "Epoch 572, Loss: 909.698974609375, Val Loss: 228.35678100585938\n",
      "Epoch 573, Loss: 909.7059936523438, Val Loss: 228.356689453125\n",
      "Epoch 574, Loss: 909.6970825195312, Val Loss: 228.35659790039062\n",
      "Epoch 575, Loss: 909.6975708007812, Val Loss: 228.35650634765625\n",
      "Epoch 576, Loss: 909.7066040039062, Val Loss: 228.3565216064453\n",
      "Epoch 577, Loss: 909.7052612304688, Val Loss: 228.3565673828125\n",
      "Epoch 578, Loss: 909.6966552734375, Val Loss: 228.35658264160156\n",
      "Epoch 579, Loss: 909.6981201171875, Val Loss: 228.356689453125\n",
      "Epoch 580, Loss: 909.7030639648438, Val Loss: 228.3567657470703\n",
      "Epoch 581, Loss: 909.6935424804688, Val Loss: 228.3568572998047\n",
      "Epoch 582, Loss: 909.7025756835938, Val Loss: 228.35693359375\n",
      "Epoch 583, Loss: 909.7022705078125, Val Loss: 228.35694885253906\n",
      "Epoch 584, Loss: 909.7031860351562, Val Loss: 228.3569793701172\n",
      "Epoch 585, Loss: 909.7112426757812, Val Loss: 228.35702514648438\n",
      "Epoch 586, Loss: 909.701171875, Val Loss: 228.35704040527344\n",
      "Epoch 587, Loss: 909.6968383789062, Val Loss: 228.35702514648438\n",
      "Epoch 588, Loss: 909.711181640625, Val Loss: 228.35707092285156\n",
      "Epoch 589, Loss: 909.7036743164062, Val Loss: 228.35716247558594\n",
      "Epoch 590, Loss: 909.6932373046875, Val Loss: 228.35707092285156\n",
      "Epoch 591, Loss: 909.6983032226562, Val Loss: 228.3569793701172\n",
      "Epoch 592, Loss: 909.6858520507812, Val Loss: 228.3568878173828\n",
      "Epoch 593, Loss: 909.6928100585938, Val Loss: 228.3567352294922\n",
      "Epoch 594, Loss: 909.6871337890625, Val Loss: 228.35653686523438\n",
      "Epoch 595, Loss: 909.6795043945312, Val Loss: 228.35626220703125\n",
      "Epoch 596, Loss: 909.701416015625, Val Loss: 228.35601806640625\n",
      "Epoch 597, Loss: 909.687255859375, Val Loss: 228.35581970214844\n",
      "Epoch 598, Loss: 909.6893920898438, Val Loss: 228.35569763183594\n",
      "Epoch 599, Loss: 909.6832275390625, Val Loss: 228.3555908203125\n",
      "Epoch 600, Loss: 909.6986083984375, Val Loss: 228.35552978515625\n",
      "Epoch 601, Loss: 909.6892700195312, Val Loss: 228.3555450439453\n",
      "Epoch 602, Loss: 909.69091796875, Val Loss: 228.35560607910156\n",
      "Epoch 603, Loss: 909.6901245117188, Val Loss: 228.35572814941406\n",
      "Epoch 604, Loss: 909.6888427734375, Val Loss: 228.35589599609375\n",
      "Epoch 605, Loss: 909.6986083984375, Val Loss: 228.35609436035156\n",
      "Epoch 606, Loss: 909.6956176757812, Val Loss: 228.35635375976562\n",
      "Epoch 607, Loss: 909.70166015625, Val Loss: 228.3566131591797\n",
      "Epoch 608, Loss: 909.692626953125, Val Loss: 228.35691833496094\n",
      "Epoch 609, Loss: 909.6867065429688, Val Loss: 228.35711669921875\n",
      "Epoch 610, Loss: 909.6793212890625, Val Loss: 228.35714721679688\n",
      "Epoch 611, Loss: 909.6840209960938, Val Loss: 228.3571014404297\n",
      "Epoch 612, Loss: 909.68701171875, Val Loss: 228.35694885253906\n",
      "Epoch 613, Loss: 909.6978759765625, Val Loss: 228.3567352294922\n",
      "Epoch 614, Loss: 909.697265625, Val Loss: 228.3565216064453\n",
      "Epoch 615, Loss: 909.701416015625, Val Loss: 228.3563232421875\n",
      "Epoch 616, Loss: 909.7142333984375, Val Loss: 228.356201171875\n",
      "Epoch 617, Loss: 909.6991577148438, Val Loss: 228.35618591308594\n",
      "Epoch 618, Loss: 909.6976318359375, Val Loss: 228.35626220703125\n",
      "Epoch 619, Loss: 909.6964111328125, Val Loss: 228.35635375976562\n",
      "Epoch 620, Loss: 909.685791015625, Val Loss: 228.35643005371094\n",
      "Epoch 621, Loss: 909.7020874023438, Val Loss: 228.35658264160156\n",
      "Epoch 622, Loss: 909.6932373046875, Val Loss: 228.35679626464844\n",
      "Epoch 623, Loss: 909.7059326171875, Val Loss: 228.3568572998047\n",
      "Epoch 624, Loss: 909.6987915039062, Val Loss: 228.35696411132812\n",
      "Epoch 625, Loss: 909.6925048828125, Val Loss: 228.35708618164062\n",
      "Epoch 626, Loss: 909.68701171875, Val Loss: 228.3571319580078\n",
      "Epoch 627, Loss: 909.6793212890625, Val Loss: 228.35707092285156\n",
      "Epoch 628, Loss: 909.6956787109375, Val Loss: 228.35699462890625\n",
      "Epoch 629, Loss: 909.6829833984375, Val Loss: 228.35682678222656\n",
      "Epoch 630, Loss: 909.6947631835938, Val Loss: 228.35662841796875\n",
      "Epoch 631, Loss: 909.689208984375, Val Loss: 228.35641479492188\n",
      "Epoch 632, Loss: 909.6983032226562, Val Loss: 228.356201171875\n",
      "Epoch 633, Loss: 909.683349609375, Val Loss: 228.35609436035156\n",
      "Epoch 634, Loss: 909.6925048828125, Val Loss: 228.35597229003906\n",
      "Epoch 635, Loss: 909.6847534179688, Val Loss: 228.35585021972656\n",
      "Epoch 636, Loss: 909.6943359375, Val Loss: 228.3557586669922\n",
      "Epoch 637, Loss: 909.6905517578125, Val Loss: 228.35574340820312\n",
      "Epoch 638, Loss: 909.6875610351562, Val Loss: 228.35572814941406\n",
      "Epoch 639, Loss: 909.7012939453125, Val Loss: 228.35580444335938\n",
      "Epoch 640, Loss: 909.70751953125, Val Loss: 228.3560028076172\n",
      "Epoch 641, Loss: 909.6950073242188, Val Loss: 228.3562774658203\n",
      "Epoch 642, Loss: 909.6851196289062, Val Loss: 228.35653686523438\n",
      "Epoch 643, Loss: 909.6834716796875, Val Loss: 228.35679626464844\n",
      "Epoch 644, Loss: 909.685302734375, Val Loss: 228.3570098876953\n",
      "Epoch 645, Loss: 909.6907958984375, Val Loss: 228.357177734375\n",
      "Epoch 646, Loss: 909.6757202148438, Val Loss: 228.35726928710938\n",
      "Epoch 647, Loss: 909.6819458007812, Val Loss: 228.3572540283203\n",
      "Epoch 648, Loss: 909.6862182617188, Val Loss: 228.3571014404297\n",
      "Epoch 649, Loss: 909.6861572265625, Val Loss: 228.3568878173828\n",
      "Epoch 650, Loss: 909.6828002929688, Val Loss: 228.35662841796875\n",
      "Epoch 651, Loss: 909.6897583007812, Val Loss: 228.3563232421875\n",
      "Epoch 652, Loss: 909.689697265625, Val Loss: 228.35606384277344\n",
      "Epoch 653, Loss: 909.6862182617188, Val Loss: 228.3558349609375\n",
      "Epoch 654, Loss: 909.688720703125, Val Loss: 228.35574340820312\n",
      "Epoch 655, Loss: 909.6758422851562, Val Loss: 228.35565185546875\n",
      "Epoch 656, Loss: 909.6918334960938, Val Loss: 228.35557556152344\n",
      "Epoch 657, Loss: 909.6983642578125, Val Loss: 228.35569763183594\n",
      "Epoch 658, Loss: 909.6946411132812, Val Loss: 228.35585021972656\n",
      "Epoch 659, Loss: 909.6891479492188, Val Loss: 228.3560791015625\n",
      "Epoch 660, Loss: 909.6845703125, Val Loss: 228.35633850097656\n",
      "Epoch 661, Loss: 909.6852416992188, Val Loss: 228.35659790039062\n",
      "Epoch 662, Loss: 909.67822265625, Val Loss: 228.35684204101562\n",
      "Epoch 663, Loss: 909.6800537109375, Val Loss: 228.35704040527344\n",
      "Epoch 664, Loss: 909.69140625, Val Loss: 228.3571014404297\n",
      "Epoch 665, Loss: 909.6862182617188, Val Loss: 228.35708618164062\n",
      "Epoch 666, Loss: 909.6884155273438, Val Loss: 228.35702514648438\n",
      "Epoch 667, Loss: 909.6909790039062, Val Loss: 228.35693359375\n",
      "Epoch 668, Loss: 909.6700439453125, Val Loss: 228.35667419433594\n",
      "Epoch 669, Loss: 909.693115234375, Val Loss: 228.35638427734375\n",
      "Epoch 670, Loss: 909.6787109375, Val Loss: 228.35604858398438\n",
      "Epoch 671, Loss: 909.69580078125, Val Loss: 228.35581970214844\n",
      "Epoch 672, Loss: 909.6798706054688, Val Loss: 228.35562133789062\n",
      "Epoch 673, Loss: 909.68359375, Val Loss: 228.35543823242188\n",
      "Epoch 674, Loss: 909.673095703125, Val Loss: 228.35523986816406\n",
      "Epoch 675, Loss: 909.6945190429688, Val Loss: 228.3551483154297\n",
      "Epoch 676, Loss: 909.6943359375, Val Loss: 228.3551483154297\n",
      "Epoch 677, Loss: 909.6883544921875, Val Loss: 228.35523986816406\n",
      "Epoch 678, Loss: 909.68115234375, Val Loss: 228.35536193847656\n",
      "Epoch 679, Loss: 909.6680297851562, Val Loss: 228.35546875\n",
      "Epoch 680, Loss: 909.6820678710938, Val Loss: 228.35569763183594\n",
      "Epoch 681, Loss: 909.6904907226562, Val Loss: 228.35597229003906\n",
      "Epoch 682, Loss: 909.6838989257812, Val Loss: 228.35626220703125\n",
      "Epoch 683, Loss: 909.695556640625, Val Loss: 228.3565216064453\n",
      "Epoch 684, Loss: 909.6905517578125, Val Loss: 228.35678100585938\n",
      "Epoch 685, Loss: 909.6885375976562, Val Loss: 228.3569793701172\n",
      "Epoch 686, Loss: 909.6876831054688, Val Loss: 228.3571014404297\n",
      "Epoch 687, Loss: 909.6917114257812, Val Loss: 228.35720825195312\n",
      "Epoch 688, Loss: 909.6879272460938, Val Loss: 228.35720825195312\n",
      "Epoch 689, Loss: 909.6802368164062, Val Loss: 228.35708618164062\n",
      "Epoch 690, Loss: 909.6871337890625, Val Loss: 228.3568572998047\n",
      "Epoch 691, Loss: 909.68701171875, Val Loss: 228.35667419433594\n",
      "Epoch 692, Loss: 909.6893310546875, Val Loss: 228.3565216064453\n",
      "Epoch 693, Loss: 909.686767578125, Val Loss: 228.35633850097656\n",
      "Epoch 694, Loss: 909.6904296875, Val Loss: 228.35621643066406\n",
      "Epoch 695, Loss: 909.6844482421875, Val Loss: 228.3561248779297\n",
      "Epoch 696, Loss: 909.6903686523438, Val Loss: 228.3560028076172\n",
      "Epoch 697, Loss: 909.6797485351562, Val Loss: 228.35589599609375\n",
      "Epoch 698, Loss: 909.6763916015625, Val Loss: 228.3558349609375\n",
      "Epoch 699, Loss: 909.67041015625, Val Loss: 228.3557586669922\n",
      "Epoch 700, Loss: 909.6796875, Val Loss: 228.35572814941406\n",
      "Epoch 701, Loss: 909.6820068359375, Val Loss: 228.35574340820312\n",
      "Epoch 702, Loss: 909.6773071289062, Val Loss: 228.35580444335938\n",
      "Epoch 703, Loss: 909.6761474609375, Val Loss: 228.35585021972656\n",
      "Epoch 704, Loss: 909.6709594726562, Val Loss: 228.3559112548828\n",
      "Epoch 705, Loss: 909.6755981445312, Val Loss: 228.35595703125\n",
      "Epoch 706, Loss: 909.6883544921875, Val Loss: 228.35594177246094\n",
      "Epoch 707, Loss: 909.684814453125, Val Loss: 228.35592651367188\n",
      "Epoch 708, Loss: 909.6949462890625, Val Loss: 228.35598754882812\n",
      "Epoch 709, Loss: 909.671630859375, Val Loss: 228.3560333251953\n",
      "Epoch 710, Loss: 909.6776733398438, Val Loss: 228.3561248779297\n",
      "Epoch 711, Loss: 909.6837768554688, Val Loss: 228.3562469482422\n",
      "Epoch 712, Loss: 909.6734619140625, Val Loss: 228.35635375976562\n",
      "Epoch 713, Loss: 909.668701171875, Val Loss: 228.35638427734375\n",
      "Epoch 714, Loss: 909.6876220703125, Val Loss: 228.3563690185547\n",
      "Epoch 715, Loss: 909.68212890625, Val Loss: 228.3563690185547\n",
      "Epoch 716, Loss: 909.6875610351562, Val Loss: 228.3563690185547\n",
      "Epoch 717, Loss: 909.6790771484375, Val Loss: 228.35635375976562\n",
      "Epoch 718, Loss: 909.676513671875, Val Loss: 228.3562469482422\n",
      "Epoch 719, Loss: 909.6791381835938, Val Loss: 228.35610961914062\n",
      "Epoch 720, Loss: 909.6775512695312, Val Loss: 228.35595703125\n",
      "Epoch 721, Loss: 909.6801147460938, Val Loss: 228.35577392578125\n",
      "Epoch 722, Loss: 909.665771484375, Val Loss: 228.35557556152344\n",
      "Epoch 723, Loss: 909.6754760742188, Val Loss: 228.3554229736328\n",
      "Epoch 724, Loss: 909.677734375, Val Loss: 228.3553466796875\n",
      "Epoch 725, Loss: 909.6808471679688, Val Loss: 228.35528564453125\n",
      "Epoch 726, Loss: 909.6715698242188, Val Loss: 228.3553009033203\n",
      "Epoch 727, Loss: 909.682373046875, Val Loss: 228.35533142089844\n",
      "Epoch 728, Loss: 909.672119140625, Val Loss: 228.35546875\n",
      "Epoch 729, Loss: 909.67041015625, Val Loss: 228.35549926757812\n",
      "Epoch 730, Loss: 909.686279296875, Val Loss: 228.35562133789062\n",
      "Epoch 731, Loss: 909.6829833984375, Val Loss: 228.35580444335938\n",
      "Epoch 732, Loss: 909.671630859375, Val Loss: 228.35594177246094\n",
      "Epoch 733, Loss: 909.6661376953125, Val Loss: 228.35606384277344\n",
      "Epoch 734, Loss: 909.6798706054688, Val Loss: 228.35618591308594\n",
      "Epoch 735, Loss: 909.67138671875, Val Loss: 228.3563232421875\n",
      "Epoch 736, Loss: 909.675048828125, Val Loss: 228.35646057128906\n",
      "Epoch 737, Loss: 909.6820678710938, Val Loss: 228.35650634765625\n",
      "Epoch 738, Loss: 909.6621704101562, Val Loss: 228.35643005371094\n",
      "Epoch 739, Loss: 909.6781616210938, Val Loss: 228.3562774658203\n",
      "Epoch 740, Loss: 909.6773681640625, Val Loss: 228.3560791015625\n",
      "Epoch 741, Loss: 909.6836547851562, Val Loss: 228.35592651367188\n",
      "Epoch 742, Loss: 909.6754760742188, Val Loss: 228.3558349609375\n",
      "Epoch 743, Loss: 909.6806030273438, Val Loss: 228.3557586669922\n",
      "Epoch 744, Loss: 909.6767578125, Val Loss: 228.35569763183594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     12\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Adjust as necessary based on convergence and performance\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    106\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss(y_pred, y_train, selection_prob)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: MAKE AN ACCURATELOSS function\u001b[39;00m\n\u001b[0;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming X_train_tensor and X_test_tensor are already defined and properly shaped\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in the input data\n",
    "feature_dim = input_dim  # The hypernetwork output and prediction input dimensions must match\n",
    "\n",
    "# Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "hypernet = HyperNet(input_dim)\n",
    "model = PredNet(feature_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(list(model.parameters()) + list(hypernet.parameters()), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 1000  # Adjust as necessary based on convergence and performance\n",
    "train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "AveOccup: 0.49326229095458984\n",
      "MedInc: 0.5038533210754395\n",
      "Uniform_Noise: 0.5383767485618591\n",
      "Cosine_Values: 0.47122064232826233\n",
      "AveRooms: 0.4962921142578125\n",
      "HouseAge: 0.5412002801895142\n",
      "Gaussian_Noise: 0.49217385053634644\n",
      "Population: 0.45029646158218384\n",
      "Longitude: 0.5366851091384888\n",
      "AveBedrms: 0.46185818314552307\n",
      "Latitude: 0.4894949793815613\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with the trained models\n",
    "def make_inference(model, hypernet, X_test):\n",
    "    model.eval()\n",
    "    hypernet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the feature selection probabilities from the hypernetwork\n",
    "        selection_prob = hypernet(X_test)\n",
    "        # For inference use the expected values (probabilities) instead of sampling\n",
    "        selected_features = X_test * selection_prob\n",
    "        # Get the model predictions\n",
    "        predictions = model(selected_features)\n",
    "    return predictions, selection_prob\n",
    "\n",
    "# Load the test data (replace this with your actual test data)\n",
    "X_test = X_test_tensor  # Assuming X_test_tensor is your test data\n",
    "\n",
    "# Make inference\n",
    "predictions, feature_importance_probabilities = make_inference(model, hypernet, X_test)\n",
    "\n",
    "\n",
    "# Print the conditional probability of each feature being significant\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    print(f\"{feature_name}: {feature_importance_probabilities[:, i].mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "HouseAge: 0.5412002801895142\n",
      "Uniform_Noise: 0.5383767485618591\n",
      "Longitude: 0.5366851091384888\n",
      "MedInc: 0.5038533210754395\n",
      "AveRooms: 0.4962921142578125\n",
      "AveOccup: 0.49326229095458984\n",
      "Gaussian_Noise: 0.49217385053634644\n",
      "Latitude: 0.4894949793815613\n",
      "Cosine_Values: 0.47122064232826233\n",
      "AveBedrms: 0.46185818314552307\n",
      "Population: 0.45029646158218384\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples (feature_name, probability)\n",
    "feature_probabilities = [(feature_name, feature_importance_probabilities[:, i].mean().item()) for i, feature_name in enumerate(feature_names)]\n",
    "\n",
    "# Sort the list by probability in descending order\n",
    "sorted_feature_probabilities = sorted(feature_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted list\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for feature, probability in sorted_feature_probabilities:\n",
    "    print(f\"{feature}: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mean_feature_importance_probabilities \u001b[38;5;241m=\u001b[39m feature_importance_probabilities\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Determine which features have a mean selection probability above the threshold\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m important_features_indices \u001b[38;5;241m=\u001b[39m mean_feature_importance_probabilities \u001b[38;5;241m>\u001b[39m \u001b[43mthreshold\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean feature selection probabilities from the hypernetwork (higher means more important):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_feature_importance_probabilities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the mean feature importance probability across all examples\n",
    "mean_feature_importance_probabilities = feature_importance_probabilities.mean(dim=0)\n",
    "\n",
    "# Determine which features have a mean selection probability above the threshold\n",
    "important_features_indices = mean_feature_importance_probabilities > threshold\n",
    "\n",
    "print(\"Mean feature selection probabilities from the hypernetwork (higher means more important):\")\n",
    "print(mean_feature_importance_probabilities)\n",
    "\n",
    "print(\"\\nFeatures with mean selection probability higher than threshold:\")\n",
    "print(important_features_indices)\n",
    "\n",
    "# Assuming you have a list of feature names\n",
    "# feature_names = [\"feature1\", \"feature2\", \"...\", \"featureN\"]  # Replace with actual names\n",
    "\n",
    "# Extract the names of the important features\n",
    "important_feature_names = [name for i, name in enumerate(feature_names) if important_features_indices[i]]\n",
    "\n",
    "print(\"\\nImportant feature names:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters of HyperNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1502, -0.1104, -0.0366,  ...,  0.1718,  0.0637, -0.0697],\n",
      "        [-0.1804,  0.0159, -0.1112,  ...,  0.1196, -0.0227, -0.0770],\n",
      "        [ 0.1346,  0.1256,  0.1248,  ..., -0.0737, -0.1655, -0.1717],\n",
      "        ...,\n",
      "        [-0.0565,  0.0891,  0.0982,  ..., -0.1358, -0.1359,  0.0080],\n",
      "        [-0.0074, -0.0703,  0.0896,  ...,  0.2032, -0.2027, -0.1844],\n",
      "        [-0.0535,  0.0381,  0.0786,  ..., -0.1920, -0.0853, -0.0579]])\n",
      "fc1.bias:\n",
      "tensor([ 0.1512,  0.2043,  0.1095, -0.1715, -0.0402, -0.1496, -0.2718,  0.2931,\n",
      "         0.1333,  0.1599,  0.0045, -0.1857, -0.2328,  0.0870,  0.1531, -0.2914,\n",
      "         0.0040,  0.2220, -0.0121, -0.1919,  0.0696, -0.0186,  0.0074,  0.0958,\n",
      "        -0.0544,  0.2858, -0.1110, -0.0218,  0.0621, -0.1147, -0.2755,  0.1071,\n",
      "         0.2763,  0.1478, -0.2083,  0.2526,  0.0627, -0.0947, -0.2117, -0.2135,\n",
      "         0.1645, -0.2223,  0.1970, -0.1847, -0.0560, -0.2816, -0.2027, -0.0012,\n",
      "         0.2529,  0.2172, -0.0118, -0.2407,  0.2348, -0.2373, -0.1128,  0.0839,\n",
      "        -0.1592,  0.2953,  0.1130,  0.0883,  0.2779, -0.0367,  0.1327, -0.0183,\n",
      "        -0.0516, -0.2265, -0.0095, -0.2309,  0.1992, -0.2779, -0.0978, -0.0260,\n",
      "        -0.2853,  0.1937,  0.2322,  0.2955, -0.0263, -0.2422,  0.2143, -0.1737,\n",
      "        -0.0019,  0.0418, -0.2932,  0.1785,  0.2519,  0.1678,  0.2759, -0.2777,\n",
      "        -0.1369, -0.0138,  0.2617, -0.2685, -0.0044,  0.1171, -0.0095,  0.1542,\n",
      "        -0.1603, -0.1790,  0.1629,  0.0338, -0.1578, -0.2427,  0.0014, -0.1207,\n",
      "        -0.1155, -0.0885, -0.1136, -0.1674, -0.1247, -0.2480, -0.1872, -0.1625,\n",
      "        -0.1832,  0.2756, -0.2429,  0.0740, -0.1579,  0.0388, -0.2921, -0.1844,\n",
      "        -0.0605, -0.0986, -0.1727, -0.1732, -0.0819,  0.0992,  0.0258,  0.1645])\n",
      "fc2.weight:\n",
      "tensor([[-0.0785, -0.0891,  0.1382,  ..., -0.0968, -0.0798,  0.0083],\n",
      "        [ 0.1183, -0.1070, -0.1937,  ..., -0.0883, -0.1277, -0.0018],\n",
      "        [-0.1392, -0.2076, -0.0042,  ...,  0.0864, -0.1327, -0.0715],\n",
      "        ...,\n",
      "        [ 0.1834,  0.0056,  0.1638,  ...,  0.0071, -0.0697,  0.1988],\n",
      "        [-0.1742,  0.1368,  0.1087,  ..., -0.1041,  0.0640, -0.1027],\n",
      "        [-0.0524, -0.1429,  0.0062,  ..., -0.1828, -0.0921,  0.1280]])\n",
      "fc2.bias:\n",
      "tensor([-0.0545, -0.0791,  0.0383,  0.0282,  0.0775,  0.0259, -0.0079, -0.0723,\n",
      "         0.0538, -0.0139, -0.0712])\n",
      "\n",
      "Final parameters of PredNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1951,  0.1289,  0.0336,  ...,  0.0818,  0.1140, -0.0908],\n",
      "        [-0.1356,  0.0355,  0.2034,  ...,  0.0818,  0.1826, -0.0745],\n",
      "        [ 0.1945,  0.0366,  0.1083,  ...,  0.1903,  0.1251,  0.1758],\n",
      "        ...,\n",
      "        [-0.0572, -0.0295, -0.1143,  ...,  0.0377,  0.0034,  0.0813],\n",
      "        [ 0.0804,  0.0441,  0.0599,  ...,  0.0587, -0.0560, -0.1215],\n",
      "        [-0.1221, -0.2030,  0.0446,  ..., -0.1810, -0.0804, -0.0874]])\n",
      "fc1.bias:\n",
      "tensor([ 0.0847,  0.2508,  0.0543, -0.1693, -0.2101,  0.0284, -0.1087, -0.0365,\n",
      "        -0.1931, -0.0249,  0.1657, -0.0997,  0.0687, -0.1125, -0.2778,  0.0163,\n",
      "        -0.2127, -0.2116, -0.1091,  0.1054,  0.2549,  0.2965,  0.0145,  0.0660,\n",
      "        -0.1624,  0.2841,  0.0210, -0.0436, -0.1952,  0.2271,  0.2290, -0.2897,\n",
      "         0.2864, -0.2730,  0.2922,  0.0786,  0.2531,  0.1794, -0.1115, -0.2798,\n",
      "         0.0356, -0.1551,  0.0831,  0.2643, -0.1746,  0.2181,  0.0653,  0.0814,\n",
      "        -0.0537,  0.0539,  0.2520,  0.0199,  0.1531,  0.0614,  0.2748, -0.3002,\n",
      "         0.1219, -0.2122, -0.1631, -0.1681, -0.0185,  0.0782, -0.1613, -0.2044,\n",
      "        -0.0142,  0.0375, -0.1722, -0.1976, -0.0613, -0.0207,  0.0363, -0.1575,\n",
      "        -0.0928,  0.1729,  0.2558, -0.1558,  0.2242,  0.2890,  0.2902,  0.0747,\n",
      "        -0.0839, -0.0538, -0.0766, -0.2373,  0.0298, -0.0279,  0.1766,  0.1254,\n",
      "        -0.0312, -0.1889,  0.2507,  0.1399, -0.2916, -0.0606,  0.2620,  0.2371,\n",
      "         0.0271,  0.2982,  0.0881, -0.1257, -0.1925,  0.1016,  0.2289,  0.2053,\n",
      "         0.1121, -0.1936,  0.0434, -0.0033,  0.0369,  0.2812, -0.0046, -0.2101,\n",
      "         0.1726,  0.1283,  0.1392, -0.2868,  0.0306,  0.2251, -0.0042,  0.2264,\n",
      "         0.0210, -0.2270,  0.0823,  0.1144,  0.1638,  0.2373, -0.0120,  0.1358])\n",
      "fc2.weight:\n",
      "tensor([[-0.0340, -0.0058, -0.0467,  0.0282, -0.0404, -0.0728,  0.0139, -0.0756,\n",
      "          0.0479,  0.0861,  0.0223, -0.0131, -0.0110, -0.0324, -0.0306, -0.0440,\n",
      "         -0.0814,  0.0595,  0.0548, -0.0188,  0.0324, -0.0526, -0.0685,  0.0643,\n",
      "          0.0150,  0.0643, -0.0713,  0.0403,  0.0188,  0.0580,  0.0643, -0.0183,\n",
      "         -0.0688,  0.0253, -0.0434,  0.0338, -0.0089,  0.0365, -0.0424, -0.0007,\n",
      "         -0.0031,  0.0186,  0.0339, -0.0069, -0.0783, -0.0002,  0.0574, -0.0139,\n",
      "         -0.0260,  0.0144,  0.0535,  0.0597,  0.0617,  0.0504,  0.0844,  0.0623,\n",
      "         -0.0482,  0.0821, -0.0063,  0.0366, -0.0527, -0.0601,  0.0478,  0.0693,\n",
      "         -0.0706, -0.0636, -0.0599,  0.0776,  0.0848,  0.0028,  0.0122, -0.0097,\n",
      "          0.0428, -0.0329, -0.0786, -0.0789, -0.0787, -0.0558, -0.0111,  0.0623,\n",
      "          0.0209,  0.0124, -0.0683, -0.0567, -0.0454, -0.0577, -0.0457, -0.0199,\n",
      "         -0.0357,  0.0092, -0.0231, -0.0800,  0.0424, -0.0070, -0.0331, -0.0531,\n",
      "         -0.0198, -0.0406, -0.0228,  0.0601, -0.0557, -0.0754,  0.0691,  0.0722,\n",
      "          0.0143,  0.0155,  0.0684, -0.0660, -0.0323,  0.0408,  0.0286, -0.0627,\n",
      "          0.0069,  0.0855, -0.0040, -0.0363,  0.0698,  0.0618,  0.0673,  0.0820,\n",
      "          0.0318,  0.0011, -0.0143, -0.0545,  0.0625,  0.0356, -0.0055, -0.0379]])\n",
      "fc2.bias:\n",
      "tensor([0.0553])\n"
     ]
    }
   ],
   "source": [
    "# ... (after the training loop)\n",
    "\n",
    "# Retrieve the final parameters of HyperNet\n",
    "hypernet_params = hypernet.state_dict()\n",
    "print(\"Final parameters of HyperNet:\")\n",
    "for param_name, param in hypernet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n",
    "\n",
    "# Retrieve the final parameters of PredNet\n",
    "prednet_params = model.state_dict()\n",
    "print(\"\\nFinal parameters of PredNet:\")\n",
    "for param_name, param in prednet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise error\n",
    "raise ValueError(\"This is a custom error message. You can replace this with an actual error message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     53\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not DataFrame"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, input_dim)  # The output should match the number of features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = X_train_tensor.shape[1]  # This should be the number of features in your input\n",
    "feature_dim = input_dim  # This should match the number of input features\n",
    "hypernet = HyperNet(input_dim)\n",
    "model = PredNet(feature_dim)\n",
    "\n",
    "# Rest of the initialization and training code...\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(list(model.parameters()) + list(hypernet.parameters()), lr=0.001)\n",
    "\n",
    "hypernet.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass through hypernetwork to get selection probabilities\n",
    "selection_prob = hypernet(X_train)\n",
    "# Sample from Bernoulli distribution to get feature mask\n",
    "selection_mask = torch.bernoulli(selection_prob)\n",
    "# Apply mask to input features\n",
    "selected_features = X_train * selection_mask\n",
    "\n",
    "# Make predictions with masked features\n",
    "y_pred = model(selected_features)\n",
    "loss = criterion(y_pred, y_train)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Validation phase\n",
    "model.eval()\n",
    "hypernet.eval()\n",
    "# with torch.no_grad():\n",
    "#     val_selection_prob = hypernet(X_val)\n",
    "#     val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "#     val_selected_features = X_val * val_selection_mask\n",
    "#     val_y_pred = model(val_selected_features)\n",
    "#     val_loss = criterion(val_y_pred, y_val)\n",
    "\n",
    "# print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Model initialization\n",
    "\n",
    "\n",
    "# Training\n",
    "# epochs = 100 # Adjust as necessary\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    119\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Adjust as necessary\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     86\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the Hypernetwork\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         # Initialize weights using Xavier initialization\n",
    "#         xavier_uniform_(self.fc1.weight)\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         xavier_uniform_(self.fc2.weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # Final layer with sigmoid to ensure outputs are between 0 and 1\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, feature_dim)  # feature_dim should match the number of input features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the Prediction Network\n",
    "# class PredNet(nn.Module):\n",
    "#     def __init__(self, feature_dim):\n",
    "#         super(PredNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(feature_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, 1) # Assuming a single output\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)  # feature_dim should match the number of input features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = 11\n",
    "feature_dim = input_dim  # Make sure this matches the number of input features\n",
    "hypernet = HyperNet(input_dim, feature_dim)\n",
    "model = PredNet(feature_dim)\n",
    "def train(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.bernoulli import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Hypernetwork, self).__init__()\n",
    "        # Define the architecture of the hypernetwork.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer mapping to Bernoulli parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = torch.sigmoid(self.fc2(x)) # Sigmoid to ensure output is in [0,1], representing probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        # Define the architecture of the prediction network.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer for the response variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = self.fc2(x) # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hypernet, criterion, optimizer, data_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for x, z, y in data_loader: # Assuming x is the feature, z is the context, y is the target\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate probabilities from the Hypernetwork\n",
    "            probs = hypernet(z)\n",
    "            \n",
    "            # Sample from Bernoulli to get the feature selection mask\n",
    "            m = Bernoulli(probs)\n",
    "            mask = m.sample()\n",
    "            \n",
    "            # Apply mask and predict\n",
    "            x_masked = x * mask\n",
    "            predictions = model(x_masked)\n",
    "            \n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
