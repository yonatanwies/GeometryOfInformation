{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AveOccup  MedInc  Uniform_Noise  Cosine_Values  AveRooms  HouseAge  \\\n",
      "0  2.555556  8.3252       0.143745       1.000000  6.984127      41.0   \n",
      "1  2.109842  8.3014      -0.515390       1.000000  6.238137      21.0   \n",
      "2  2.802260  7.2574       0.677635       1.000000  8.288136      52.0   \n",
      "3  2.547945  5.6431      -0.038881       0.999999  5.817352      52.0   \n",
      "4  2.181467  3.8462       0.285882       0.999998  6.281853      52.0   \n",
      "\n",
      "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
      "0        0.496714       322.0    -122.23   1.023810     37.88  \n",
      "1       -0.138264      2401.0    -122.22   0.971880     37.86  \n",
      "2        0.647689       496.0    -122.24   1.073446     37.85  \n",
      "3        1.523030       558.0    -122.25   1.073059     37.85  \n",
      "4       -0.234153       565.0    -122.25   1.081081     37.85  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Values'] = cosine_values\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "print(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    119\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Adjust as necessary\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     86\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the Hypernetwork\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         # Initialize weights using Xavier initialization\n",
    "#         xavier_uniform_(self.fc1.weight)\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         xavier_uniform_(self.fc2.weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # Final layer with sigmoid to ensure outputs are between 0 and 1\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, feature_dim)  # feature_dim should match the number of input features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the Prediction Network\n",
    "# class PredNet(nn.Module):\n",
    "#     def __init__(self, feature_dim):\n",
    "#         super(PredNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(feature_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, 1) # Assuming a single output\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)  # feature_dim should match the number of input features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = 11\n",
    "feature_dim = input_dim  # Make sure this matches the number of input features\n",
    "hypernet = HyperNet(input_dim, feature_dim)\n",
    "model = PredNet(feature_dim)\n",
    "\n",
    "def train(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hypernet.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through hypernetwork to get selection probabilities\n",
    "        selection_prob = hypernet(X_train)\n",
    "        # Sample from Bernoulli distribution to get feature mask\n",
    "        selection_mask = torch.bernoulli(selection_prob)\n",
    "        # Apply mask to input features\n",
    "        selected_features = X_train * selection_mask\n",
    "        \n",
    "        # Make predictions with masked features\n",
    "        y_pred = model(selected_features)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        hypernet.eval()\n",
    "        with torch.no_grad():\n",
    "            val_selection_prob = hypernet(X_val)\n",
    "            val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "            val_selected_features = X_val * val_selection_mask\n",
    "            val_y_pred = model(val_selected_features)\n",
    "            val_loss = criterion(val_y_pred, y_val)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Model initialization\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "feature_dim = input_dim # Assuming context and feature dimensions are the same\n",
    "hypernet = HyperNet(input_dim, feature_dim)\n",
    "model = PredNet(feature_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(list(model.parameters()) + list(hypernet.parameters()), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 100 # Adjust as necessary\n",
    "train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.bernoulli import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Hypernetwork, self).__init__()\n",
    "        # Define the architecture of the hypernetwork.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer mapping to Bernoulli parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = torch.sigmoid(self.fc2(x)) # Sigmoid to ensure output is in [0,1], representing probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        # Define the architecture of the prediction network.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer for the response variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = self.fc2(x) # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hypernet, criterion, optimizer, data_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for x, z, y in data_loader: # Assuming x is the feature, z is the context, y is the target\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate probabilities from the Hypernetwork\n",
    "            probs = hypernet(z)\n",
    "            \n",
    "            # Sample from Bernoulli to get the feature selection mask\n",
    "            m = Bernoulli(probs)\n",
    "            mask = m.sample()\n",
    "            \n",
    "            # Apply mask and predict\n",
    "            x_masked = x * mask\n",
    "            predictions = model(x_masked)\n",
    "            \n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
