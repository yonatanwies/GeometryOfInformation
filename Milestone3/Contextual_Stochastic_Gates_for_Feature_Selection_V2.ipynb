{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Uniform_Noise</th>\n",
       "      <th>Cosine_Noise</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Gaussian_Noise</th>\n",
       "      <th>Population</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.247299</td>\n",
       "      <td>4.5625</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>-0.958842</td>\n",
       "      <td>4.845138</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-0.706843</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>-122.59</td>\n",
       "      <td>1.027611</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.267930</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>-0.134879</td>\n",
       "      <td>0.105413</td>\n",
       "      <td>5.262517</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.869742</td>\n",
       "      <td>2415.0</td>\n",
       "      <td>-119.19</td>\n",
       "      <td>1.012179</td>\n",
       "      <td>34.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.445217</td>\n",
       "      <td>5.2174</td>\n",
       "      <td>0.784740</td>\n",
       "      <td>0.261862</td>\n",
       "      <td>7.306957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.116566</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>-117.21</td>\n",
       "      <td>1.078261</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.345733</td>\n",
       "      <td>2.3083</td>\n",
       "      <td>-0.577435</td>\n",
       "      <td>-0.877524</td>\n",
       "      <td>5.485777</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.198482</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-122.63</td>\n",
       "      <td>1.262582</td>\n",
       "      <td>38.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.496000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.833185</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>5.442667</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.133279</td>\n",
       "      <td>936.0</td>\n",
       "      <td>-117.24</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>34.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveOccup  MedInc  Uniform_Noise  Cosine_Noise  AveRooms  HouseAge  \\\n",
       "0  2.247299  4.5625       0.760274     -0.958842  4.845138      46.0   \n",
       "1  3.267930  4.5000      -0.134879      0.105413  5.262517      17.0   \n",
       "2  3.445217  5.2174       0.784740      0.261862  7.306957       5.0   \n",
       "3  2.345733  2.3083      -0.577435     -0.877524  5.485777      20.0   \n",
       "4  2.496000  6.0000       0.833185      0.042673  5.442667      26.0   \n",
       "\n",
       "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
       "0       -0.706843      1872.0    -122.59   1.027611     37.97  \n",
       "1       -1.869742      2415.0    -119.19   1.012179     34.23  \n",
       "2        0.116566      3962.0    -117.21   1.078261     33.95  \n",
       "3        1.198482      1072.0    -122.63   1.262582     38.96  \n",
       "4       -0.133279       936.0    -117.24   0.781333     34.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Noise'] = cosine_values\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "display(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "# Let's define a function to calculate the custom loss\n",
    "def custom_loss_function(model_output, y_true, mu_d_z, sigma, D, lambda_reg):\n",
    "    # Calculate MSE loss\n",
    "    mse_loss = nn.functional.mse_loss(model_output, y_true)\n",
    "    # Calculate the regularization term\n",
    "    normal_dist = Normal(torch.zeros_like(mu_d_z), torch.ones_like(mu_d_z) * sigma)\n",
    "    regularization_term = torch.sum(normal_dist.cdf(mu_d_z)) / D  # normalize by D\n",
    "    # Combine MSE loss with the regularization term\n",
    "    total_loss = mse_loss + lambda_reg * regularization_term\n",
    "    return total_loss\n",
    "lambda_reg = 0.01\n",
    "sigma = 1.0  # Given constant sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.init import xavier_uniform_\n",
    "    \n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the Hypernetwork\n",
    "class C_StochasticGates(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(C_StochasticGates, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout layer with 50% probability\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        xavier_uniform_(self.fc3.weight)  # Xavier initialization\n",
    "        self.fc4 = nn.Linear(32, input_dim)\n",
    "        xavier_uniform_(self.fc4.weight)  # Xavier initialization\n",
    "        self.fc5 = nn.Linear(64, 1) # final layer for regression\n",
    "        xavier_uniform_(self.fc5.weight)  # Xavier initialization\n",
    "        self.sigma = 1.0  # Given constant sigma\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\" hϕ: selection of contextual probabilities \"\"\"\n",
    "        x_original = x# Copy the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Assuming that self.fc4 outputs the logit z which is fed into the sigmoid to obtain mu_z\n",
    "        mu_z = torch.sigmoid(x)  # Sigmoid to ensure output is a probability\n",
    "\n",
    "        # stochastic_gates = torch.stack(stochastic_gates, dim=1)\n",
    "        stochastic_gates = []\n",
    "        for d in range(mu_z.size(1)):  # Loop over each dimension\n",
    "            epsilon_d = torch.normal(0, self.sigma, size=(mu_z.size(0),), device=mu_z.device)\n",
    "            sigma_d = torch.clamp(mu_z[:, d] + epsilon_d, min=0, max=1)\n",
    "            stochastic_gates.append(sigma_d)\n",
    "\n",
    "        stochastic_gates = torch.stack(stochastic_gates, dim=1)\n",
    "        \"\"\"\" Fully connected network for regression parameters yˆ(k)\"\"\"\n",
    "        selected_features = x_original * stochastic_gates\n",
    "        x = torch.relu(self.fc1(selected_features))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = self.fc5(x)  # No activation function, suitable for regression\n",
    "        return x,stochastic_gates\n",
    "c_stg = C_StochasticGates(input_dim=X_train_scaled.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your custom_loss_function is defined somewhere above\n",
    "\n",
    "def mse_metric(preds, targets):\n",
    "    return torch.mean((preds - targets) ** 2)\n",
    "\n",
    "def rmse_metric(preds, targets):\n",
    "    return torch.sqrt(mse_metric(preds, targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      "Train: \n",
      "  Loss: 568.4468383789062, MSE: 4.833375930786133, RMSE: 2.1984939575195312, \n",
      "Val:\n",
      "   Loss: 144.49537658691406, Val MSE: 4.48626184463501, Val RMSE: 2.118079662322998\n",
      "Epoch 2,\n",
      "Train: \n",
      "  Loss: 567.5112915039062, MSE: 4.478652000427246, RMSE: 2.1162827014923096, \n",
      "Val:\n",
      "   Loss: 144.12757873535156, Val MSE: 4.1342668533325195, Val RMSE: 2.033289670944214\n",
      "Epoch 3,\n",
      "Train: \n",
      "  Loss: 567.1289672851562, MSE: 4.129367828369141, RMSE: 2.0320847034454346, \n",
      "Val:\n",
      "   Loss: 143.71194458007812, Val MSE: 3.8135781288146973, Val RMSE: 1.95283842086792\n",
      "Epoch 4,\n",
      "Train: \n",
      "  Loss: 566.5739135742188, MSE: 3.8911373615264893, RMSE: 1.9725966453552246, \n",
      "Val:\n",
      "   Loss: 143.5850067138672, Val MSE: 3.5433366298675537, Val RMSE: 1.8823752403259277\n",
      "Epoch 5,\n",
      "Train: \n",
      "  Loss: 566.3935546875, MSE: 3.555668830871582, RMSE: 1.8856481313705444, \n",
      "Val:\n",
      "   Loss: 143.20066833496094, Val MSE: 3.261601686477661, Val RMSE: 1.80599045753479\n",
      "Epoch 6,\n",
      "Train: \n",
      "  Loss: 565.8513793945312, MSE: 3.425224781036377, RMSE: 1.8507362604141235, \n",
      "Val:\n",
      "   Loss: 142.8744354248047, Val MSE: 3.043769121170044, Val RMSE: 1.7446401119232178\n",
      "Epoch 7,\n",
      "Train: \n",
      "  Loss: 565.422607421875, MSE: 3.20619535446167, RMSE: 1.7905851602554321, \n",
      "Val:\n",
      "   Loss: 142.8636474609375, Val MSE: 2.805070161819458, Val RMSE: 1.6748343706130981\n",
      "Epoch 8,\n",
      "Train: \n",
      "  Loss: 565.6500244140625, MSE: 2.9557931423187256, RMSE: 1.719241976737976, \n",
      "Val:\n",
      "   Loss: 142.4219207763672, Val MSE: 2.6284382343292236, Val RMSE: 1.6212458610534668\n",
      "Epoch 9,\n",
      "Train: \n",
      "  Loss: 564.9906616210938, MSE: 3.1314854621887207, RMSE: 1.7696003913879395, \n",
      "Val:\n",
      "   Loss: 142.12315368652344, Val MSE: 2.452589988708496, Val RMSE: 1.5660747289657593\n",
      "Epoch 10,\n",
      "Train: \n",
      "  Loss: 564.7216796875, MSE: 2.729959726333618, RMSE: 1.6522589921951294, \n",
      "Val:\n",
      "   Loss: 142.0586700439453, Val MSE: 2.295954465866089, Val RMSE: 1.5152406692504883\n",
      "Epoch 11,\n",
      "Train: \n",
      "  Loss: 564.5928955078125, MSE: 2.5210108757019043, RMSE: 1.5877691507339478, \n",
      "Val:\n",
      "   Loss: 141.92929077148438, Val MSE: 2.1788175106048584, Val RMSE: 1.4760818481445312\n",
      "Epoch 12,\n",
      "Train: \n",
      "  Loss: 564.2500610351562, MSE: 2.3673763275146484, RMSE: 1.538628101348877, \n",
      "Val:\n",
      "   Loss: 141.6753387451172, Val MSE: 2.0438899993896484, Val RMSE: 1.4296468496322632\n",
      "Epoch 13,\n",
      "Train: \n",
      "  Loss: 563.3085327148438, MSE: 2.367917776107788, RMSE: 1.538804054260254, \n",
      "Val:\n",
      "   Loss: 141.70005798339844, Val MSE: 1.9457048177719116, Val RMSE: 1.3948851823806763\n",
      "Epoch 14,\n",
      "Train: \n",
      "  Loss: 563.3631591796875, MSE: 2.204801082611084, RMSE: 1.484857201576233, \n",
      "Val:\n",
      "   Loss: 141.5182342529297, Val MSE: 1.872414469718933, Val RMSE: 1.3683619499206543\n",
      "Epoch 15,\n",
      "Train: \n",
      "  Loss: 562.84228515625, MSE: 2.1753222942352295, RMSE: 1.4748973846435547, \n",
      "Val:\n",
      "   Loss: 141.56590270996094, Val MSE: 1.7777752876281738, Val RMSE: 1.3333324193954468\n",
      "Epoch 16,\n",
      "Train: \n",
      "  Loss: 563.47021484375, MSE: 2.310089349746704, RMSE: 1.5198978185653687, \n",
      "Val:\n",
      "   Loss: 141.67967224121094, Val MSE: 1.7138643264770508, Val RMSE: 1.3091464042663574\n",
      "Epoch 17,\n",
      "Train: \n",
      "  Loss: 562.8109741210938, MSE: 2.0761871337890625, RMSE: 1.440898060798645, \n",
      "Val:\n",
      "   Loss: 141.75369262695312, Val MSE: 1.6768914461135864, Val RMSE: 1.2949484586715698\n",
      "Epoch 18,\n",
      "Train: \n",
      "  Loss: 563.2509765625, MSE: 2.051959991455078, RMSE: 1.4324663877487183, \n",
      "Val:\n",
      "   Loss: 141.5731964111328, Val MSE: 1.630379557609558, Val RMSE: 1.2768632173538208\n",
      "Epoch 19,\n",
      "Train: \n",
      "  Loss: 562.8768920898438, MSE: 1.9292261600494385, RMSE: 1.3889658451080322, \n",
      "Val:\n",
      "   Loss: 141.32305908203125, Val MSE: 1.575268030166626, Val RMSE: 1.2550967931747437\n",
      "Epoch 20,\n",
      "Train: \n",
      "  Loss: 562.4854125976562, MSE: 2.0453202724456787, RMSE: 1.4301469326019287, \n",
      "Val:\n",
      "   Loss: 141.3649139404297, Val MSE: 1.5531171560287476, Val RMSE: 1.2462412118911743\n",
      "Epoch 21,\n",
      "Train: \n",
      "  Loss: 562.2818603515625, MSE: 1.9583884477615356, RMSE: 1.3994243144989014, \n",
      "Val:\n",
      "   Loss: 141.14732360839844, Val MSE: 1.5481011867523193, Val RMSE: 1.2442271709442139\n",
      "Epoch 22,\n",
      "Train: \n",
      "  Loss: 562.1103515625, MSE: 1.8708970546722412, RMSE: 1.367807388305664, \n",
      "Val:\n",
      "   Loss: 141.21226501464844, Val MSE: 1.5171452760696411, Val RMSE: 1.231724500656128\n",
      "Epoch 23,\n",
      "Train: \n",
      "  Loss: 562.4069213867188, MSE: 1.9264978170394897, RMSE: 1.3879833221435547, \n",
      "Val:\n",
      "   Loss: 141.3659210205078, Val MSE: 1.497175931930542, Val RMSE: 1.2235914468765259\n",
      "Epoch 24,\n",
      "Train: \n",
      "  Loss: 562.5504150390625, MSE: 1.8494819402694702, RMSE: 1.3599566221237183, \n",
      "Val:\n",
      "   Loss: 141.2376708984375, Val MSE: 1.5101584196090698, Val RMSE: 1.2288850545883179\n",
      "Epoch 25,\n",
      "Train: \n",
      "  Loss: 562.3818359375, MSE: 2.0001919269561768, RMSE: 1.4142813682556152, \n",
      "Val:\n",
      "   Loss: 141.24118041992188, Val MSE: 1.4978176355361938, Val RMSE: 1.223853588104248\n",
      "Epoch 26,\n",
      "Train: \n",
      "  Loss: 562.0767211914062, MSE: 1.9052983522415161, RMSE: 1.380325436592102, \n",
      "Val:\n",
      "   Loss: 140.95083618164062, Val MSE: 1.4907503128051758, Val RMSE: 1.2209628820419312\n",
      "Epoch 27,\n",
      "Train: \n",
      "  Loss: 561.73828125, MSE: 1.8596677780151367, RMSE: 1.3636963367462158, \n",
      "Val:\n",
      "   Loss: 141.45693969726562, Val MSE: 1.4812901020050049, Val RMSE: 1.2170826196670532\n",
      "Epoch 28,\n",
      "Train: \n",
      "  Loss: 561.8204956054688, MSE: 1.8414021730422974, RMSE: 1.356982707977295, \n",
      "Val:\n",
      "   Loss: 141.2925262451172, Val MSE: 1.4734901189804077, Val RMSE: 1.2138739824295044\n",
      "Epoch 29,\n",
      "Train: \n",
      "  Loss: 561.730224609375, MSE: 2.0988306999206543, RMSE: 1.448734164237976, \n",
      "Val:\n",
      "   Loss: 141.32542419433594, Val MSE: 1.4900346994400024, Val RMSE: 1.2206697463989258\n",
      "Epoch 30,\n",
      "Train: \n",
      "  Loss: 561.6817626953125, MSE: 1.8386024236679077, RMSE: 1.3559507131576538, \n",
      "Val:\n",
      "   Loss: 140.92654418945312, Val MSE: 1.4884165525436401, Val RMSE: 1.2200068235397339\n",
      "Epoch 31,\n",
      "Train: \n",
      "  Loss: 561.6091918945312, MSE: 1.8541011810302734, RMSE: 1.3616538047790527, \n",
      "Val:\n",
      "   Loss: 141.42747497558594, Val MSE: 1.4906690120697021, Val RMSE: 1.2209296226501465\n",
      "Epoch 32,\n",
      "Train: \n",
      "  Loss: 561.8929443359375, MSE: 2.003229856491089, RMSE: 1.4153550863265991, \n",
      "Val:\n",
      "   Loss: 141.09385681152344, Val MSE: 1.4965460300445557, Val RMSE: 1.2233339548110962\n",
      "Epoch 33,\n",
      "Train: \n",
      "  Loss: 561.1526489257812, MSE: 1.815300703048706, RMSE: 1.347330927848816, \n",
      "Val:\n",
      "   Loss: 141.11964416503906, Val MSE: 1.512786626815796, Val RMSE: 1.2299538850784302\n",
      "Epoch 34,\n",
      "Train: \n",
      "  Loss: 561.4539794921875, MSE: 1.8443914651870728, RMSE: 1.358083724975586, \n",
      "Val:\n",
      "   Loss: 141.0604248046875, Val MSE: 1.51594877243042, Val RMSE: 1.2312387228012085\n",
      "Epoch 35,\n",
      "Train: \n",
      "  Loss: 561.8084106445312, MSE: 1.8551952838897705, RMSE: 1.3620555400848389, \n",
      "Val:\n",
      "   Loss: 141.23574829101562, Val MSE: 1.5171576738357544, Val RMSE: 1.231729507446289\n",
      "Epoch 36,\n",
      "Train: \n",
      "  Loss: 561.7784423828125, MSE: 1.9583841562271118, RMSE: 1.3994227647781372, \n",
      "Val:\n",
      "   Loss: 141.09933471679688, Val MSE: 1.5275676250457764, Val RMSE: 1.235948085784912\n",
      "Epoch 37,\n",
      "Train: \n",
      "  Loss: 561.1806640625, MSE: 1.8873496055603027, RMSE: 1.3738083839416504, \n",
      "Val:\n",
      "   Loss: 141.3422393798828, Val MSE: 1.5283796787261963, Val RMSE: 1.2362765073776245\n",
      "Epoch 38,\n",
      "Train: \n",
      "  Loss: 561.0535278320312, MSE: 1.8399511575698853, RMSE: 1.3564479351043701, \n",
      "Val:\n",
      "   Loss: 141.23818969726562, Val MSE: 1.5400898456573486, Val RMSE: 1.2410035133361816\n",
      "Epoch 39,\n",
      "Train: \n",
      "  Loss: 561.1693115234375, MSE: 1.7915979623794556, RMSE: 1.3385058641433716, \n",
      "Val:\n",
      "   Loss: 141.31179809570312, Val MSE: 1.528022050857544, Val RMSE: 1.2361319065093994\n",
      "Epoch 40,\n",
      "Train: \n",
      "  Loss: 561.1751098632812, MSE: 1.8012123107910156, RMSE: 1.342092514038086, \n",
      "Val:\n",
      "   Loss: 141.1339111328125, Val MSE: 1.5278549194335938, Val RMSE: 1.2360643148422241\n",
      "Epoch 41,\n",
      "Train: \n",
      "  Loss: 560.8868408203125, MSE: 1.827538251876831, RMSE: 1.3518646955490112, \n",
      "Val:\n",
      "   Loss: 141.0423583984375, Val MSE: 1.5186296701431274, Val RMSE: 1.2323269844055176\n",
      "Epoch 42,\n",
      "Train: \n",
      "  Loss: 561.6698608398438, MSE: 2.201714038848877, RMSE: 1.4838173389434814, \n",
      "Val:\n",
      "   Loss: 141.054443359375, Val MSE: 1.5226852893829346, Val RMSE: 1.233971357345581\n",
      "Epoch 43,\n",
      "Train: \n",
      "  Loss: 560.8843383789062, MSE: 1.8043298721313477, RMSE: 1.343253493309021, \n",
      "Val:\n",
      "   Loss: 140.91697692871094, Val MSE: 1.5219002962112427, Val RMSE: 1.23365318775177\n",
      "Epoch 44,\n",
      "Train: \n",
      "  Loss: 561.0294799804688, MSE: 1.8034636974334717, RMSE: 1.3429310321807861, \n",
      "Val:\n",
      "   Loss: 140.83065795898438, Val MSE: 1.514023780822754, Val RMSE: 1.2304567098617554\n",
      "Epoch 45,\n",
      "Train: \n",
      "  Loss: 560.4979248046875, MSE: 1.790922999382019, RMSE: 1.3382537364959717, \n",
      "Val:\n",
      "   Loss: 141.10076904296875, Val MSE: 1.5031931400299072, Val RMSE: 1.2260477542877197\n",
      "Epoch 46,\n",
      "Train: \n",
      "  Loss: 560.4158325195312, MSE: 1.775618076324463, RMSE: 1.332523226737976, \n",
      "Val:\n",
      "   Loss: 140.9680633544922, Val MSE: 1.496582269668579, Val RMSE: 1.2233487367630005\n",
      "Epoch 47,\n",
      "Train: \n",
      "  Loss: 561.3861083984375, MSE: 1.8083961009979248, RMSE: 1.3447661399841309, \n",
      "Val:\n",
      "   Loss: 141.11231994628906, Val MSE: 1.491181492805481, Val RMSE: 1.2211394309997559\n",
      "Epoch 48,\n",
      "Train: \n",
      "  Loss: 561.125244140625, MSE: 1.7565726041793823, RMSE: 1.3253575563430786, \n",
      "Val:\n",
      "   Loss: 141.1560516357422, Val MSE: 1.4745416641235352, Val RMSE: 1.2143070697784424\n",
      "Epoch 49,\n",
      "Train: \n",
      "  Loss: 560.5772094726562, MSE: 1.8210006952285767, RMSE: 1.3494446277618408, \n",
      "Val:\n",
      "   Loss: 140.9029541015625, Val MSE: 1.4671096801757812, Val RMSE: 1.2112430334091187\n",
      "Epoch 50,\n",
      "Train: \n",
      "  Loss: 560.6068725585938, MSE: 1.749009132385254, RMSE: 1.3225010633468628, \n",
      "Val:\n",
      "   Loss: 141.08108520507812, Val MSE: 1.4579416513442993, Val RMSE: 1.2074525356292725\n",
      "Epoch 51,\n",
      "Train: \n",
      "  Loss: 560.7533569335938, MSE: 1.7517669200897217, RMSE: 1.3235433101654053, \n",
      "Val:\n",
      "   Loss: 140.9846954345703, Val MSE: 1.443786859512329, Val RMSE: 1.201576828956604\n",
      "Epoch 52,\n",
      "Train: \n",
      "  Loss: 560.89794921875, MSE: 1.7843133211135864, RMSE: 1.3357819318771362, \n",
      "Val:\n",
      "   Loss: 140.83624267578125, Val MSE: 1.44171941280365, Val RMSE: 1.200716257095337\n",
      "Epoch 53,\n",
      "Train: \n",
      "  Loss: 560.5724487304688, MSE: 1.7615208625793457, RMSE: 1.3272229433059692, \n",
      "Val:\n",
      "   Loss: 140.9795379638672, Val MSE: 1.4265005588531494, Val RMSE: 1.1943620443344116\n",
      "Epoch 54,\n",
      "Train: \n",
      "  Loss: 560.9097290039062, MSE: 1.7640454769134521, RMSE: 1.3281737565994263, \n",
      "Val:\n",
      "   Loss: 141.02520751953125, Val MSE: 1.4271399974822998, Val RMSE: 1.1946296691894531\n",
      "Epoch 55,\n",
      "Train: \n",
      "  Loss: 560.4563598632812, MSE: 1.7099058628082275, RMSE: 1.307633638381958, \n",
      "Val:\n",
      "   Loss: 140.78773498535156, Val MSE: 1.420054316520691, Val RMSE: 1.1916602849960327\n",
      "Epoch 56,\n",
      "Train: \n",
      "  Loss: 560.315185546875, MSE: 1.7307860851287842, RMSE: 1.3155934810638428, \n",
      "Val:\n",
      "   Loss: 140.9313201904297, Val MSE: 1.4140592813491821, Val RMSE: 1.1891422271728516\n",
      "Epoch 57,\n",
      "Train: \n",
      "  Loss: 560.744140625, MSE: 1.7201755046844482, RMSE: 1.3115546703338623, \n",
      "Val:\n",
      "   Loss: 140.96556091308594, Val MSE: 1.4104945659637451, Val RMSE: 1.1876424551010132\n",
      "Epoch 58,\n",
      "Train: \n",
      "  Loss: 560.615966796875, MSE: 1.7213764190673828, RMSE: 1.3120123147964478, \n",
      "Val:\n",
      "   Loss: 140.79721069335938, Val MSE: 1.4113109111785889, Val RMSE: 1.1879860162734985\n",
      "Epoch 59,\n",
      "Train: \n",
      "  Loss: 560.4677124023438, MSE: 1.7048534154891968, RMSE: 1.3057003021240234, \n",
      "Val:\n",
      "   Loss: 140.7664794921875, Val MSE: 1.4043625593185425, Val RMSE: 1.1850579977035522\n",
      "Epoch 60,\n",
      "Train: \n",
      "  Loss: 560.804931640625, MSE: 1.7154088020324707, RMSE: 1.3097361326217651, \n",
      "Val:\n",
      "   Loss: 141.24754333496094, Val MSE: 1.404175043106079, Val RMSE: 1.18497896194458\n",
      "Epoch 61,\n",
      "Train: \n",
      "  Loss: 559.9360961914062, MSE: 1.6998350620269775, RMSE: 1.3037772178649902, \n",
      "Val:\n",
      "   Loss: 140.9008331298828, Val MSE: 1.404778242111206, Val RMSE: 1.1852333545684814\n",
      "Epoch 62,\n",
      "Train: \n",
      "  Loss: 560.6624755859375, MSE: 1.7911944389343262, RMSE: 1.3383551836013794, \n",
      "Val:\n",
      "   Loss: 140.79046630859375, Val MSE: 1.4112881422042847, Val RMSE: 1.1879764795303345\n",
      "Epoch 63,\n",
      "Train: \n",
      "  Loss: 559.9157104492188, MSE: 1.7252249717712402, RMSE: 1.3134782314300537, \n",
      "Val:\n",
      "   Loss: 140.80235290527344, Val MSE: 1.401942491531372, Val RMSE: 1.1840364933013916\n",
      "Epoch 64,\n",
      "Train: \n",
      "  Loss: 560.513916015625, MSE: 1.7036608457565308, RMSE: 1.3052436113357544, \n",
      "Val:\n",
      "   Loss: 140.8314208984375, Val MSE: 1.4060252904891968, Val RMSE: 1.185759425163269\n",
      "Epoch 65,\n",
      "Train: \n",
      "  Loss: 560.2899169921875, MSE: 1.7537955045700073, RMSE: 1.3243094682693481, \n",
      "Val:\n",
      "   Loss: 140.94772338867188, Val MSE: 1.4070260524749756, Val RMSE: 1.1861813068389893\n",
      "Epoch 66,\n",
      "Train: \n",
      "  Loss: 560.0164794921875, MSE: 1.7152296304702759, RMSE: 1.309667706489563, \n",
      "Val:\n",
      "   Loss: 140.95013427734375, Val MSE: 1.4103552103042603, Val RMSE: 1.1875838041305542\n",
      "Epoch 67,\n",
      "Train: \n",
      "  Loss: 560.7637939453125, MSE: 1.7247309684753418, RMSE: 1.3132901191711426, \n",
      "Val:\n",
      "   Loss: 140.90061950683594, Val MSE: 1.4110652208328247, Val RMSE: 1.187882661819458\n",
      "Epoch 68,\n",
      "Train: \n",
      "  Loss: 560.5300903320312, MSE: 1.6821935176849365, RMSE: 1.2969940900802612, \n",
      "Val:\n",
      "   Loss: 140.8832550048828, Val MSE: 1.414006233215332, Val RMSE: 1.1891199350357056\n",
      "Epoch 69,\n",
      "Train: \n",
      "  Loss: 560.1158447265625, MSE: 1.6948773860931396, RMSE: 1.3018745183944702, \n",
      "Val:\n",
      "   Loss: 141.1240234375, Val MSE: 1.412672519683838, Val RMSE: 1.1885590553283691\n",
      "Epoch 70,\n",
      "Train: \n",
      "  Loss: 560.8755493164062, MSE: 1.7164902687072754, RMSE: 1.3101489543914795, \n",
      "Val:\n",
      "   Loss: 140.80548095703125, Val MSE: 1.420059084892273, Val RMSE: 1.191662311553955\n",
      "Epoch 71,\n",
      "Train: \n",
      "  Loss: 560.0555419921875, MSE: 1.7193934917449951, RMSE: 1.3112564086914062, \n",
      "Val:\n",
      "   Loss: 140.91848754882812, Val MSE: 1.4193354845046997, Val RMSE: 1.1913586854934692\n",
      "Epoch 72,\n",
      "Train: \n",
      "  Loss: 560.0847778320312, MSE: 1.699596643447876, RMSE: 1.3036857843399048, \n",
      "Val:\n",
      "   Loss: 141.061767578125, Val MSE: 1.4090056419372559, Val RMSE: 1.187015414237976\n",
      "Epoch 73,\n",
      "Train: \n",
      "  Loss: 560.1981201171875, MSE: 1.8201489448547363, RMSE: 1.3491289615631104, \n",
      "Val:\n",
      "   Loss: 140.8156280517578, Val MSE: 1.4128804206848145, Val RMSE: 1.1886464357376099\n",
      "Epoch 74,\n",
      "Train: \n",
      "  Loss: 559.6611328125, MSE: 1.7410058975219727, RMSE: 1.319471836090088, \n",
      "Val:\n",
      "   Loss: 140.92794799804688, Val MSE: 1.4088836908340454, Val RMSE: 1.1869640350341797\n",
      "Epoch 75,\n",
      "Train: \n",
      "  Loss: 560.1316528320312, MSE: 1.7974258661270142, RMSE: 1.3406810760498047, \n",
      "Val:\n",
      "   Loss: 140.8833465576172, Val MSE: 1.4133979082107544, Val RMSE: 1.1888641119003296\n",
      "Epoch 76,\n",
      "Train: \n",
      "  Loss: 560.1276245117188, MSE: 1.6885337829589844, RMSE: 1.2994359731674194, \n",
      "Val:\n",
      "   Loss: 140.72598266601562, Val MSE: 1.4074435234069824, Val RMSE: 1.1863572597503662\n",
      "Epoch 77,\n",
      "Train: \n",
      "  Loss: 560.3304443359375, MSE: 1.6987980604171753, RMSE: 1.3033795356750488, \n",
      "Val:\n",
      "   Loss: 140.96925354003906, Val MSE: 1.408355951309204, Val RMSE: 1.1867417097091675\n",
      "Epoch 78,\n",
      "Train: \n",
      "  Loss: 559.9395751953125, MSE: 1.6911280155181885, RMSE: 1.3004337549209595, \n",
      "Val:\n",
      "   Loss: 141.04876708984375, Val MSE: 1.4024156332015991, Val RMSE: 1.1842362880706787\n",
      "Epoch 79,\n",
      "Train: \n",
      "  Loss: 560.6973266601562, MSE: 1.705566644668579, RMSE: 1.3059734106063843, \n",
      "Val:\n",
      "   Loss: 141.0147705078125, Val MSE: 1.4047735929489136, Val RMSE: 1.1852314472198486\n",
      "Epoch 80,\n",
      "Train: \n",
      "  Loss: 559.7972412109375, MSE: 1.6815569400787354, RMSE: 1.2967486381530762, \n",
      "Val:\n",
      "   Loss: 141.08204650878906, Val MSE: 1.4002392292022705, Val RMSE: 1.1833170652389526\n",
      "Epoch 81,\n",
      "Train: \n",
      "  Loss: 560.37890625, MSE: 1.7595484256744385, RMSE: 1.3264796733856201, \n",
      "Val:\n",
      "   Loss: 140.82408142089844, Val MSE: 1.3996986150741577, Val RMSE: 1.1830885410308838\n",
      "Epoch 82,\n",
      "Train: \n",
      "  Loss: 559.492431640625, MSE: 1.6892234086990356, RMSE: 1.2997013330459595, \n",
      "Val:\n",
      "   Loss: 141.0885772705078, Val MSE: 1.3923310041427612, Val RMSE: 1.1799707412719727\n",
      "Epoch 83,\n",
      "Train: \n",
      "  Loss: 560.1914672851562, MSE: 1.6947917938232422, RMSE: 1.3018417358398438, \n",
      "Val:\n",
      "   Loss: 141.01412963867188, Val MSE: 1.388407588005066, Val RMSE: 1.178307056427002\n",
      "Epoch 84,\n",
      "Train: \n",
      "  Loss: 560.3095703125, MSE: 1.7223916053771973, RMSE: 1.31239914894104, \n",
      "Val:\n",
      "   Loss: 140.8787384033203, Val MSE: 1.3937633037567139, Val RMSE: 1.1805775165557861\n",
      "Epoch 85,\n",
      "Train: \n",
      "  Loss: 559.9218139648438, MSE: 1.6905889511108398, RMSE: 1.3002264499664307, \n",
      "Val:\n",
      "   Loss: 140.9349822998047, Val MSE: 1.3875585794448853, Val RMSE: 1.1779468059539795\n",
      "Epoch 86,\n",
      "Train: \n",
      "  Loss: 560.283203125, MSE: 1.6731611490249634, RMSE: 1.2935073375701904, \n",
      "Val:\n",
      "   Loss: 140.91810607910156, Val MSE: 1.38540518283844, Val RMSE: 1.1770323514938354\n",
      "Epoch 87,\n",
      "Train: \n",
      "  Loss: 559.9618530273438, MSE: 1.689542293548584, RMSE: 1.2998239994049072, \n",
      "Val:\n",
      "   Loss: 141.07179260253906, Val MSE: 1.3835591077804565, Val RMSE: 1.1762478351593018\n",
      "Epoch 88,\n",
      "Train: \n",
      "  Loss: 560.119140625, MSE: 1.6906524896621704, RMSE: 1.3002508878707886, \n",
      "Val:\n",
      "   Loss: 140.94041442871094, Val MSE: 1.3785053491592407, Val RMSE: 1.1740976572036743\n",
      "Epoch 89,\n",
      "Train: \n",
      "  Loss: 559.9810791015625, MSE: 1.6955469846725464, RMSE: 1.3021316528320312, \n",
      "Val:\n",
      "   Loss: 140.74270629882812, Val MSE: 1.380907654762268, Val RMSE: 1.175120234489441\n",
      "Epoch 90,\n",
      "Train: \n",
      "  Loss: 560.2174682617188, MSE: 1.6395323276519775, RMSE: 1.280442237854004, \n",
      "Val:\n",
      "   Loss: 140.94143676757812, Val MSE: 1.376537799835205, Val RMSE: 1.1732594966888428\n",
      "Epoch 91,\n",
      "Train: \n",
      "  Loss: 559.8712158203125, MSE: 1.6850661039352417, RMSE: 1.2981009483337402, \n",
      "Val:\n",
      "   Loss: 141.05540466308594, Val MSE: 1.378169298171997, Val RMSE: 1.1739546060562134\n",
      "Epoch 92,\n",
      "Train: \n",
      "  Loss: 560.05615234375, MSE: 1.670548439025879, RMSE: 1.292496919631958, \n",
      "Val:\n",
      "   Loss: 140.7573699951172, Val MSE: 1.3822797536849976, Val RMSE: 1.1757038831710815\n",
      "Epoch 93,\n",
      "Train: \n",
      "  Loss: 559.6824951171875, MSE: 1.6811491250991821, RMSE: 1.2965914011001587, \n",
      "Val:\n",
      "   Loss: 140.9775390625, Val MSE: 1.384725570678711, Val RMSE: 1.1767436265945435\n",
      "Epoch 94,\n",
      "Train: \n",
      "  Loss: 560.1095581054688, MSE: 1.6632431745529175, RMSE: 1.2896678447723389, \n",
      "Val:\n",
      "   Loss: 140.85089111328125, Val MSE: 1.3803969621658325, Val RMSE: 1.1749029159545898\n",
      "Epoch 95,\n",
      "Train: \n",
      "  Loss: 559.98095703125, MSE: 1.7380496263504028, RMSE: 1.318351149559021, \n",
      "Val:\n",
      "   Loss: 140.9664306640625, Val MSE: 1.378783941268921, Val RMSE: 1.1742162704467773\n",
      "Epoch 96,\n",
      "Train: \n",
      "  Loss: 559.9945678710938, MSE: 1.6621628999710083, RMSE: 1.2892489433288574, \n",
      "Val:\n",
      "   Loss: 140.76084899902344, Val MSE: 1.3815840482711792, Val RMSE: 1.1754080057144165\n",
      "Epoch 97,\n",
      "Train: \n",
      "  Loss: 559.5111694335938, MSE: 1.6597535610198975, RMSE: 1.2883142232894897, \n",
      "Val:\n",
      "   Loss: 140.89634704589844, Val MSE: 1.382660150527954, Val RMSE: 1.175865650177002\n",
      "Epoch 98,\n",
      "Train: \n",
      "  Loss: 559.9506225585938, MSE: 1.6712695360183716, RMSE: 1.2927758693695068, \n",
      "Val:\n",
      "   Loss: 141.08572387695312, Val MSE: 1.380065679550171, Val RMSE: 1.1747620105743408\n",
      "Epoch 99,\n",
      "Train: \n",
      "  Loss: 560.1675415039062, MSE: 1.6644078493118286, RMSE: 1.2901192903518677, \n",
      "Val:\n",
      "   Loss: 141.06185913085938, Val MSE: 1.3773164749145508, Val RMSE: 1.1735912561416626\n",
      "Epoch 100,\n",
      "Train: \n",
      "  Loss: 560.2891235351562, MSE: 1.6498264074325562, RMSE: 1.28445565700531, \n",
      "Val:\n",
      "   Loss: 140.89303588867188, Val MSE: 1.380043625831604, Val RMSE: 1.1747525930404663\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, stochastic_gates_pred = model(X_train)\n",
    "        loss = custom_loss_function(y_pred, y_train, stochastic_gates_pred, sigma=1, D=X_train.shape[1], lambda_reg=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse = mse_metric(y_pred, y_train).item()\n",
    "        train_rmse = rmse_metric(y_pred, y_train).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_pred, val_stochastic_gates = model(X_val)\n",
    "            val_loss = custom_loss_function(val_y_pred, y_val, val_stochastic_gates, sigma=1, D=X_val.shape[1], lambda_reg=0.05)\n",
    "            val_mse = mse_metric(val_y_pred, y_val).item()\n",
    "            val_rmse = rmse_metric(val_y_pred, y_val).item()\n",
    "        #\"Gates: {stochastic_gates_pred}\\n \n",
    "        print(f'Epoch {epoch+1},\\nTrain: \\n  Loss: {loss.item()}, MSE: {train_mse}, RMSE: {train_rmse}, \\nVal:\\n   Loss: {val_loss.item()}, Val MSE: {val_mse}, Val RMSE: {val_rmse}')\n",
    "    return stochastic_gates_pred\n",
    "\n",
    "# Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "model = C_StochasticGates(input_dim=X_train_tensor.shape[1])\n",
    "optimizer = Adam(list(model.parameters()) , lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 100  # Adjust as necessary based on convergence and performance\n",
    "gates = train(model,  optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      "Train: Loss: 571.0518188476562, MSE: 7.17909574508667, RMSE: 2.6793835163116455, R^2: -4.370446681976318\n",
      "Val: Loss: 145.882080078125, Val MSE: 5.757336616516113, Val RMSE: 2.399445056915283, Val R^2: -3.3935413360595703\n",
      "Epoch 2,\n",
      "Train: Loss: 570.5059204101562, MSE: 6.826672077178955, RMSE: 2.612790107727051, R^2: -4.106810092926025\n",
      "Val: Loss: 145.67938232421875, Val MSE: 5.391133785247803, Val RMSE: 2.3218815326690674, Val R^2: -3.114084243774414\n",
      "Epoch 3,\n",
      "Train: Loss: 569.3228759765625, MSE: 6.30553674697876, RMSE: 2.511082887649536, R^2: -3.716965675354004\n",
      "Val: Loss: 145.19595336914062, Val MSE: 5.089291095733643, Val RMSE: 2.2559456825256348, Val R^2: -2.883742332458496\n",
      "Epoch 4,\n",
      "Train: Loss: 568.7809448242188, MSE: 5.924668788909912, RMSE: 2.4340641498565674, R^2: -3.432051181793213\n",
      "Val: Loss: 144.83914184570312, Val MSE: 4.781925678253174, Val RMSE: 2.1867613792419434, Val R^2: -2.6491854190826416\n",
      "Epoch 5,\n",
      "Train: Loss: 567.9943237304688, MSE: 5.593095779418945, RMSE: 2.3649725914001465, R^2: -3.184011936187744\n",
      "Val: Loss: 144.48587036132812, Val MSE: 4.507302284240723, Val RMSE: 2.1230409145355225, Val R^2: -2.439614772796631\n",
      "Epoch 6,\n",
      "Train: Loss: 567.843017578125, MSE: 5.240457057952881, RMSE: 2.2892043590545654, R^2: -2.9202146530151367\n",
      "Val: Loss: 144.01881408691406, Val MSE: 4.247859477996826, Val RMSE: 2.0610334873199463, Val R^2: -2.241628646850586\n",
      "Epoch 7,\n",
      "Train: Loss: 567.2328491210938, MSE: 4.95249080657959, RMSE: 2.225419282913208, R^2: -2.704796552658081\n",
      "Val: Loss: 144.00975036621094, Val MSE: 4.031311988830566, Val RMSE: 2.007812738418579, Val R^2: -2.0763766765594482\n",
      "Epoch 8,\n",
      "Train: Loss: 567.069091796875, MSE: 4.678267478942871, RMSE: 2.1629302501678467, R^2: -2.4996588230133057\n",
      "Val: Loss: 143.6614990234375, Val MSE: 3.8332271575927734, Val RMSE: 1.9578628540039062, Val R^2: -1.9252140522003174\n",
      "Epoch 9,\n",
      "Train: Loss: 565.4830932617188, MSE: 4.429498195648193, RMSE: 2.104637384414673, R^2: -2.3135626316070557\n",
      "Val: Loss: 143.64097595214844, Val MSE: 3.645580530166626, Val RMSE: 1.9093403816223145, Val R^2: -1.7820169925689697\n",
      "Epoch 10,\n",
      "Train: Loss: 565.6377563476562, MSE: 4.202585697174072, RMSE: 2.050020933151245, R^2: -2.1438167095184326\n",
      "Val: Loss: 143.56011962890625, Val MSE: 3.4617886543273926, Val RMSE: 1.8605883121490479, Val R^2: -1.6417617797851562\n",
      "Epoch 11,\n",
      "Train: Loss: 565.7581176757812, MSE: 4.017095565795898, RMSE: 2.0042693614959717, R^2: -2.0050578117370605\n",
      "Val: Loss: 143.16445922851562, Val MSE: 3.2965450286865234, Val RMSE: 1.8156390190124512, Val R^2: -1.5156610012054443\n",
      "Epoch 12,\n",
      "Train: Loss: 565.7452392578125, MSE: 3.8018155097961426, RMSE: 1.9498244524002075, R^2: -1.8440139293670654\n",
      "Val: Loss: 142.95338439941406, Val MSE: 3.150332450866699, Val RMSE: 1.7749176025390625, Val R^2: -1.404083251953125\n",
      "Epoch 13,\n",
      "Train: Loss: 565.0313110351562, MSE: 3.6214468479156494, RMSE: 1.9030098915100098, R^2: -1.7090859413146973\n",
      "Val: Loss: 142.82623291015625, Val MSE: 3.0068018436431885, Val RMSE: 1.7340131998062134, Val R^2: -1.2945520877838135\n",
      "Epoch 14,\n",
      "Train: Loss: 564.9865112304688, MSE: 3.4554569721221924, RMSE: 1.8588860034942627, R^2: -1.584914207458496\n",
      "Val: Loss: 142.45188903808594, Val MSE: 2.869791269302368, Val RMSE: 1.6940457820892334, Val R^2: -1.1899964809417725\n",
      "Epoch 15,\n",
      "Train: Loss: 563.6173706054688, MSE: 3.3038828372955322, RMSE: 1.8176586627960205, R^2: -1.4715266227722168\n",
      "Val: Loss: 142.66488647460938, Val MSE: 2.748858690261841, Val RMSE: 1.657968282699585, Val R^2: -1.097710371017456\n",
      "Epoch 16,\n",
      "Train: Loss: 564.5140380859375, MSE: 3.1713192462921143, RMSE: 1.7808197736740112, R^2: -1.3723602294921875\n",
      "Val: Loss: 142.14862060546875, Val MSE: 2.6366708278656006, Val RMSE: 1.6237828731536865, Val R^2: -1.0120973587036133\n",
      "Epoch 17,\n",
      "Train: Loss: 563.6094360351562, MSE: 3.036745548248291, RMSE: 1.7426260709762573, R^2: -1.2716898918151855\n",
      "Val: Loss: 141.94906616210938, Val MSE: 2.5271782875061035, Val RMSE: 1.5897101163864136, Val R^2: -0.9285415410995483\n",
      "Epoch 18,\n",
      "Train: Loss: 563.811279296875, MSE: 2.9497745037078857, RMSE: 1.717490792274475, R^2: -1.206629753112793\n",
      "Val: Loss: 142.2325897216797, Val MSE: 2.442532777786255, Val RMSE: 1.5628604888916016, Val R^2: -0.863946795463562\n",
      "Epoch 19,\n",
      "Train: Loss: 562.943115234375, MSE: 2.825901508331299, RMSE: 1.6810418367385864, R^2: -1.113964319229126\n",
      "Val: Loss: 142.07980346679688, Val MSE: 2.336061477661133, Val RMSE: 1.528417944908142, Val R^2: -0.7826963663101196\n",
      "Epoch 20,\n",
      "Train: Loss: 563.4361572265625, MSE: 2.7436892986297607, RMSE: 1.6564085483551025, R^2: -1.052464246749878\n",
      "Val: Loss: 142.0079345703125, Val MSE: 2.2442431449890137, Val RMSE: 1.498079776763916, Val R^2: -0.7126278877258301\n",
      "Epoch 21,\n",
      "Train: Loss: 563.1492919921875, MSE: 2.624619245529175, RMSE: 1.6200677156448364, R^2: -0.963391900062561\n",
      "Val: Loss: 141.56520080566406, Val MSE: 2.1825854778289795, Val RMSE: 1.4773576259613037, Val R^2: -0.6655757427215576\n",
      "Epoch 22,\n",
      "Train: Loss: 562.9718627929688, MSE: 2.5318586826324463, RMSE: 1.5911815166473389, R^2: -0.8940006494522095\n",
      "Val: Loss: 141.80612182617188, Val MSE: 2.115384340286255, Val RMSE: 1.4544360637664795, Val R^2: -0.614293098449707\n",
      "Epoch 23,\n",
      "Train: Loss: 562.9638061523438, MSE: 2.4659361839294434, RMSE: 1.5703299045562744, R^2: -0.8446862697601318\n",
      "Val: Loss: 141.41799926757812, Val MSE: 2.021812915802002, Val RMSE: 1.4219046831130981, Val R^2: -0.5428868532180786\n",
      "Epoch 24,\n",
      "Train: Loss: 562.294677734375, MSE: 2.4103214740753174, RMSE: 1.552520990371704, R^2: -0.8030825853347778\n",
      "Val: Loss: 141.29066467285156, Val MSE: 1.955844759941101, Val RMSE: 1.398515224456787, Val R^2: -0.49254512786865234\n",
      "Epoch 25,\n",
      "Train: Loss: 562.8975830078125, MSE: 2.566096067428589, RMSE: 1.6019039154052734, R^2: -0.9196125268936157\n",
      "Val: Loss: 141.35374450683594, Val MSE: 1.8909341096878052, Val RMSE: 1.3751124143600464, Val R^2: -0.4430105686187744\n",
      "Epoch 26,\n",
      "Train: Loss: 562.5438232421875, MSE: 2.3625850677490234, RMSE: 1.5370702743530273, R^2: -0.7673726081848145\n",
      "Val: Loss: 141.46258544921875, Val MSE: 1.8607219457626343, Val RMSE: 1.3640828132629395, Val R^2: -0.4199550151824951\n",
      "Epoch 27,\n",
      "Train: Loss: 562.3627319335938, MSE: 2.297414541244507, RMSE: 1.515722393989563, R^2: -0.7186206579208374\n",
      "Val: Loss: 141.5220947265625, Val MSE: 1.7832133769989014, Val RMSE: 1.3353701829910278, Val R^2: -0.3608067035675049\n",
      "Epoch 28,\n",
      "Train: Loss: 562.0418090820312, MSE: 2.198033094406128, RMSE: 1.4825764894485474, R^2: -0.6442766189575195\n",
      "Val: Loss: 141.09405517578125, Val MSE: 1.7673755884170532, Val RMSE: 1.3294267654418945, Val R^2: -0.3487205505371094\n",
      "Epoch 29,\n",
      "Train: Loss: 562.0242919921875, MSE: 2.2456979751586914, RMSE: 1.4985653162002563, R^2: -0.6799331903457642\n",
      "Val: Loss: 141.25941467285156, Val MSE: 1.7399163246154785, Val RMSE: 1.319058895111084, Val R^2: -0.3277658224105835\n",
      "Epoch 30,\n",
      "Train: Loss: 561.99169921875, MSE: 2.111798048019409, RMSE: 1.453202724456787, R^2: -0.5797669887542725\n",
      "Val: Loss: 141.34182739257812, Val MSE: 1.7078195810317993, Val RMSE: 1.3068357706069946, Val R^2: -0.3032721281051636\n",
      "Epoch 31,\n",
      "Train: Loss: 562.169677734375, MSE: 2.0891757011413574, RMSE: 1.4453980922698975, R^2: -0.562843918800354\n",
      "Val: Loss: 140.99856567382812, Val MSE: 1.653684139251709, Val RMSE: 1.2859565019607544, Val R^2: -0.2619602680206299\n",
      "Epoch 32,\n",
      "Train: Loss: 561.6349487304688, MSE: 2.086031198501587, RMSE: 1.444309949874878, R^2: -0.560491681098938\n",
      "Val: Loss: 141.28610229492188, Val MSE: 1.6329680681228638, Val RMSE: 1.277876377105713, Val R^2: -0.24615132808685303\n",
      "Epoch 33,\n",
      "Train: Loss: 561.333984375, MSE: 2.056802272796631, RMSE: 1.4341555833816528, R^2: -0.5386264324188232\n",
      "Val: Loss: 141.24842834472656, Val MSE: 1.5909868478775024, Val RMSE: 1.261343240737915, Val R^2: -0.21411466598510742\n",
      "Epoch 34,\n",
      "Train: Loss: 561.4844970703125, MSE: 2.111515522003174, RMSE: 1.4531054496765137, R^2: -0.5795556306838989\n",
      "Val: Loss: 141.139892578125, Val MSE: 1.5642269849777222, Val RMSE: 1.2506905794143677, Val R^2: -0.1936936378479004\n",
      "Epoch 35,\n",
      "Train: Loss: 561.508056640625, MSE: 2.212883472442627, RMSE: 1.4875763654708862, R^2: -0.6553857326507568\n",
      "Val: Loss: 141.05410766601562, Val MSE: 1.558633804321289, Val RMSE: 1.2484525442123413, Val R^2: -0.18942546844482422\n",
      "Epoch 36,\n",
      "Train: Loss: 561.5979614257812, MSE: 1.9660179615020752, RMSE: 1.402147650718689, R^2: -0.47071373462677\n",
      "Val: Loss: 141.32693481445312, Val MSE: 1.540759563446045, Val RMSE: 1.2412734031677246, Val R^2: -0.17578518390655518\n",
      "Epoch 37,\n",
      "Train: Loss: 561.3172607421875, MSE: 1.993992567062378, RMSE: 1.4120880365371704, R^2: -0.491640567779541\n",
      "Val: Loss: 140.8560333251953, Val MSE: 1.5248788595199585, Val RMSE: 1.234859824180603, Val R^2: -0.1636662483215332\n",
      "Epoch 38,\n",
      "Train: Loss: 561.0276489257812, MSE: 1.9056681394577026, RMSE: 1.3804594278335571, R^2: -0.42556798458099365\n",
      "Val: Loss: 141.30990600585938, Val MSE: 1.4927204847335815, Val RMSE: 1.2217694520950317, Val R^2: -0.13912558555603027\n",
      "Epoch 39,\n",
      "Train: Loss: 561.3435668945312, MSE: 1.914694905281067, RMSE: 1.3837250471115112, R^2: -0.43232059478759766\n",
      "Val: Loss: 141.40817260742188, Val MSE: 1.4889568090438843, Val RMSE: 1.2202281951904297, Val R^2: -0.13625335693359375\n",
      "Epoch 40,\n",
      "Train: Loss: 561.1641235351562, MSE: 1.9647293090820312, RMSE: 1.4016879796981812, R^2: -0.46974968910217285\n",
      "Val: Loss: 140.86090087890625, Val MSE: 1.4783645868301392, Val RMSE: 1.2158801555633545, Val R^2: -0.12817025184631348\n",
      "Epoch 41,\n",
      "Train: Loss: 561.5397338867188, MSE: 1.9974502325057983, RMSE: 1.4133118391036987, R^2: -0.49422717094421387\n",
      "Val: Loss: 140.91893005371094, Val MSE: 1.4770270586013794, Val RMSE: 1.2153300046920776, Val R^2: -0.1271495819091797\n",
      "Epoch 42,\n",
      "Train: Loss: 561.3881225585938, MSE: 1.9187941551208496, RMSE: 1.3852055072784424, R^2: -0.43538713455200195\n",
      "Val: Loss: 140.8759307861328, Val MSE: 1.451656699180603, Val RMSE: 1.2048472166061401, Val R^2: -0.10778892040252686\n",
      "Epoch 43,\n",
      "Train: Loss: 561.35888671875, MSE: 1.9323502779006958, RMSE: 1.390089988708496, R^2: -0.4455280303955078\n",
      "Val: Loss: 140.81349182128906, Val MSE: 1.462890863418579, Val RMSE: 1.2095003128051758, Val R^2: -0.11636197566986084\n",
      "Epoch 44,\n",
      "Train: Loss: 561.181884765625, MSE: 1.9465434551239014, RMSE: 1.3951858282089233, R^2: -0.4561455249786377\n",
      "Val: Loss: 140.98666381835938, Val MSE: 1.4319899082183838, Val RMSE: 1.1966577768325806, Val R^2: -0.09278082847595215\n",
      "Epoch 45,\n",
      "Train: Loss: 560.9437866210938, MSE: 1.8840214014053345, RMSE: 1.3725966215133667, R^2: -0.4093747138977051\n",
      "Val: Loss: 140.96241760253906, Val MSE: 1.4394909143447876, Val RMSE: 1.1997878551483154, Val R^2: -0.09850490093231201\n",
      "Epoch 46,\n",
      "Train: Loss: 561.1646118164062, MSE: 1.8689762353897095, RMSE: 1.3671050071716309, R^2: -0.3981199264526367\n",
      "Val: Loss: 140.99578857421875, Val MSE: 1.450787901878357, Val RMSE: 1.204486608505249, Val R^2: -0.10712599754333496\n",
      "Epoch 47,\n",
      "Train: Loss: 560.4835815429688, MSE: 1.845078706741333, RMSE: 1.3583366870880127, R^2: -0.38024306297302246\n",
      "Val: Loss: 140.78924560546875, Val MSE: 1.4193395376205444, Val RMSE: 1.191360354423523, Val R^2: -0.08312714099884033\n",
      "Epoch 48,\n",
      "Train: Loss: 560.9644775390625, MSE: 1.8821032047271729, RMSE: 1.3718976974487305, R^2: -0.4079399108886719\n",
      "Val: Loss: 140.9541778564453, Val MSE: 1.4221382141113281, Val RMSE: 1.192534327507019, Val R^2: -0.08526277542114258\n",
      "Epoch 49,\n",
      "Train: Loss: 560.6363525390625, MSE: 1.8473269939422607, RMSE: 1.3591641187667847, R^2: -0.3819248676300049\n",
      "Val: Loss: 140.91941833496094, Val MSE: 1.426204800605774, Val RMSE: 1.1942381858825684, Val R^2: -0.08836615085601807\n",
      "Epoch 50,\n",
      "Train: Loss: 560.7802124023438, MSE: 1.8295689821243286, RMSE: 1.3526155948638916, R^2: -0.3686407804489136\n",
      "Val: Loss: 141.02064514160156, Val MSE: 1.4295966625213623, Val RMSE: 1.1956573724746704, Val R^2: -0.09095454216003418\n",
      "Epoch 51,\n",
      "Train: Loss: 560.493408203125, MSE: 1.8501543998718262, RMSE: 1.3602038621902466, R^2: -0.3840399980545044\n",
      "Val: Loss: 140.8603057861328, Val MSE: 1.4250255823135376, Val RMSE: 1.1937443017959595, Val R^2: -0.08746612071990967\n",
      "Epoch 52,\n",
      "Train: Loss: 560.1498413085938, MSE: 1.8224848508834839, RMSE: 1.349994421005249, R^2: -0.36334121227264404\n",
      "Val: Loss: 141.04461669921875, Val MSE: 1.4210991859436035, Val RMSE: 1.192098617553711, Val R^2: -0.08446991443634033\n",
      "Epoch 53,\n",
      "Train: Loss: 560.5480346679688, MSE: 1.8244683742523193, RMSE: 1.3507288694381714, R^2: -0.3648250102996826\n",
      "Val: Loss: 141.0434112548828, Val MSE: 1.418446660041809, Val RMSE: 1.1909855604171753, Val R^2: -0.08244562149047852\n",
      "Epoch 54,\n",
      "Train: Loss: 560.564697265625, MSE: 1.8168494701385498, RMSE: 1.3479056358337402, R^2: -0.35912561416625977\n",
      "Val: Loss: 140.88671875, Val MSE: 1.4181900024414062, Val RMSE: 1.1908777952194214, Val R^2: -0.08224976062774658\n",
      "Epoch 55,\n",
      "Train: Loss: 560.18505859375, MSE: 1.840492844581604, RMSE: 1.3566476106643677, R^2: -0.37681257724761963\n",
      "Val: Loss: 141.13729858398438, Val MSE: 1.4097148180007935, Val RMSE: 1.1873141527175903, Val R^2: -0.07578229904174805\n",
      "Epoch 56,\n",
      "Train: Loss: 560.3284301757812, MSE: 1.833580732345581, RMSE: 1.3540977239608765, R^2: -0.37164175510406494\n",
      "Val: Loss: 141.06649780273438, Val MSE: 1.3956377506256104, Val RMSE: 1.1813710927963257, Val R^2: -0.0650397539138794\n",
      "Epoch 57,\n",
      "Train: Loss: 560.3262939453125, MSE: 1.8003697395324707, RMSE: 1.3417785167694092, R^2: -0.3467977046966553\n",
      "Val: Loss: 141.16329956054688, Val MSE: 1.3972865343093872, Val RMSE: 1.1820687055587769, Val R^2: -0.06629800796508789\n",
      "Epoch 58,\n",
      "Train: Loss: 560.5001831054688, MSE: 1.7779873609542847, RMSE: 1.3334119319915771, R^2: -0.3300541639328003\n",
      "Val: Loss: 140.79356384277344, Val MSE: 1.399593472480774, Val RMSE: 1.183044195175171, Val R^2: -0.06805849075317383\n",
      "Epoch 59,\n",
      "Train: Loss: 560.3934936523438, MSE: 1.7907785177230835, RMSE: 1.3381997346878052, R^2: -0.33962273597717285\n",
      "Val: Loss: 140.84825134277344, Val MSE: 1.3961799144744873, Val RMSE: 1.181600570678711, Val R^2: -0.06545352935791016\n",
      "Epoch 60,\n",
      "Train: Loss: 560.619384765625, MSE: 1.79574453830719, RMSE: 1.340053915977478, R^2: -0.3433377742767334\n",
      "Val: Loss: 141.0629425048828, Val MSE: 1.3882651329040527, Val RMSE: 1.1782466173171997, Val R^2: -0.05941355228424072\n",
      "Epoch 61,\n",
      "Train: Loss: 561.0846557617188, MSE: 1.814980387687683, RMSE: 1.3472120761871338, R^2: -0.3577275276184082\n",
      "Val: Loss: 140.97621154785156, Val MSE: 1.3846521377563477, Val RMSE: 1.1767123937606812, Val R^2: -0.056656479835510254\n",
      "Epoch 62,\n",
      "Train: Loss: 560.7091674804688, MSE: 1.7847366333007812, RMSE: 1.3359403610229492, R^2: -0.33510303497314453\n",
      "Val: Loss: 140.99745178222656, Val MSE: 1.382290244102478, Val RMSE: 1.1757084131240845, Val R^2: -0.05485403537750244\n",
      "Epoch 63,\n",
      "Train: Loss: 560.189453125, MSE: 1.7729887962341309, RMSE: 1.3315362930297852, R^2: -0.32631492614746094\n",
      "Val: Loss: 140.99508666992188, Val MSE: 1.3796169757843018, Val RMSE: 1.174570918083191, Val R^2: -0.05281400680541992\n",
      "Epoch 64,\n",
      "Train: Loss: 560.2056884765625, MSE: 1.7621631622314453, RMSE: 1.3274649381637573, R^2: -0.31821656227111816\n",
      "Val: Loss: 141.0654754638672, Val MSE: 1.378616213798523, Val RMSE: 1.1741448640823364, Val R^2: -0.052050232887268066\n",
      "Epoch 65,\n",
      "Train: Loss: 560.0738525390625, MSE: 1.820365309715271, RMSE: 1.349209189414978, R^2: -0.36175572872161865\n",
      "Val: Loss: 140.83248901367188, Val MSE: 1.3728548288345337, Val RMSE: 1.1716889142990112, Val R^2: -0.04765355587005615\n",
      "Epoch 66,\n",
      "Train: Loss: 559.6917724609375, MSE: 1.764115810394287, RMSE: 1.3282002210617065, R^2: -0.31967735290527344\n",
      "Val: Loss: 140.69741821289062, Val MSE: 1.3730242252349854, Val RMSE: 1.171761155128479, Val R^2: -0.04778289794921875\n",
      "Epoch 67,\n",
      "Train: Loss: 560.457763671875, MSE: 1.7474017143249512, RMSE: 1.3218932151794434, R^2: -0.30717408657073975\n",
      "Val: Loss: 140.6978759765625, Val MSE: 1.3764111995697021, Val RMSE: 1.1732054948806763, Val R^2: -0.05036747455596924\n",
      "Epoch 68,\n",
      "Train: Loss: 560.4976196289062, MSE: 1.7545853853225708, RMSE: 1.3246076107025146, R^2: -0.3125479221343994\n",
      "Val: Loss: 141.16343688964844, Val MSE: 1.3681634664535522, Val RMSE: 1.1696852445602417, Val R^2: -0.04407358169555664\n",
      "Epoch 69,\n",
      "Train: Loss: 559.9990234375, MSE: 1.7687407732009888, RMSE: 1.3299400806427002, R^2: -0.3231370449066162\n",
      "Val: Loss: 140.8451690673828, Val MSE: 1.3715428113937378, Val RMSE: 1.1711288690567017, Val R^2: -0.04665231704711914\n",
      "Epoch 70,\n",
      "Train: Loss: 560.4342651367188, MSE: 1.7572088241577148, RMSE: 1.3255975246429443, R^2: -0.3145103454589844\n",
      "Val: Loss: 140.7807159423828, Val MSE: 1.3598299026489258, Val RMSE: 1.1661174297332764, Val R^2: -0.03771412372589111\n",
      "Epoch 71,\n",
      "Train: Loss: 560.0059204101562, MSE: 1.764023780822754, RMSE: 1.3281655311584473, R^2: -0.3196084499359131\n",
      "Val: Loss: 141.1311492919922, Val MSE: 1.3605366945266724, Val RMSE: 1.1664204597473145, Val R^2: -0.03825342655181885\n",
      "Epoch 72,\n",
      "Train: Loss: 560.0098876953125, MSE: 1.7751168012619019, RMSE: 1.332335114479065, R^2: -0.32790684700012207\n",
      "Val: Loss: 140.79835510253906, Val MSE: 1.361250400543213, Val RMSE: 1.1667263507843018, Val R^2: -0.03879809379577637\n",
      "Epoch 73,\n",
      "Train: Loss: 560.4175415039062, MSE: 1.755984902381897, RMSE: 1.3251358270645142, R^2: -0.3135948181152344\n",
      "Val: Loss: 140.84109497070312, Val MSE: 1.3583265542984009, Val RMSE: 1.1654726266860962, Val R^2: -0.036566734313964844\n",
      "Epoch 74,\n",
      "Train: Loss: 559.9072265625, MSE: 1.753655195236206, RMSE: 1.324256420135498, R^2: -0.3118520975112915\n",
      "Val: Loss: 141.0151824951172, Val MSE: 1.3633095026016235, Val RMSE: 1.1676084995269775, Val R^2: -0.040369391441345215\n",
      "Epoch 75,\n",
      "Train: Loss: 559.9429321289062, MSE: 1.76812744140625, RMSE: 1.329709529876709, R^2: -0.3226783275604248\n",
      "Val: Loss: 140.97573852539062, Val MSE: 1.3544552326202393, Val RMSE: 1.1638106107711792, Val R^2: -0.033612608909606934\n",
      "Epoch 76,\n",
      "Train: Loss: 560.1788940429688, MSE: 1.7458301782608032, RMSE: 1.3212987184524536, R^2: -0.30599844455718994\n",
      "Val: Loss: 140.9014434814453, Val MSE: 1.3537883758544922, Val RMSE: 1.1635241508483887, Val R^2: -0.03310370445251465\n",
      "Epoch 77,\n",
      "Train: Loss: 559.7987060546875, MSE: 1.762233853340149, RMSE: 1.3274915218353271, R^2: -0.3182694911956787\n",
      "Val: Loss: 140.8101806640625, Val MSE: 1.3547090291976929, Val RMSE: 1.1639196872711182, Val R^2: -0.0338062047958374\n",
      "Epoch 78,\n",
      "Train: Loss: 560.0517578125, MSE: 1.7236597537994385, RMSE: 1.3128821849822998, R^2: -0.2894134521484375\n",
      "Val: Loss: 140.94471740722656, Val MSE: 1.3461190462112427, Val RMSE: 1.1602237224578857, Val R^2: -0.027251005172729492\n",
      "Epoch 79,\n",
      "Train: Loss: 560.0254516601562, MSE: 1.709100365638733, RMSE: 1.3073256015777588, R^2: -0.2785221338272095\n",
      "Val: Loss: 140.91400146484375, Val MSE: 1.350995421409607, Val RMSE: 1.162323236465454, Val R^2: -0.03097224235534668\n",
      "Epoch 80,\n",
      "Train: Loss: 560.4168090820312, MSE: 1.7506296634674072, RMSE: 1.3231136798858643, R^2: -0.30958878993988037\n",
      "Val: Loss: 140.9337615966797, Val MSE: 1.3524473905563354, Val RMSE: 1.1629477739334106, Val R^2: -0.03208029270172119\n",
      "Epoch 81,\n",
      "Train: Loss: 560.2880859375, MSE: 1.729425072669983, RMSE: 1.3150761127471924, R^2: -0.2937263250350952\n",
      "Val: Loss: 140.8233642578125, Val MSE: 1.3501287698745728, Val RMSE: 1.1619504690170288, Val R^2: -0.030310869216918945\n",
      "Epoch 82,\n",
      "Train: Loss: 559.7755737304688, MSE: 1.7588917016983032, RMSE: 1.3262321949005127, R^2: -0.3157693147659302\n",
      "Val: Loss: 141.01351928710938, Val MSE: 1.347861647605896, Val RMSE: 1.160974383354187, Val R^2: -0.028580784797668457\n",
      "Epoch 83,\n",
      "Train: Loss: 560.4966430664062, MSE: 1.7205325365066528, RMSE: 1.3116906881332397, R^2: -0.28707408905029297\n",
      "Val: Loss: 140.84197998046875, Val MSE: 1.3484971523284912, Val RMSE: 1.1612480878829956, Val R^2: -0.029065728187561035\n",
      "Epoch 84,\n",
      "Train: Loss: 560.0371704101562, MSE: 1.7063910961151123, RMSE: 1.3062890768051147, R^2: -0.2764953374862671\n",
      "Val: Loss: 140.940673828125, Val MSE: 1.3459203243255615, Val RMSE: 1.1601380109786987, Val R^2: -0.0270993709564209\n",
      "Epoch 85,\n",
      "Train: Loss: 559.7781372070312, MSE: 1.7209097146987915, RMSE: 1.311834454536438, R^2: -0.28735625743865967\n",
      "Val: Loss: 140.74871826171875, Val MSE: 1.349713683128357, Val RMSE: 1.1617717742919922, Val R^2: -0.02999413013458252\n",
      "Epoch 86,\n",
      "Train: Loss: 560.3070678710938, MSE: 1.793189287185669, RMSE: 1.3391001224517822, R^2: -0.3414262533187866\n",
      "Val: Loss: 140.6004180908203, Val MSE: 1.347888708114624, Val RMSE: 1.160986065864563, Val R^2: -0.028601408004760742\n",
      "Epoch 87,\n",
      "Train: Loss: 560.3168334960938, MSE: 1.7360539436340332, RMSE: 1.317594051361084, R^2: -0.2986851930618286\n",
      "Val: Loss: 140.96482849121094, Val MSE: 1.3556451797485352, Val RMSE: 1.164321780204773, Val R^2: -0.034520626068115234\n",
      "Epoch 88,\n",
      "Train: Loss: 560.1990356445312, MSE: 1.718062400817871, RMSE: 1.310748815536499, R^2: -0.2852262258529663\n",
      "Val: Loss: 140.9099578857422, Val MSE: 1.3504444360733032, Val RMSE: 1.1620862483978271, Val R^2: -0.030551791191101074\n",
      "Epoch 89,\n",
      "Train: Loss: 560.3228149414062, MSE: 1.7167301177978516, RMSE: 1.3102405071258545, R^2: -0.2842296361923218\n",
      "Val: Loss: 140.89845275878906, Val MSE: 1.3485770225524902, Val RMSE: 1.1612825393676758, Val R^2: -0.029126763343811035\n",
      "Epoch 90,\n",
      "Train: Loss: 559.9505615234375, MSE: 1.6964770555496216, RMSE: 1.3024888038635254, R^2: -0.26907896995544434\n",
      "Val: Loss: 140.89320373535156, Val MSE: 1.345576286315918, Val RMSE: 1.159989833831787, Val R^2: -0.026836872100830078\n",
      "Epoch 91,\n",
      "Train: Loss: 559.7921142578125, MSE: 1.7182579040527344, RMSE: 1.3108233213424683, R^2: -0.2853724956512451\n",
      "Val: Loss: 140.6912078857422, Val MSE: 1.3521218299865723, Val RMSE: 1.1628077030181885, Val R^2: -0.03183186054229736\n",
      "Epoch 92,\n",
      "Train: Loss: 559.8731079101562, MSE: 1.70790433883667, RMSE: 1.3068681955337524, R^2: -0.2776273488998413\n",
      "Val: Loss: 140.8797149658203, Val MSE: 1.3462910652160645, Val RMSE: 1.1602978706359863, Val R^2: -0.027382373809814453\n",
      "Epoch 93,\n",
      "Train: Loss: 560.62353515625, MSE: 1.7143480777740479, RMSE: 1.3093311786651611, R^2: -0.2824476957321167\n",
      "Val: Loss: 140.65750122070312, Val MSE: 1.3449028730392456, Val RMSE: 1.1596994400024414, Val R^2: -0.02632296085357666\n",
      "Epoch 94,\n",
      "Train: Loss: 559.7843017578125, MSE: 1.7319955825805664, RMSE: 1.316053032875061, R^2: -0.2956491708755493\n",
      "Val: Loss: 141.0537872314453, Val MSE: 1.344393253326416, Val RMSE: 1.1594797372817993, Val R^2: -0.02593398094177246\n",
      "Epoch 95,\n",
      "Train: Loss: 559.3226928710938, MSE: 1.7210094928741455, RMSE: 1.3118724822998047, R^2: -0.28743088245391846\n",
      "Val: Loss: 140.8302459716797, Val MSE: 1.3436304330825806, Val RMSE: 1.1591507196426392, Val R^2: -0.025351881980895996\n",
      "Epoch 96,\n",
      "Train: Loss: 560.1085815429688, MSE: 1.6901791095733643, RMSE: 1.3000688552856445, R^2: -0.2643676996231079\n",
      "Val: Loss: 140.99252319335938, Val MSE: 1.3424015045166016, Val RMSE: 1.1586204767227173, Val R^2: -0.0244140625\n",
      "Epoch 97,\n",
      "Train: Loss: 560.1111450195312, MSE: 1.700826644897461, RMSE: 1.3041574954986572, R^2: -0.2723327875137329\n",
      "Val: Loss: 140.85108947753906, Val MSE: 1.345775842666626, Val RMSE: 1.1600757837295532, Val R^2: -0.026989102363586426\n",
      "Epoch 98,\n",
      "Train: Loss: 559.6674194335938, MSE: 1.7067115306854248, RMSE: 1.3064117431640625, R^2: -0.2767350673675537\n",
      "Val: Loss: 141.03717041015625, Val MSE: 1.3415032625198364, Val RMSE: 1.1582328081130981, Val R^2: -0.023728609085083008\n",
      "Epoch 99,\n",
      "Train: Loss: 560.400146484375, MSE: 1.7234914302825928, RMSE: 1.312818169593811, R^2: -0.2892875671386719\n",
      "Val: Loss: 140.9868621826172, Val MSE: 1.3420125246047974, Val RMSE: 1.1584526300430298, Val R^2: -0.024117231369018555\n",
      "Epoch 100,\n",
      "Train: Loss: 560.574462890625, MSE: 1.712705373764038, RMSE: 1.3087036609649658, R^2: -0.28121888637542725\n",
      "Val: Loss: 140.75927734375, Val MSE: 1.3449935913085938, Val RMSE: 1.159738540649414, Val R^2: -0.026392102241516113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared score.\"\"\"\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "\n",
    "def train(model, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, stochastic_gates_pred = model(X_train)\n",
    "        loss = custom_loss_function(y_pred, y_train, stochastic_gates_pred, sigma=1, D=X_train.shape[1], lambda_reg=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse = mse_metric(y_pred, y_train).item()\n",
    "        train_rmse = rmse_metric(y_pred, y_train).item()\n",
    "        train_r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_pred, val_stochastic_gates = model(X_val)\n",
    "            val_loss = custom_loss_function(val_y_pred, y_val, val_stochastic_gates, sigma=1, D=X_val.shape[1], lambda_reg=0.05)\n",
    "            val_mse = mse_metric(val_y_pred, y_val).item()\n",
    "            val_rmse = rmse_metric(val_y_pred, y_val).item()\n",
    "            val_r2 = r2_score(y_val, val_y_pred)\n",
    "\n",
    "        print(f'Epoch {epoch+1},\\n'\n",
    "              f'Train: Loss: {loss.item()}, MSE: {train_mse}, RMSE: {train_rmse}, R^2: {train_r2}\\n'\n",
    "              f'Val: Loss: {val_loss.item()}, Val MSE: {val_mse}, Val RMSE: {val_rmse}, Val R^2: {val_r2}')\n",
    "\n",
    "    return stochastic_gates_pred\n",
    "\n",
    "# Note: Ensure custom_loss_function, mse_metric, rmse_metric, and C_StochasticGates are defined\n",
    "# Also, ensure X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor are properly initialized\n",
    "\n",
    "# Example initialization (for illustration purposes)\n",
    "model = C_StochasticGates(input_dim=X_train_tensor.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 100  # Adjust as necessary\n",
    "gates = train(model, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define a function to train and evaluate models using STG algorithm\n",
    "# def train_evaluate_stg(X_train, y_train, X_test, y_test, context_dim):\n",
    "#     # Instantiate HyperNetwork and PredictionNetwork for STG\n",
    "#     hypernetwork_stg = HyperNetwork(context_dim, X_train.shape[1] - context_dim)\n",
    "#     prediction_network_stg = PredictionNetwork(X_train.shape[1] - context_dim)\n",
    "\n",
    "#     # Train the model\n",
    "#     stg_losses_train, stg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "#                                                     hypernetwork_stg, prediction_network_stg)\n",
    "#     return stg_losses_train, stg_losses_val\n",
    "\n",
    "# # Define a function to train and evaluate models using CSTG algorithm\n",
    "# def train_evaluate_cstg(X_train, y_train, X_test, y_test, context_dim):\n",
    "#     # Instantiate ConditionalStochasticGates model\n",
    "#     cstg_model = ConditionalStochasticGates(context_dim, X_train.shape[1] - context_dim)\n",
    "\n",
    "#     # Train the model\n",
    "#     cstg_losses_train, cstg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "#                                                       cstg_model, prediction_network)\n",
    "#     return cstg_losses_train, cstg_losses_val\n",
    "\n",
    "# # Define a function to train the model and return training and validation losses\n",
    "# def train_model(X_train, y_train, X_test, y_test, hypernetwork, prediction_network):\n",
    "#     optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "#     loss_fn = nn.MSELoss()\n",
    "\n",
    "#     num_epochs = 100\n",
    "#     stg_losses_train, stg_losses_val = [], []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training step\n",
    "#         hypernetwork.train()\n",
    "#         prediction_network.train()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         gates_train = hypernetwork(X_train[:, :context_dim])\n",
    "#         selected_features_train = X_train[:, context_dim:] * stochastic_gates(gates_train)\n",
    "#         predictions_train = prediction_network(selected_features_train)\n",
    "#         loss_train = loss_fn(predictions_train, y_train)\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss_train.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             hypernetwork.eval()\n",
    "#             prediction_network.eval()\n",
    "\n",
    "#             gates_val = hypernetwork(X_test[:, :context_dim])\n",
    "#             selected_features_val = X_test[:, context_dim:] * stochastic_gates(gates_val)\n",
    "#             predictions_val = prediction_network(selected_features_val)\n",
    "#             loss_val = loss_fn(predictions_val, y_test)\n",
    "\n",
    "#         stg_losses_train.append(loss_train.item())\n",
    "#         stg_losses_val.append(loss_val.item())\n",
    "\n",
    "#     return stg_losses_train, stg_losses_val\n",
    "\n",
    "# # Define the context dimension\n",
    "# context_dim = 3  # Assuming the first 3 features are used as context\n",
    "\n",
    "# # Train and evaluate models using STG algorithm\n",
    "# stg_losses_train, stg_losses_val = train_evaluate_stg(X_train_tensor, y_train_tensor, \n",
    "#                                                       X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# # Train and evaluate models using CSTG algorithm\n",
    "# cstg_losses_train, cstg_losses_val = train_evaluate_cstg(X_train_tensor, y_train_tensor, \n",
    "#                                                           X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# # Plot training and validation losses for STG and CSTG algorithms\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, num_epochs + 1), stg_losses_train, label='STG Train Loss', color='blue')\n",
    "# plt.plot(range(1, num_epochs + 1), stg_losses_val, label='STG Validation Loss', linestyle='--', color='blue')\n",
    "# plt.plot(range(1, num_epochs + 1), cstg_losses_train, label='CSTG Train Loss', color='orange')\n",
    "# plt.plot(range(1, num_epochs + 1), cstg_losses_val, label='CSTG Validation Loss', linestyle='--', color='orange')\n",
    "# plt.title('Training and Validation Losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Assuming X_train_tensor and X_test_tensor are already defined and properly shaped\n",
    "# input_dim = X_train_tensor.shape[1]  # Number of features in the input data\n",
    "# feature_dim = input_dim  # The hypernetwork output and prediction input dimensions must match\n",
    "\n",
    "# # Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "# hypernet = HyperNet(input_dim)\n",
    "# model = PredNet(feature_dim)\n",
    "# optimizer = Adam(list(hypernet.parameters()) + list(model.parameters()) , lr=0.01)\n",
    "\n",
    "# # Training\n",
    "# epochs = 2  # Adjust as necessary based on convergence and performance\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "AveOccup: 0.44150832295417786\n",
      "MedInc: 0.5098904967308044\n",
      "Uniform_Noise: 0.474697470664978\n",
      "Cosine_Noise: 0.4606289863586426\n",
      "AveRooms: 0.47779104113578796\n",
      "HouseAge: 0.461354523897171\n",
      "Gaussian_Noise: 0.5596177577972412\n",
      "Population: 0.407277375459671\n",
      "Longitude: 0.5244668126106262\n",
      "AveBedrms: 0.4461166560649872\n",
      "Latitude: 0.5383712649345398\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with the trained models\n",
    "def make_inference(model, hypernet, X_test):\n",
    "    model.eval()\n",
    "    hypernet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the feature selection probabilities from the hypernetwork\n",
    "        selection_prob = hypernet(X_test)\n",
    "        # For inference use the expected values (probabilities) instead of sampling\n",
    "        selected_features = X_test * selection_prob\n",
    "        # Get the model predictions\n",
    "        predictions = model(selected_features)\n",
    "    return predictions, selection_prob\n",
    "\n",
    "# Load the test data (replace this with your actual test data)\n",
    "X_test = X_test_tensor  # Assuming X_test_tensor is your test data\n",
    "\n",
    "# Make inference\n",
    "predictions, feature_importance_probabilities = make_inference(model, hypernet, X_test)\n",
    "\n",
    "\n",
    "# Print the conditional probability of each feature being significant\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    print(f\"{feature_name}: {feature_importance_probabilities[:, i].mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities of feature significance given other features:\n",
      "Gaussian_Noise: 0.5596177577972412\n",
      "Latitude: 0.5383712649345398\n",
      "Longitude: 0.5244668126106262\n",
      "MedInc: 0.5098904967308044\n",
      "AveRooms: 0.47779104113578796\n",
      "Uniform_Noise: 0.474697470664978\n",
      "HouseAge: 0.461354523897171\n",
      "Cosine_Noise: 0.4606289863586426\n",
      "AveBedrms: 0.4461166560649872\n",
      "AveOccup: 0.44150832295417786\n",
      "Population: 0.407277375459671\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples (feature_name, probability)\n",
    "feature_probabilities = [(feature_name, feature_importance_probabilities[:, i].mean().item()) for i, feature_name in enumerate(feature_names)]\n",
    "\n",
    "# Sort the list by probability in descending order\n",
    "sorted_feature_probabilities = sorted(feature_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted list\n",
    "print(\"Conditional probabilities of feature significance given other features:\")\n",
    "for feature, probability in sorted_feature_probabilities:\n",
    "    print(f\"{feature}: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m mean_feature_importance_probabilities \u001b[38;5;241m=\u001b[39m feature_importance_probabilities\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Determine which features have a mean selection probability above the threshold\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m important_features_indices \u001b[38;5;241m=\u001b[39m mean_feature_importance_probabilities \u001b[38;5;241m>\u001b[39m \u001b[43mthreshold\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean feature selection probabilities from the hypernetwork (higher means more important):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_feature_importance_probabilities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate the mean feature importance probability across all examples\n",
    "mean_feature_importance_probabilities = feature_importance_probabilities.mean(dim=0)\n",
    "\n",
    "# Determine which features have a mean selection probability above the threshold\n",
    "important_features_indices = mean_feature_importance_probabilities > threshold\n",
    "\n",
    "print(\"Mean feature selection probabilities from the hypernetwork (higher means more important):\")\n",
    "print(mean_feature_importance_probabilities)\n",
    "\n",
    "print(\"\\nFeatures with mean selection probability higher than threshold:\")\n",
    "print(important_features_indices)\n",
    "# Extract the names of the important features\n",
    "important_feature_names = [name for i, name in enumerate(feature_names) if important_features_indices[i]]\n",
    "\n",
    "print(\"\\nImportant feature names:\")\n",
    "print(important_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters of HyperNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1502, -0.1104, -0.0366,  ...,  0.1718,  0.0637, -0.0697],\n",
      "        [-0.1804,  0.0159, -0.1112,  ...,  0.1196, -0.0227, -0.0770],\n",
      "        [ 0.1346,  0.1256,  0.1248,  ..., -0.0737, -0.1655, -0.1717],\n",
      "        ...,\n",
      "        [-0.0565,  0.0891,  0.0982,  ..., -0.1358, -0.1359,  0.0080],\n",
      "        [-0.0074, -0.0703,  0.0896,  ...,  0.2032, -0.2027, -0.1844],\n",
      "        [-0.0535,  0.0381,  0.0786,  ..., -0.1920, -0.0853, -0.0579]])\n",
      "fc1.bias:\n",
      "tensor([ 0.1512,  0.2043,  0.1095, -0.1715, -0.0402, -0.1496, -0.2718,  0.2931,\n",
      "         0.1333,  0.1599,  0.0045, -0.1857, -0.2328,  0.0870,  0.1531, -0.2914,\n",
      "         0.0040,  0.2220, -0.0121, -0.1919,  0.0696, -0.0186,  0.0074,  0.0958,\n",
      "        -0.0544,  0.2858, -0.1110, -0.0218,  0.0621, -0.1147, -0.2755,  0.1071,\n",
      "         0.2763,  0.1478, -0.2083,  0.2526,  0.0627, -0.0947, -0.2117, -0.2135,\n",
      "         0.1645, -0.2223,  0.1970, -0.1847, -0.0560, -0.2816, -0.2027, -0.0012,\n",
      "         0.2529,  0.2172, -0.0118, -0.2407,  0.2348, -0.2373, -0.1128,  0.0839,\n",
      "        -0.1592,  0.2953,  0.1130,  0.0883,  0.2779, -0.0367,  0.1327, -0.0183,\n",
      "        -0.0516, -0.2265, -0.0095, -0.2309,  0.1992, -0.2779, -0.0978, -0.0260,\n",
      "        -0.2853,  0.1937,  0.2322,  0.2955, -0.0263, -0.2422,  0.2143, -0.1737,\n",
      "        -0.0019,  0.0418, -0.2932,  0.1785,  0.2519,  0.1678,  0.2759, -0.2777,\n",
      "        -0.1369, -0.0138,  0.2617, -0.2685, -0.0044,  0.1171, -0.0095,  0.1542,\n",
      "        -0.1603, -0.1790,  0.1629,  0.0338, -0.1578, -0.2427,  0.0014, -0.1207,\n",
      "        -0.1155, -0.0885, -0.1136, -0.1674, -0.1247, -0.2480, -0.1872, -0.1625,\n",
      "        -0.1832,  0.2756, -0.2429,  0.0740, -0.1579,  0.0388, -0.2921, -0.1844,\n",
      "        -0.0605, -0.0986, -0.1727, -0.1732, -0.0819,  0.0992,  0.0258,  0.1645])\n",
      "fc2.weight:\n",
      "tensor([[-0.0785, -0.0891,  0.1382,  ..., -0.0968, -0.0798,  0.0083],\n",
      "        [ 0.1183, -0.1070, -0.1937,  ..., -0.0883, -0.1277, -0.0018],\n",
      "        [-0.1392, -0.2076, -0.0042,  ...,  0.0864, -0.1327, -0.0715],\n",
      "        ...,\n",
      "        [ 0.1834,  0.0056,  0.1638,  ...,  0.0071, -0.0697,  0.1988],\n",
      "        [-0.1742,  0.1368,  0.1087,  ..., -0.1041,  0.0640, -0.1027],\n",
      "        [-0.0524, -0.1429,  0.0062,  ..., -0.1828, -0.0921,  0.1280]])\n",
      "fc2.bias:\n",
      "tensor([-0.0545, -0.0791,  0.0383,  0.0282,  0.0775,  0.0259, -0.0079, -0.0723,\n",
      "         0.0538, -0.0139, -0.0712])\n",
      "\n",
      "Final parameters of PredNet:\n",
      "fc1.weight:\n",
      "tensor([[ 0.1951,  0.1289,  0.0336,  ...,  0.0818,  0.1140, -0.0908],\n",
      "        [-0.1356,  0.0355,  0.2034,  ...,  0.0818,  0.1826, -0.0745],\n",
      "        [ 0.1945,  0.0366,  0.1083,  ...,  0.1903,  0.1251,  0.1758],\n",
      "        ...,\n",
      "        [-0.0572, -0.0295, -0.1143,  ...,  0.0377,  0.0034,  0.0813],\n",
      "        [ 0.0804,  0.0441,  0.0599,  ...,  0.0587, -0.0560, -0.1215],\n",
      "        [-0.1221, -0.2030,  0.0446,  ..., -0.1810, -0.0804, -0.0874]])\n",
      "fc1.bias:\n",
      "tensor([ 0.0847,  0.2508,  0.0543, -0.1693, -0.2101,  0.0284, -0.1087, -0.0365,\n",
      "        -0.1931, -0.0249,  0.1657, -0.0997,  0.0687, -0.1125, -0.2778,  0.0163,\n",
      "        -0.2127, -0.2116, -0.1091,  0.1054,  0.2549,  0.2965,  0.0145,  0.0660,\n",
      "        -0.1624,  0.2841,  0.0210, -0.0436, -0.1952,  0.2271,  0.2290, -0.2897,\n",
      "         0.2864, -0.2730,  0.2922,  0.0786,  0.2531,  0.1794, -0.1115, -0.2798,\n",
      "         0.0356, -0.1551,  0.0831,  0.2643, -0.1746,  0.2181,  0.0653,  0.0814,\n",
      "        -0.0537,  0.0539,  0.2520,  0.0199,  0.1531,  0.0614,  0.2748, -0.3002,\n",
      "         0.1219, -0.2122, -0.1631, -0.1681, -0.0185,  0.0782, -0.1613, -0.2044,\n",
      "        -0.0142,  0.0375, -0.1722, -0.1976, -0.0613, -0.0207,  0.0363, -0.1575,\n",
      "        -0.0928,  0.1729,  0.2558, -0.1558,  0.2242,  0.2890,  0.2902,  0.0747,\n",
      "        -0.0839, -0.0538, -0.0766, -0.2373,  0.0298, -0.0279,  0.1766,  0.1254,\n",
      "        -0.0312, -0.1889,  0.2507,  0.1399, -0.2916, -0.0606,  0.2620,  0.2371,\n",
      "         0.0271,  0.2982,  0.0881, -0.1257, -0.1925,  0.1016,  0.2289,  0.2053,\n",
      "         0.1121, -0.1936,  0.0434, -0.0033,  0.0369,  0.2812, -0.0046, -0.2101,\n",
      "         0.1726,  0.1283,  0.1392, -0.2868,  0.0306,  0.2251, -0.0042,  0.2264,\n",
      "         0.0210, -0.2270,  0.0823,  0.1144,  0.1638,  0.2373, -0.0120,  0.1358])\n",
      "fc2.weight:\n",
      "tensor([[-0.0340, -0.0058, -0.0467,  0.0282, -0.0404, -0.0728,  0.0139, -0.0756,\n",
      "          0.0479,  0.0861,  0.0223, -0.0131, -0.0110, -0.0324, -0.0306, -0.0440,\n",
      "         -0.0814,  0.0595,  0.0548, -0.0188,  0.0324, -0.0526, -0.0685,  0.0643,\n",
      "          0.0150,  0.0643, -0.0713,  0.0403,  0.0188,  0.0580,  0.0643, -0.0183,\n",
      "         -0.0688,  0.0253, -0.0434,  0.0338, -0.0089,  0.0365, -0.0424, -0.0007,\n",
      "         -0.0031,  0.0186,  0.0339, -0.0069, -0.0783, -0.0002,  0.0574, -0.0139,\n",
      "         -0.0260,  0.0144,  0.0535,  0.0597,  0.0617,  0.0504,  0.0844,  0.0623,\n",
      "         -0.0482,  0.0821, -0.0063,  0.0366, -0.0527, -0.0601,  0.0478,  0.0693,\n",
      "         -0.0706, -0.0636, -0.0599,  0.0776,  0.0848,  0.0028,  0.0122, -0.0097,\n",
      "          0.0428, -0.0329, -0.0786, -0.0789, -0.0787, -0.0558, -0.0111,  0.0623,\n",
      "          0.0209,  0.0124, -0.0683, -0.0567, -0.0454, -0.0577, -0.0457, -0.0199,\n",
      "         -0.0357,  0.0092, -0.0231, -0.0800,  0.0424, -0.0070, -0.0331, -0.0531,\n",
      "         -0.0198, -0.0406, -0.0228,  0.0601, -0.0557, -0.0754,  0.0691,  0.0722,\n",
      "          0.0143,  0.0155,  0.0684, -0.0660, -0.0323,  0.0408,  0.0286, -0.0627,\n",
      "          0.0069,  0.0855, -0.0040, -0.0363,  0.0698,  0.0618,  0.0673,  0.0820,\n",
      "          0.0318,  0.0011, -0.0143, -0.0545,  0.0625,  0.0356, -0.0055, -0.0379]])\n",
      "fc2.bias:\n",
      "tensor([0.0553])\n"
     ]
    }
   ],
   "source": [
    "# ... (after the training loop)\n",
    "\n",
    "# Retrieve the final parameters of HyperNet\n",
    "hypernet_params = hypernet.state_dict()\n",
    "print(\"Final parameters of HyperNet:\")\n",
    "for param_name, param in hypernet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n",
    "\n",
    "# Retrieve the final parameters of PredNet\n",
    "prednet_params = model.state_dict()\n",
    "print(\"\\nFinal parameters of PredNet:\")\n",
    "for param_name, param in prednet_params.items():\n",
    "    print(f\"{param_name}:\\n{param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise error\n",
    "raise ValueError(\"This is a custom error message. You can replace this with an actual error message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     53\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not DataFrame"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Bernoulli\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, input_dim)  # The output should match the number of features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = X_train_tensor.shape[1]  # This should be the number of features in your input\n",
    "feature_dim = input_dim  # This should match the number of input features\n",
    "hypernet = HyperNet(input_dim)\n",
    "model = PredNet(feature_dim)\n",
    "\n",
    "# Rest of the initialization and training code...\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(list(model.parameters()) + list(hypernet.parameters()), lr=0.001)\n",
    "\n",
    "hypernet.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass through hypernetwork to get selection probabilities\n",
    "selection_prob = hypernet(X_train)\n",
    "# Sample from Bernoulli distribution to get feature mask\n",
    "selection_mask = torch.bernoulli(selection_prob)\n",
    "# Apply mask to input features\n",
    "selected_features = X_train * selection_mask\n",
    "\n",
    "# Make predictions with masked features\n",
    "y_pred = model(selected_features)\n",
    "loss = criterion(y_pred, y_train)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Validation phase\n",
    "model.eval()\n",
    "hypernet.eval()\n",
    "# with torch.no_grad():\n",
    "#     val_selection_prob = hypernet(X_val)\n",
    "#     val_selection_mask = torch.bernoulli(val_selection_prob)\n",
    "#     val_selected_features = X_val * val_selection_mask\n",
    "#     val_y_pred = model(val_selected_features)\n",
    "#     val_loss = criterion(val_y_pred, y_val)\n",
    "\n",
    "# print(f'Epoch {epoch+1}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "# Model initialization\n",
    "\n",
    "\n",
    "# Training\n",
    "# epochs = 100 # Adjust as necessary\n",
    "# train(model, hypernet, criterion, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m    119\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Adjust as necessary\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypernet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Forward pass through hypernetwork to get selection probabilities\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m selection_prob \u001b[38;5;241m=\u001b[39m \u001b[43mhypernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Sample from Bernoulli distribution to get feature mask\u001b[39;00m\n\u001b[0;32m     86\u001b[0m selection_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(selection_prob)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mHyperNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16512x128 and 11x128)"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the Hypernetwork\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class HyperNet(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim):\n",
    "#         super(HyperNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 128)\n",
    "#         # Initialize weights using Xavier initialization\n",
    "#         xavier_uniform_(self.fc1.weight)\n",
    "#         self.fc2 = nn.Linear(128, feature_dim)\n",
    "#         xavier_uniform_(self.fc2.weight)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         # Final layer with sigmoid to ensure outputs are between 0 and 1\n",
    "#         x = self.sigmoid(self.fc2(x))\n",
    "#         return x\n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(128, feature_dim)  # feature_dim should match the number of input features\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the Prediction Network\n",
    "# class PredNet(nn.Module):\n",
    "#     def __init__(self, feature_dim):\n",
    "#         super(PredNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(feature_dim, 128) # Adjust the size as necessary\n",
    "#         self.fc2 = nn.Linear(128, 1) # Assuming a single output\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class PredNet(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(PredNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 128)  # feature_dim should match the number of input features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "# Initialize models (assuming 11 input features)\n",
    "input_dim = 11\n",
    "feature_dim = input_dim  # Make sure this matches the number of input features\n",
    "hypernet = HyperNet(input_dim, feature_dim)\n",
    "model = PredNet(feature_dim)\n",
    "def train(model, hypernet, criterion, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.bernoulli import Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Hypernetwork, self).__init__()\n",
    "        # Define the architecture of the hypernetwork.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer mapping to Bernoulli parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = torch.sigmoid(self.fc2(x)) # Sigmoid to ensure output is in [0,1], representing probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        # Define the architecture of the prediction network.\n",
    "        self.fc1 = nn.Linear(input_dim, 128) # Input layer\n",
    "        self.fc2 = nn.Linear(128, output_dim) # Output layer for the response variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x)) # Activation function for non-linearity\n",
    "        x = self.fc2(x) # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hypernet, criterion, optimizer, data_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for x, z, y in data_loader: # Assuming x is the feature, z is the context, y is the target\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Generate probabilities from the Hypernetwork\n",
    "            probs = hypernet(z)\n",
    "            \n",
    "            # Sample from Bernoulli to get the feature selection mask\n",
    "            m = Bernoulli(probs)\n",
    "            mask = m.sample()\n",
    "            \n",
    "            # Apply mask and predict\n",
    "            x_masked = x * mask\n",
    "            predictions = model(x_masked)\n",
    "            \n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
