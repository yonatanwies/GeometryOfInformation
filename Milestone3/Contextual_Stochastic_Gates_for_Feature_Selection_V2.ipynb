{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTEXTUAL FEATURE SELECTION WITH CONDITIONAL STOCHASTIC GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>MedInc</th>\n",
       "      <th>Uniform_Noise</th>\n",
       "      <th>Cosine_Noise</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>Gaussian_Noise</th>\n",
       "      <th>Population</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.247299</td>\n",
       "      <td>4.5625</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>-0.958842</td>\n",
       "      <td>4.845138</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-0.706843</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>-122.59</td>\n",
       "      <td>1.027611</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.267930</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>-0.134879</td>\n",
       "      <td>0.105413</td>\n",
       "      <td>5.262517</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.869742</td>\n",
       "      <td>2415.0</td>\n",
       "      <td>-119.19</td>\n",
       "      <td>1.012179</td>\n",
       "      <td>34.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.445217</td>\n",
       "      <td>5.2174</td>\n",
       "      <td>0.784740</td>\n",
       "      <td>0.261862</td>\n",
       "      <td>7.306957</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.116566</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>-117.21</td>\n",
       "      <td>1.078261</td>\n",
       "      <td>33.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.345733</td>\n",
       "      <td>2.3083</td>\n",
       "      <td>-0.577435</td>\n",
       "      <td>-0.877524</td>\n",
       "      <td>5.485777</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.198482</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-122.63</td>\n",
       "      <td>1.262582</td>\n",
       "      <td>38.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.496000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>0.833185</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>5.442667</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.133279</td>\n",
       "      <td>936.0</td>\n",
       "      <td>-117.24</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>34.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AveOccup  MedInc  Uniform_Noise  Cosine_Noise  AveRooms  HouseAge  \\\n",
       "0  2.247299  4.5625       0.760274     -0.958842  4.845138      46.0   \n",
       "1  3.267930  4.5000      -0.134879      0.105413  5.262517      17.0   \n",
       "2  3.445217  5.2174       0.784740      0.261862  7.306957       5.0   \n",
       "3  2.345733  2.3083      -0.577435     -0.877524  5.485777      20.0   \n",
       "4  2.496000  6.0000       0.833185      0.042673  5.442667      26.0   \n",
       "\n",
       "   Gaussian_Noise  Population  Longitude  AveBedrms  Latitude  \n",
       "0       -0.706843      1872.0    -122.59   1.027611     37.97  \n",
       "1       -1.869742      2415.0    -119.19   1.012179     34.23  \n",
       "2        0.116566      3962.0    -117.21   1.078261     33.95  \n",
       "3        1.198482      1072.0    -122.63   1.262582     38.96  \n",
       "4       -0.133279       936.0    -117.24   0.781333     34.15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Reduce to maximum 1000 rows\n",
    "# X = X[:1000, :]\n",
    "# y = y[:1000]\n",
    "\n",
    "# Original column names\n",
    "column_names = housing.feature_names\n",
    "feature_names = np.array(column_names)\n",
    "# Add noise columns\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Gaussian noise\n",
    "gaussian_noise = np.random.normal(0, 1, size=X.shape[0])\n",
    "\n",
    "# Uniform noise\n",
    "uniform_noise = np.random.uniform(-1, 1, size=X.shape[0])\n",
    "\n",
    "# Cosine function\n",
    "cosine_values = np.cos(np.linspace(0, 10, X.shape[0]))\n",
    "\n",
    "# Create a DataFrame from X\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "# Add the noise columns to DataFrame\n",
    "df['Gaussian_Noise'] = gaussian_noise\n",
    "df['Uniform_Noise'] = uniform_noise\n",
    "df['Cosine_Noise'] = cosine_values\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Shuffle column locations\n",
    "np.random.seed(42)  # Ensure reproducibility for column shuffling\n",
    "shuffled_columns = np.random.permutation(df.columns)\n",
    "df = df[shuffled_columns]\n",
    "\n",
    "# Now, df is a DataFrame with shuffled columns and includes the noise features\n",
    "# You can view the DataFrame as follows:\n",
    "display(df.head())\n",
    "\n",
    "# Convert target to a DataFrame and concatenate with features for a complete view\n",
    "y_df = pd.DataFrame(y, columns=['Target'])\n",
    "df_full = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "# If you wish to proceed with splitting and scaling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Splitting the data (assuming you want to keep DataFrame structure for X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# new column names\n",
    "feature_names = np.array(X_train_scaled.columns)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "# Let's define a function to calculate the custom loss\n",
    "def custom_loss_function(model_output, y_true, mu_d_z, sigma, D, lambda_reg):\n",
    "    # Calculate MSE loss\n",
    "    mse_loss = nn.functional.mse_loss(model_output, y_true)\n",
    "    # Calculate the regularization term\n",
    "    normal_dist = Normal(torch.zeros_like(mu_d_z), torch.ones_like(mu_d_z) * sigma)\n",
    "    regularization_term = torch.sum(normal_dist.cdf(mu_d_z)) / D  # normalize by D\n",
    "    # Combine MSE loss with the regularization term\n",
    "    total_loss = mse_loss + lambda_reg * regularization_term\n",
    "    return total_loss\n",
    "lambda_reg = 0.01\n",
    "sigma = 1.0  # Given constant sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.init import xavier_uniform_\n",
    "    \n",
    "from torch.distributions.normal import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the Hypernetwork\n",
    "class C_StochasticGates(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(C_StochasticGates, self).__init__()\n",
    "        self.dropout1 = nn.Dropout(0.5)   # Dropout layer with 50% probability\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        xavier_uniform_(self.fc1.weight)  # Xavier initialization\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        xavier_uniform_(self.fc2.weight)  # Xavier initialization\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        xavier_uniform_(self.fc3.weight)  # Xavier initialization\n",
    "        self.fc4 = nn.Linear(32, input_dim)\n",
    "        xavier_uniform_(self.fc4.weight)  # Xavier initialization\n",
    "        self.fc5 = nn.Linear(64, 1) # final layer for regression\n",
    "        xavier_uniform_(self.fc5.weight)  # Xavier initialization\n",
    "        self.sigma = 1.0  # Given constant sigma\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\" hϕ: selection of contextual probabilities \"\"\"\n",
    "        x_original = x# Copy the input tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Assuming that self.fc4 outputs the logit z which is fed into the sigmoid to obtain mu_z\n",
    "        mu_z = torch.sigmoid(x)  # Sigmoid to ensure output is a probability\n",
    "\n",
    "        # stochastic_gates = torch.stack(stochastic_gates, dim=1)\n",
    "        stochastic_gates = []\n",
    "        for d in range(mu_z.size(1)):  # Loop over each dimension\n",
    "            epsilon_d = torch.normal(0, self.sigma, size=(mu_z.size(0),), device=mu_z.device)\n",
    "            sigma_d = torch.clamp(mu_z[:, d] + epsilon_d, min=0, max=1)\n",
    "            stochastic_gates.append(sigma_d)\n",
    "\n",
    "        stochastic_gates = torch.stack(stochastic_gates, dim=1)\n",
    "        \"\"\"\" Fully connected network for regression parameters yˆ(k)\"\"\"\n",
    "        selected_features = x_original * stochastic_gates\n",
    "        x = torch.relu(self.fc1(selected_features))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after activation\n",
    "        x = self.fc5(x)  # No activation function, suitable for regression\n",
    "        return x,stochastic_gates\n",
    "# c_stg = C_StochasticGates(input_dim=X_train_scaled.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your custom_loss_function is defined somewhere above\n",
    "\n",
    "def mse_metric(preds, targets):\n",
    "    return torch.mean((preds - targets) ** 2)\n",
    "\n",
    "def rmse_metric(preds, targets):\n",
    "    return torch.sqrt(mse_metric(preds, targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, stochastic_gates_pred = model(X_train)\n",
    "        loss = custom_loss_function(y_pred, y_train, stochastic_gates_pred, sigma=1, D=X_train.shape[1], lambda_reg=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse = mse_metric(y_pred, y_train).item()\n",
    "        train_rmse = rmse_metric(y_pred, y_train).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_pred, val_stochastic_gates = model(X_val)\n",
    "            val_loss = custom_loss_function(val_y_pred, y_val, val_stochastic_gates, sigma=1, D=X_val.shape[1], lambda_reg=0.05)\n",
    "            val_mse = mse_metric(val_y_pred, y_val).item()\n",
    "            val_rmse = rmse_metric(val_y_pred, y_val).item()\n",
    "        #\"Gates: {stochastic_gates_pred}\\n \n",
    "        print(f'Epoch {epoch+1},\\nTrain: \\n  Loss: {loss.item()}, MSE: {train_mse}, RMSE: {train_rmse}, \\nVal:\\n   Loss: {val_loss.item()}, Val MSE: {val_mse}, Val RMSE: {val_rmse}')\n",
    "    return stochastic_gates_pred\n",
    "\n",
    "# Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "model = C_StochasticGates(input_dim=X_train_tensor.shape[1])\n",
    "optimizer = Adam(list(model.parameters()) , lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 100  # Adjust as necessary based on convergence and performance\n",
    "# gates = train(model,  optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      "Train: Loss: 570.35791015625, MSE: 6.9034423828125, RMSE: 2.6274402141571045, R^2: -4.164239406585693\n",
      "Val: Loss: 146.06898498535156, Val MSE: 5.910678863525391, Val RMSE: 2.4311888217926025, Val R^2: -3.5105600357055664\n",
      "Epoch 2,\n",
      "Train: Loss: 570.2364501953125, MSE: 6.448670864105225, RMSE: 2.5394232273101807, R^2: -3.8240394592285156\n",
      "Val: Loss: 145.5257110595703, Val MSE: 5.504674434661865, Val RMSE: 2.3462042808532715, Val R^2: -3.2007298469543457\n",
      "Epoch 3,\n",
      "Train: Loss: 568.9860229492188, MSE: 5.948492527008057, RMSE: 2.438953161239624, R^2: -3.4498724937438965\n",
      "Val: Loss: 144.679443359375, Val MSE: 5.118729114532471, Val RMSE: 2.262460947036743, Val R^2: -2.9062068462371826\n",
      "Epoch 4,\n",
      "Train: Loss: 568.32421875, MSE: 5.557180881500244, RMSE: 2.3573672771453857, R^2: -3.1571450233459473\n",
      "Val: Loss: 144.61224365234375, Val MSE: 4.73947286605835, Val RMSE: 2.177032947540283, Val R^2: -2.616788864135742\n",
      "Epoch 5,\n",
      "Train: Loss: 568.0288696289062, MSE: 5.1136579513549805, RMSE: 2.2613399028778076, R^2: -2.825360059738159\n",
      "Val: Loss: 144.1827392578125, Val MSE: 4.423236846923828, Val RMSE: 2.103149175643921, Val R^2: -2.375462532043457\n",
      "Epoch 6,\n",
      "Train: Loss: 567.0006103515625, MSE: 4.778165817260742, RMSE: 2.185901641845703, R^2: -2.5743894577026367\n",
      "Val: Loss: 143.8756561279297, Val MSE: 4.100383281707764, Val RMSE: 2.024940252304077, Val R^2: -2.129086494445801\n",
      "Epoch 7,\n",
      "Train: Loss: 566.370361328125, MSE: 4.428427696228027, RMSE: 2.1043829917907715, R^2: -2.3127617835998535\n",
      "Val: Loss: 143.6156005859375, Val MSE: 3.8517391681671143, Val RMSE: 1.9625848531723022, Val R^2: -1.9393410682678223\n",
      "Epoch 8,\n",
      "Train: Loss: 566.1666870117188, MSE: 4.220728397369385, RMSE: 2.054441213607788, R^2: -2.157388687133789\n",
      "Val: Loss: 143.2236785888672, Val MSE: 3.5421388149261475, Val RMSE: 1.8820570707321167, Val R^2: -1.7030787467956543\n",
      "Epoch 9,\n",
      "Train: Loss: 566.2197875976562, MSE: 3.8552498817443848, RMSE: 1.9634790420532227, R^2: -1.883986234664917\n",
      "Val: Loss: 143.0911407470703, Val MSE: 3.308608055114746, Val RMSE: 1.8189579248428345, Val R^2: -1.5248665809631348\n",
      "Epoch 10,\n",
      "Train: Loss: 565.091796875, MSE: 3.6746370792388916, RMSE: 1.9169342517852783, R^2: -1.7488758563995361\n",
      "Val: Loss: 142.90599060058594, Val MSE: 3.082526683807373, Val RMSE: 1.755712628364563, Val R^2: -1.352339267730713\n",
      "Epoch 11,\n",
      "Train: Loss: 564.7763061523438, MSE: 3.4555881023406982, RMSE: 1.8589211702346802, R^2: -1.585012435913086\n",
      "Val: Loss: 142.77127075195312, Val MSE: 2.936126470565796, Val RMSE: 1.713512897491455, Val R^2: -1.2406182289123535\n",
      "Epoch 12,\n",
      "Train: Loss: 564.4091186523438, MSE: 3.2449662685394287, RMSE: 1.8013789653778076, R^2: -1.4274530410766602\n",
      "Val: Loss: 142.2862548828125, Val MSE: 2.7209019660949707, Val RMSE: 1.6495156288146973, Val R^2: -1.07637619972229\n",
      "Epoch 13,\n",
      "Train: Loss: 563.8328247070312, MSE: 3.11128568649292, RMSE: 1.7638837099075317, R^2: -1.3274509906768799\n",
      "Val: Loss: 142.16595458984375, Val MSE: 2.557666301727295, Val RMSE: 1.5992705821990967, Val R^2: -0.9518076181411743\n",
      "Epoch 14,\n",
      "Train: Loss: 563.748779296875, MSE: 2.912125825881958, RMSE: 1.7064951658248901, R^2: -1.1784660816192627\n",
      "Val: Loss: 142.1327362060547, Val MSE: 2.4338300228118896, Val RMSE: 1.560073733329773, Val R^2: -0.8573055267333984\n",
      "Epoch 15,\n",
      "Train: Loss: 563.8067626953125, MSE: 2.8118722438812256, RMSE: 1.6768637895584106, R^2: -1.1034696102142334\n",
      "Val: Loss: 142.22291564941406, Val MSE: 2.3396365642547607, Val RMSE: 1.5295870304107666, Val R^2: -0.7854245901107788\n",
      "Epoch 16,\n",
      "Train: Loss: 563.7886962890625, MSE: 2.738353967666626, RMSE: 1.6547973155975342, R^2: -1.0484728813171387\n",
      "Val: Loss: 141.86163330078125, Val MSE: 2.2185237407684326, Val RMSE: 1.4894709587097168, Val R^2: -0.6930010318756104\n",
      "Epoch 17,\n",
      "Train: Loss: 563.4153442382812, MSE: 2.5472466945648193, RMSE: 1.596009612083435, R^2: -0.9055118560791016\n",
      "Val: Loss: 141.61866760253906, Val MSE: 2.080498218536377, Val RMSE: 1.442393183708191, Val R^2: -0.5876708030700684\n",
      "Epoch 18,\n",
      "Train: Loss: 562.536376953125, MSE: 2.462625741958618, RMSE: 1.5692754983901978, R^2: -0.8422096967697144\n",
      "Val: Loss: 141.63954162597656, Val MSE: 2.0411791801452637, Val RMSE: 1.4286984205245972, Val R^2: -0.5576655864715576\n",
      "Epoch 19,\n",
      "Train: Loss: 563.137451171875, MSE: 2.457048177719116, RMSE: 1.5674973726272583, R^2: -0.8380374908447266\n",
      "Val: Loss: 141.49928283691406, Val MSE: 1.9096367359161377, Val RMSE: 1.3818960189819336, Val R^2: -0.4572829008102417\n",
      "Epoch 20,\n",
      "Train: Loss: 562.4773559570312, MSE: 2.3480238914489746, RMSE: 1.532326340675354, R^2: -0.7564798593521118\n",
      "Val: Loss: 141.55015563964844, Val MSE: 1.8949366807937622, Val RMSE: 1.376567006111145, Val R^2: -0.4460650682449341\n",
      "Epoch 21,\n",
      "Train: Loss: 562.865234375, MSE: 2.335097074508667, RMSE: 1.5281023979187012, R^2: -0.7468096017837524\n",
      "Val: Loss: 141.51670837402344, Val MSE: 1.8010910749435425, Val RMSE: 1.3420473337173462, Val R^2: -0.3744494915008545\n",
      "Epoch 22,\n",
      "Train: Loss: 562.4120483398438, MSE: 2.240384817123413, RMSE: 1.4967914819717407, R^2: -0.6759586334228516\n",
      "Val: Loss: 141.4480438232422, Val MSE: 1.753790020942688, Val RMSE: 1.3243073225021362, Val R^2: -0.3383530378341675\n",
      "Epoch 23,\n",
      "Train: Loss: 562.3853149414062, MSE: 2.411675453186035, RMSE: 1.5529570579528809, R^2: -0.8040955066680908\n",
      "Val: Loss: 141.3124237060547, Val MSE: 1.7475855350494385, Val RMSE: 1.3219627141952515, Val R^2: -0.33361828327178955\n",
      "Epoch 24,\n",
      "Train: Loss: 562.54296875, MSE: 2.2578392028808594, RMSE: 1.5026108026504517, R^2: -0.6890156269073486\n",
      "Val: Loss: 141.17449951171875, Val MSE: 1.7065702676773071, Val RMSE: 1.3063576221466064, Val R^2: -0.3023186922073364\n",
      "Epoch 25,\n",
      "Train: Loss: 562.1121826171875, MSE: 2.2424299716949463, RMSE: 1.4974745512008667, R^2: -0.6774884462356567\n",
      "Val: Loss: 141.08114624023438, Val MSE: 1.6134493350982666, Val RMSE: 1.2702162265777588, Val R^2: -0.23125624656677246\n",
      "Epoch 26,\n",
      "Train: Loss: 562.222900390625, MSE: 2.0719571113586426, RMSE: 1.4394294023513794, R^2: -0.5499633550643921\n",
      "Val: Loss: 141.16909790039062, Val MSE: 1.626822590827942, Val RMSE: 1.2754695415496826, Val R^2: -0.2414616346359253\n",
      "Epoch 27,\n",
      "Train: Loss: 562.2958984375, MSE: 2.174870252609253, RMSE: 1.4747440814971924, R^2: -0.6269493103027344\n",
      "Val: Loss: 141.49627685546875, Val MSE: 1.5870779752731323, Val RMSE: 1.2597928047180176, Val R^2: -0.21113169193267822\n",
      "Epoch 28,\n",
      "Train: Loss: 561.6332397460938, MSE: 2.035280227661133, RMSE: 1.4266325235366821, R^2: -0.5225265026092529\n",
      "Val: Loss: 141.3025665283203, Val MSE: 1.623473882675171, Val RMSE: 1.274156093597412, Val R^2: -0.2389061450958252\n",
      "Epoch 29,\n",
      "Train: Loss: 562.0159301757812, MSE: 2.033158540725708, RMSE: 1.4258886575698853, R^2: -0.5209394693374634\n",
      "Val: Loss: 141.00344848632812, Val MSE: 1.5460346937179565, Val RMSE: 1.2433964014053345, Val R^2: -0.1798107624053955\n",
      "Epoch 30,\n",
      "Train: Loss: 561.4419555664062, MSE: 2.3451473712921143, RMSE: 1.531387448310852, R^2: -0.7543280124664307\n",
      "Val: Loss: 141.37364196777344, Val MSE: 1.5443110466003418, Val RMSE: 1.2427030801773071, Val R^2: -0.1784954071044922\n",
      "Epoch 31,\n",
      "Train: Loss: 561.6702880859375, MSE: 2.2387008666992188, RMSE: 1.4962289333343506, R^2: -0.6746989488601685\n",
      "Val: Loss: 140.85394287109375, Val MSE: 1.5394903421401978, Val RMSE: 1.2407619953155518, Val R^2: -0.17481660842895508\n",
      "Epoch 32,\n",
      "Train: Loss: 561.415771484375, MSE: 2.0034542083740234, RMSE: 1.4154342412948608, R^2: -0.4987185001373291\n",
      "Val: Loss: 141.02745056152344, Val MSE: 1.5717827081680298, Val RMSE: 1.2537075281143188, Val R^2: -0.19945955276489258\n",
      "Epoch 33,\n",
      "Train: Loss: 561.504638671875, MSE: 1.985432744026184, RMSE: 1.4090538024902344, R^2: -0.4852372407913208\n",
      "Val: Loss: 140.93389892578125, Val MSE: 1.5290212631225586, Val RMSE: 1.2365360260009766, Val R^2: -0.16682744026184082\n",
      "Epoch 34,\n",
      "Train: Loss: 561.2318115234375, MSE: 2.104191780090332, RMSE: 1.4505832195281982, R^2: -0.5740770101547241\n",
      "Val: Loss: 140.97860717773438, Val MSE: 1.5111669301986694, Val RMSE: 1.229295253753662, Val R^2: -0.15320241451263428\n",
      "Epoch 35,\n",
      "Train: Loss: 561.0962524414062, MSE: 1.9632095098495483, RMSE: 1.4011458158493042, R^2: -0.46861279010772705\n",
      "Val: Loss: 141.1116180419922, Val MSE: 1.5284420251846313, Val RMSE: 1.2363017797470093, Val R^2: -0.16638541221618652\n",
      "Epoch 36,\n",
      "Train: Loss: 561.4471435546875, MSE: 2.060382843017578, RMSE: 1.4354033470153809, R^2: -0.5413050651550293\n",
      "Val: Loss: 140.86581420898438, Val MSE: 1.5062600374221802, Val RMSE: 1.2272979021072388, Val R^2: -0.14945781230926514\n",
      "Epoch 37,\n",
      "Train: Loss: 561.0328979492188, MSE: 1.9386146068572998, RMSE: 1.3923413753509521, R^2: -0.450214147567749\n",
      "Val: Loss: 140.94520568847656, Val MSE: 1.510847568511963, Val RMSE: 1.2291654348373413, Val R^2: -0.15295875072479248\n",
      "Epoch 38,\n",
      "Train: Loss: 560.506591796875, MSE: 1.923043966293335, RMSE: 1.386738657951355, R^2: -0.43856632709503174\n",
      "Val: Loss: 140.90548706054688, Val MSE: 1.5090758800506592, Val RMSE: 1.2284444570541382, Val R^2: -0.15160667896270752\n",
      "Epoch 39,\n",
      "Train: Loss: 560.9530639648438, MSE: 1.9358352422714233, RMSE: 1.3913429975509644, R^2: -0.44813501834869385\n",
      "Val: Loss: 141.0435791015625, Val MSE: 1.5222619771957397, Val RMSE: 1.2337998151779175, Val R^2: -0.16166925430297852\n",
      "Epoch 40,\n",
      "Train: Loss: 560.427734375, MSE: 2.0756564140319824, RMSE: 1.440713882446289, R^2: -0.5527306795120239\n",
      "Val: Loss: 140.90318298339844, Val MSE: 1.5352603197097778, Val RMSE: 1.2390562295913696, Val R^2: -0.17158865928649902\n",
      "Epoch 41,\n",
      "Train: Loss: 561.1798095703125, MSE: 1.9906507730484009, RMSE: 1.4109042882919312, R^2: -0.4891406297683716\n",
      "Val: Loss: 140.90452575683594, Val MSE: 1.510685682296753, Val RMSE: 1.2290995121002197, Val R^2: -0.15283513069152832\n",
      "Epoch 42,\n",
      "Train: Loss: 560.9161987304688, MSE: 1.9502850770950317, RMSE: 1.3965260982513428, R^2: -0.4589444398880005\n",
      "Val: Loss: 141.09376525878906, Val MSE: 1.518876552581787, Val RMSE: 1.2324271202087402, Val R^2: -0.15908575057983398\n",
      "Epoch 43,\n",
      "Train: Loss: 561.179931640625, MSE: 1.9598057270050049, RMSE: 1.3999305963516235, R^2: -0.4660665988922119\n",
      "Val: Loss: 140.88140869140625, Val MSE: 1.5166095495224, Val RMSE: 1.2315070629119873, Val R^2: -0.1573559045791626\n",
      "Epoch 44,\n",
      "Train: Loss: 560.4921875, MSE: 1.9028937816619873, RMSE: 1.3794541358947754, R^2: -0.42349255084991455\n",
      "Val: Loss: 141.11215209960938, Val MSE: 1.5106991529464722, Val RMSE: 1.229104995727539, Val R^2: -0.15284550189971924\n",
      "Epoch 45,\n",
      "Train: Loss: 560.87890625, MSE: 1.9538439512252808, RMSE: 1.3977997303009033, R^2: -0.4616067409515381\n",
      "Val: Loss: 141.11045837402344, Val MSE: 1.4968252182006836, Val RMSE: 1.2234480381011963, Val R^2: -0.1422579288482666\n",
      "Epoch 46,\n",
      "Train: Loss: 560.8963623046875, MSE: 1.9271479845046997, RMSE: 1.388217568397522, R^2: -0.44163644313812256\n",
      "Val: Loss: 141.12411499023438, Val MSE: 1.4956960678100586, Val RMSE: 1.2229865789413452, Val R^2: -0.14139628410339355\n",
      "Epoch 47,\n",
      "Train: Loss: 560.9546508789062, MSE: 1.913722276687622, RMSE: 1.383373498916626, R^2: -0.43159306049346924\n",
      "Val: Loss: 140.8875274658203, Val MSE: 1.4962540864944458, Val RMSE: 1.2232146263122559, Val R^2: -0.1418222188949585\n",
      "Epoch 48,\n",
      "Train: Loss: 561.3865356445312, MSE: 1.9389266967773438, RMSE: 1.3924534320831299, R^2: -0.45044755935668945\n",
      "Val: Loss: 141.0596160888672, Val MSE: 1.4812157154083252, Val RMSE: 1.2170521020889282, Val R^2: -0.13034605979919434\n",
      "Epoch 49,\n",
      "Train: Loss: 560.4663696289062, MSE: 1.9117494821548462, RMSE: 1.3826602697372437, R^2: -0.43011724948883057\n",
      "Val: Loss: 141.38238525390625, Val MSE: 1.4626948833465576, Val RMSE: 1.2094192504882812, Val R^2: -0.11621236801147461\n",
      "Epoch 50,\n",
      "Train: Loss: 560.9161987304688, MSE: 1.85938560962677, RMSE: 1.3635928630828857, R^2: -0.39094555377960205\n",
      "Val: Loss: 141.03065490722656, Val MSE: 1.45919668674469, Val RMSE: 1.2079721689224243, Val R^2: -0.11354291439056396\n",
      "Epoch 51,\n",
      "Train: Loss: 560.8496704101562, MSE: 1.943213939666748, RMSE: 1.3939920663833618, R^2: -0.45365476608276367\n",
      "Val: Loss: 140.90322875976562, Val MSE: 1.4554301500320435, Val RMSE: 1.2064120769500732, Val R^2: -0.11066854000091553\n",
      "Epoch 52,\n",
      "Train: Loss: 560.7031860351562, MSE: 1.892694354057312, RMSE: 1.375752329826355, R^2: -0.4158627986907959\n",
      "Val: Loss: 140.88038635253906, Val MSE: 1.448047399520874, Val RMSE: 1.2033483982086182, Val R^2: -0.10503458976745605\n",
      "Epoch 53,\n",
      "Train: Loss: 560.6047973632812, MSE: 1.874647855758667, RMSE: 1.3691778182983398, R^2: -0.4023627042770386\n",
      "Val: Loss: 141.27516174316406, Val MSE: 1.4319016933441162, Val RMSE: 1.1966209411621094, Val R^2: -0.09271347522735596\n",
      "Epoch 54,\n",
      "Train: Loss: 560.71875, MSE: 1.867994785308838, RMSE: 1.3667460680007935, R^2: -0.397385835647583\n",
      "Val: Loss: 140.9804229736328, Val MSE: 1.429368019104004, Val RMSE: 1.1955617666244507, Val R^2: -0.09078001976013184\n",
      "Epoch 55,\n",
      "Train: Loss: 560.5462646484375, MSE: 1.831957459449768, RMSE: 1.3534982204437256, R^2: -0.37042737007141113\n",
      "Val: Loss: 141.08558654785156, Val MSE: 1.4239192008972168, Val RMSE: 1.193280816078186, Val R^2: -0.08662188053131104\n",
      "Epoch 56,\n",
      "Train: Loss: 560.4622192382812, MSE: 1.8957462310791016, RMSE: 1.3768609762191772, R^2: -0.4181457757949829\n",
      "Val: Loss: 140.8055877685547, Val MSE: 1.4209407567977905, Val RMSE: 1.1920322179794312, Val R^2: -0.08434903621673584\n",
      "Epoch 57,\n",
      "Train: Loss: 560.3562622070312, MSE: 1.9547337293624878, RMSE: 1.3981178998947144, R^2: -0.4622722864151001\n",
      "Val: Loss: 140.86862182617188, Val MSE: 1.4135228395462036, Val RMSE: 1.1889166831970215, Val R^2: -0.07868826389312744\n",
      "Epoch 58,\n",
      "Train: Loss: 559.9606323242188, MSE: 1.8208061456680298, RMSE: 1.3493725061416626, R^2: -0.3620854616165161\n",
      "Val: Loss: 140.9696807861328, Val MSE: 1.415948510169983, Val RMSE: 1.1899363994598389, Val R^2: -0.08053934574127197\n",
      "Epoch 59,\n",
      "Train: Loss: 560.423583984375, MSE: 1.8179247379302979, RMSE: 1.3483043909072876, R^2: -0.35993003845214844\n",
      "Val: Loss: 140.62147521972656, Val MSE: 1.4150696992874146, Val RMSE: 1.189566969871521, Val R^2: -0.07986867427825928\n",
      "Epoch 60,\n",
      "Train: Loss: 560.444091796875, MSE: 1.867704153060913, RMSE: 1.3666397333145142, R^2: -0.3971683979034424\n",
      "Val: Loss: 140.92442321777344, Val MSE: 1.40293550491333, Val RMSE: 1.1844557523727417, Val R^2: -0.07060885429382324\n",
      "Epoch 61,\n",
      "Train: Loss: 560.37451171875, MSE: 1.9140303134918213, RMSE: 1.3834848403930664, R^2: -0.4318234920501709\n",
      "Val: Loss: 140.75502014160156, Val MSE: 1.407346248626709, Val RMSE: 1.1863162517547607, Val R^2: -0.07397472858428955\n",
      "Epoch 62,\n",
      "Train: Loss: 560.6035766601562, MSE: 1.8114055395126343, RMSE: 1.3458846807479858, R^2: -0.355053186416626\n",
      "Val: Loss: 141.184326171875, Val MSE: 1.3977837562561035, Val RMSE: 1.182279109954834, Val R^2: -0.06667745113372803\n",
      "Epoch 63,\n",
      "Train: Loss: 560.7271728515625, MSE: 1.8242110013961792, RMSE: 1.3506335020065308, R^2: -0.3646324872970581\n",
      "Val: Loss: 141.02291870117188, Val MSE: 1.4004796743392944, Val RMSE: 1.18341863155365, Val R^2: -0.06873476505279541\n",
      "Epoch 64,\n",
      "Train: Loss: 559.852294921875, MSE: 1.877195119857788, RMSE: 1.370107650756836, R^2: -0.4042682647705078\n",
      "Val: Loss: 140.9305877685547, Val MSE: 1.3993539810180664, Val RMSE: 1.1829429864883423, Val R^2: -0.06787562370300293\n",
      "Epoch 65,\n",
      "Train: Loss: 560.0504760742188, MSE: 1.7809112071990967, RMSE: 1.3345078229904175, R^2: -0.332241415977478\n",
      "Val: Loss: 141.13990783691406, Val MSE: 1.3939557075500488, Val RMSE: 1.1806590557098389, Val R^2: -0.06375610828399658\n",
      "Epoch 66,\n",
      "Train: Loss: 560.1480712890625, MSE: 1.7949129343032837, RMSE: 1.3397436141967773, R^2: -0.34271562099456787\n",
      "Val: Loss: 140.8147430419922, Val MSE: 1.3923673629760742, Val RMSE: 1.1799861192703247, Val R^2: -0.06254398822784424\n",
      "Epoch 67,\n",
      "Train: Loss: 560.3931274414062, MSE: 1.8087633848190308, RMSE: 1.344902753829956, R^2: -0.353076696395874\n",
      "Val: Loss: 140.88438415527344, Val MSE: 1.3954463005065918, Val RMSE: 1.1812901496887207, Val R^2: -0.06489360332489014\n",
      "Epoch 68,\n",
      "Train: Loss: 560.7785034179688, MSE: 1.7848817110061646, RMSE: 1.3359946012496948, R^2: -0.3352116346359253\n",
      "Val: Loss: 140.9935760498047, Val MSE: 1.3976500034332275, Val RMSE: 1.1822224855422974, Val R^2: -0.06657528877258301\n",
      "Epoch 69,\n",
      "Train: Loss: 559.8945922851562, MSE: 1.769701600074768, RMSE: 1.330301284790039, R^2: -0.3238558769226074\n",
      "Val: Loss: 140.81517028808594, Val MSE: 1.3881438970565796, Val RMSE: 1.1781952381134033, Val R^2: -0.059321045875549316\n",
      "Epoch 70,\n",
      "Train: Loss: 560.001708984375, MSE: 1.7915512323379517, RMSE: 1.3384884595870972, R^2: -0.34020090103149414\n",
      "Val: Loss: 141.21438598632812, Val MSE: 1.3893228769302368, Val RMSE: 1.1786954402923584, Val R^2: -0.06022071838378906\n",
      "Epoch 71,\n",
      "Train: Loss: 560.2135620117188, MSE: 1.7987616062164307, RMSE: 1.3411791324615479, R^2: -0.34559476375579834\n",
      "Val: Loss: 140.90016174316406, Val MSE: 1.392795443534851, Val RMSE: 1.180167555809021, Val R^2: -0.06287074089050293\n",
      "Epoch 72,\n",
      "Train: Loss: 560.0633544921875, MSE: 1.7876278162002563, RMSE: 1.3370219469070435, R^2: -0.33726584911346436\n",
      "Val: Loss: 140.73875427246094, Val MSE: 1.3884800672531128, Val RMSE: 1.178337812423706, Val R^2: -0.0595775842666626\n",
      "Epoch 73,\n",
      "Train: Loss: 560.0872802734375, MSE: 1.7754919528961182, RMSE: 1.3324759006500244, R^2: -0.3281874656677246\n",
      "Val: Loss: 140.7327880859375, Val MSE: 1.3874263763427734, Val RMSE: 1.177890658378601, Val R^2: -0.05877339839935303\n",
      "Epoch 74,\n",
      "Train: Loss: 559.8916015625, MSE: 1.7740997076034546, RMSE: 1.3319532871246338, R^2: -0.32714593410491943\n",
      "Val: Loss: 140.81723022460938, Val MSE: 1.3876049518585205, Val RMSE: 1.1779664754867554, Val R^2: -0.05890977382659912\n",
      "Epoch 75,\n",
      "Train: Loss: 559.8953247070312, MSE: 1.764296054840088, RMSE: 1.328268051147461, R^2: -0.31981217861175537\n",
      "Val: Loss: 140.7448272705078, Val MSE: 1.379422664642334, Val RMSE: 1.1744883060455322, Val R^2: -0.05266571044921875\n",
      "Epoch 76,\n",
      "Train: Loss: 559.9901123046875, MSE: 1.790508508682251, RMSE: 1.3380988836288452, R^2: -0.33942079544067383\n",
      "Val: Loss: 140.8528594970703, Val MSE: 1.3776254653930664, Val RMSE: 1.1737228631973267, Val R^2: -0.05129420757293701\n",
      "Epoch 77,\n",
      "Train: Loss: 560.5756225585938, MSE: 1.7831761837005615, RMSE: 1.3353562355041504, R^2: -0.3339357376098633\n",
      "Val: Loss: 140.91050720214844, Val MSE: 1.3796324729919434, Val RMSE: 1.1745775938034058, Val R^2: -0.05282580852508545\n",
      "Epoch 78,\n",
      "Train: Loss: 560.1470947265625, MSE: 1.7798678874969482, RMSE: 1.3341169357299805, R^2: -0.33146095275878906\n",
      "Val: Loss: 141.02186584472656, Val MSE: 1.3763905763626099, Val RMSE: 1.173196792602539, Val R^2: -0.050351858139038086\n",
      "Epoch 79,\n",
      "Train: Loss: 560.2163696289062, MSE: 1.7754024267196655, RMSE: 1.332442283630371, R^2: -0.32812047004699707\n",
      "Val: Loss: 140.97705078125, Val MSE: 1.3803775310516357, Val RMSE: 1.1748946905136108, Val R^2: -0.053394317626953125\n",
      "Epoch 80,\n",
      "Train: Loss: 560.1490478515625, MSE: 1.747900128364563, RMSE: 1.3220816850662231, R^2: -0.30754685401916504\n",
      "Val: Loss: 141.12353515625, Val MSE: 1.3832464218139648, Val RMSE: 1.1761149168014526, Val R^2: -0.05558359622955322\n",
      "Epoch 81,\n",
      "Train: Loss: 560.3501586914062, MSE: 1.9270247220993042, RMSE: 1.3881731033325195, R^2: -0.4415440559387207\n",
      "Val: Loss: 140.8425750732422, Val MSE: 1.3733971118927002, Val RMSE: 1.1719202995300293, Val R^2: -0.048067450523376465\n",
      "Epoch 82,\n",
      "Train: Loss: 560.363037109375, MSE: 1.8522981405258179, RMSE: 1.3609915971755981, R^2: -0.3856436014175415\n",
      "Val: Loss: 141.08863830566406, Val MSE: 1.377295970916748, Val RMSE: 1.1735825538635254, Val R^2: -0.05104267597198486\n",
      "Epoch 83,\n",
      "Train: Loss: 560.4818725585938, MSE: 1.7449477910995483, RMSE: 1.3209646940231323, R^2: -0.30533838272094727\n",
      "Val: Loss: 140.85826110839844, Val MSE: 1.3817335367202759, Val RMSE: 1.1754716634750366, Val R^2: -0.05442917346954346\n",
      "Epoch 84,\n",
      "Train: Loss: 559.662841796875, MSE: 1.7615339756011963, RMSE: 1.3272279500961304, R^2: -0.3177459239959717\n",
      "Val: Loss: 140.9521942138672, Val MSE: 1.3818296194076538, Val RMSE: 1.175512433052063, Val R^2: -0.05450248718261719\n",
      "Epoch 85,\n",
      "Train: Loss: 560.4287109375, MSE: 1.7593663930892944, RMSE: 1.3264111280441284, R^2: -0.31612443923950195\n",
      "Val: Loss: 140.9403533935547, Val MSE: 1.377198576927185, Val RMSE: 1.1735410690307617, Val R^2: -0.05096840858459473\n",
      "Epoch 86,\n",
      "Train: Loss: 559.3401489257812, MSE: 1.783953070640564, RMSE: 1.3356471061706543, R^2: -0.33451688289642334\n",
      "Val: Loss: 141.06455993652344, Val MSE: 1.3765807151794434, Val RMSE: 1.173277735710144, Val R^2: -0.05049693584442139\n",
      "Epoch 87,\n",
      "Train: Loss: 559.8717651367188, MSE: 1.7671291828155518, RMSE: 1.3293341398239136, R^2: -0.3219316005706787\n",
      "Val: Loss: 141.0377655029297, Val MSE: 1.381698727607727, Val RMSE: 1.1754567623138428, Val R^2: -0.05440258979797363\n",
      "Epoch 88,\n",
      "Train: Loss: 559.8964233398438, MSE: 1.7501226663589478, RMSE: 1.3229219913482666, R^2: -0.3092094659805298\n",
      "Val: Loss: 140.81190490722656, Val MSE: 1.3731857538223267, Val RMSE: 1.1718300580978394, Val R^2: -0.04790616035461426\n",
      "Epoch 89,\n",
      "Train: Loss: 559.8306884765625, MSE: 1.7244240045547485, RMSE: 1.3131732940673828, R^2: -0.28998517990112305\n",
      "Val: Loss: 140.89028930664062, Val MSE: 1.3758394718170166, Val RMSE: 1.1729618310928345, Val R^2: -0.04993128776550293\n",
      "Epoch 90,\n",
      "Train: Loss: 560.0701293945312, MSE: 1.7625255584716797, RMSE: 1.327601432800293, R^2: -0.3184877634048462\n",
      "Val: Loss: 140.9739227294922, Val MSE: 1.375329852104187, Val RMSE: 1.172744631767273, Val R^2: -0.04954242706298828\n",
      "Epoch 91,\n",
      "Train: Loss: 560.186279296875, MSE: 1.8023550510406494, RMSE: 1.3425182104110718, R^2: -0.3482828140258789\n",
      "Val: Loss: 140.77919006347656, Val MSE: 1.3716681003570557, Val RMSE: 1.17118239402771, Val R^2: -0.04674804210662842\n",
      "Epoch 92,\n",
      "Train: Loss: 559.6958618164062, MSE: 1.7617090940475464, RMSE: 1.327293872833252, R^2: -0.317876935005188\n",
      "Val: Loss: 140.88392639160156, Val MSE: 1.3705123662948608, Val RMSE: 1.1706888675689697, Val R^2: -0.04586601257324219\n",
      "Epoch 93,\n",
      "Train: Loss: 559.5867919921875, MSE: 1.7386115789413452, RMSE: 1.3185641765594482, R^2: -0.30059850215911865\n",
      "Val: Loss: 141.05349731445312, Val MSE: 1.3765676021575928, Val RMSE: 1.1732721328735352, Val R^2: -0.05048692226409912\n",
      "Epoch 94,\n",
      "Train: Loss: 560.0259399414062, MSE: 1.733527421951294, RMSE: 1.3166348934173584, R^2: -0.296795129776001\n",
      "Val: Loss: 140.83822631835938, Val MSE: 1.3601034879684448, Val RMSE: 1.1662347316741943, Val R^2: -0.03792285919189453\n",
      "Epoch 95,\n",
      "Train: Loss: 559.9873657226562, MSE: 1.7369308471679688, RMSE: 1.3179267644882202, R^2: -0.299341082572937\n",
      "Val: Loss: 140.6546173095703, Val MSE: 1.3720945119857788, Val RMSE: 1.171364426612854, Val R^2: -0.04707348346710205\n",
      "Epoch 96,\n",
      "Train: Loss: 559.7002563476562, MSE: 1.7344379425048828, RMSE: 1.3169806003570557, R^2: -0.29747629165649414\n",
      "Val: Loss: 140.88760375976562, Val MSE: 1.3694705963134766, Val RMSE: 1.1702438592910767, Val R^2: -0.04507112503051758\n",
      "Epoch 97,\n",
      "Train: Loss: 559.9636840820312, MSE: 1.739311933517456, RMSE: 1.3188297748565674, R^2: -0.30112242698669434\n",
      "Val: Loss: 140.94032287597656, Val MSE: 1.364579200744629, Val RMSE: 1.168152093887329, Val R^2: -0.041338324546813965\n",
      "Epoch 98,\n",
      "Train: Loss: 560.1983642578125, MSE: 1.7044161558151245, RMSE: 1.3055329322814941, R^2: -0.27501797676086426\n",
      "Val: Loss: 140.92550659179688, Val MSE: 1.3628970384597778, Val RMSE: 1.1674318313598633, Val R^2: -0.04005467891693115\n",
      "Epoch 99,\n",
      "Train: Loss: 560.0178833007812, MSE: 1.7208552360534668, RMSE: 1.3118137121200562, R^2: -0.2873154878616333\n",
      "Val: Loss: 141.20533752441406, Val MSE: 1.3625757694244385, Val RMSE: 1.1672942638397217, Val R^2: -0.039809465408325195\n",
      "Epoch 100,\n",
      "Train: Loss: 559.8036499023438, MSE: 1.7306817770004272, RMSE: 1.3155537843704224, R^2: -0.2946664094924927\n",
      "Val: Loss: 140.9543914794922, Val MSE: 1.3602195978164673, Val RMSE: 1.1662845611572266, Val R^2: -0.03801143169403076\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared score.\"\"\"\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "\n",
    "def train(model, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, stochastic_gates_pred = model(X_train)\n",
    "        loss = custom_loss_function(y_pred, y_train, stochastic_gates_pred, sigma=1, D=X_train.shape[1], lambda_reg=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse = mse_metric(y_pred, y_train).item()\n",
    "        train_rmse = rmse_metric(y_pred, y_train).item()\n",
    "        train_r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_pred, val_stochastic_gates = model(X_val)\n",
    "            val_loss = custom_loss_function(val_y_pred, y_val, val_stochastic_gates, sigma=1, D=X_val.shape[1], lambda_reg=0.05)\n",
    "            val_mse = mse_metric(val_y_pred, y_val).item()\n",
    "            val_rmse = rmse_metric(val_y_pred, y_val).item()\n",
    "            val_r2 = r2_score(y_val, val_y_pred)\n",
    "\n",
    "        print(f'Epoch {epoch+1},\\n'\n",
    "              f'Train: Loss: {loss.item()}, MSE: {train_mse}, RMSE: {train_rmse}, R^2: {train_r2}\\n'\n",
    "              f'Val: Loss: {val_loss.item()}, Val MSE: {val_mse}, Val RMSE: {val_rmse}, Val R^2: {val_r2}')\n",
    "\n",
    "    return stochastic_gates_pred\n",
    "\n",
    "# Note: Ensure custom_loss_function, mse_metric, rmse_metric, and C_StochasticGates are defined\n",
    "# Also, ensure X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor are properly initialized\n",
    "\n",
    "# Example initialization (for illustration purposes)\n",
    "model = C_StochasticGates(input_dim=X_train_tensor.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "epochs = 100  # Adjust as necessary\n",
    "gates = train(model, optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "# from stg import STG\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# split the data into features and target variable \n",
    "\n",
    "reg_url = 'https://raw.githubusercontent.com/FreeDataSets/DataPool/main/tracks_150000.csv' # this is the url for the dataset\n",
    "reg_df = pd.read_csv(reg_url)#.sample(100000,random_state=42) # In order to reduce the size of the dataset, we are taking a random sample of 5000 rows from the dataset\n",
    "reg_df.drop(['name', 'artists','id','release_date', 'artists_id','genre',], axis=1, inplace=True, errors='ignore') # Removing Categorical features with more then 10 unique values\n",
    "reg_df = reg_df.sample(800, random_state=42)\n",
    "# a preview of the dataframe\n",
    "# reg_df.info() \n",
    "# display(reg_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xreg = reg_df.drop('popularity', axis=1).values # features\n",
    "yreg = reg_df['popularity'] # target variable\n",
    "\n",
    "\n",
    "# split the data into train and test sets\n",
    "Xreg_train, Xreg_test, yreg_train, yreg_test = train_test_split(Xreg, yreg, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler_reg = StandardScaler().fit(Xreg_train)\n",
    "\n",
    "Xreg_train_scaled = scaler_reg.transform(Xreg_train)\n",
    "Xreg_test_scaled = scaler_reg.transform(Xreg_test)\n",
    "scaler_y = StandardScaler().fit(yreg_train.values.reshape(-1, 1))\n",
    "yreg_train_scaled = scaler_y.transform(yreg_train.values.reshape(-1, 1))\n",
    "yreg_test_scaled = scaler_y.transform(yreg_test.values.reshape(-1, 1))\n",
    "# Define model parameters\n",
    "args_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if args_cuda else \"cpu\")\n",
    "\n",
    "print()\n",
    "\n",
    "# X_train_scaled and X_test_scaled are now DataFrames with scaled features and retained column names.\n",
    "# convert them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(Xreg_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(Xreg_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(yreg_train_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(yreg_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      "Train: \n",
      "  Loss: 23.503328323364258, MSE: 1.5982611179351807, RMSE: 1.264223575592041, \n",
      "Val:\n",
      "   Loss: 6.6467061042785645, Val MSE: 1.256059169769287, Val RMSE: 1.1207404136657715\n",
      "Epoch 2,\n",
      "Train: \n",
      "  Loss: 23.501220703125, MSE: 1.616357445716858, RMSE: 1.2713605165481567, \n",
      "Val:\n",
      "   Loss: 6.767306804656982, Val MSE: 1.2940722703933716, Val RMSE: 1.137573003768921\n",
      "Epoch 3,\n",
      "Train: \n",
      "  Loss: 23.533437728881836, MSE: 1.6092185974121094, RMSE: 1.2685497999191284, \n",
      "Val:\n",
      "   Loss: 6.732451438903809, Val MSE: 1.3145971298217773, Val RMSE: 1.1465587615966797\n",
      "Epoch 4,\n",
      "Train: \n",
      "  Loss: 23.592121124267578, MSE: 1.6794666051864624, RMSE: 1.2959423065185547, \n",
      "Val:\n",
      "   Loss: 6.7568769454956055, Val MSE: 1.3675134181976318, Val RMSE: 1.1694072484970093\n",
      "Epoch 5,\n",
      "Train: \n",
      "  Loss: 23.553632736206055, MSE: 1.6617352962493896, RMSE: 1.2890831232070923, \n",
      "Val:\n",
      "   Loss: 6.754984378814697, Val MSE: 1.3274564743041992, Val RMSE: 1.1521530151367188\n",
      "Epoch 6,\n",
      "Train: \n",
      "  Loss: 23.503028869628906, MSE: 1.6658868789672852, RMSE: 1.2906924486160278, \n",
      "Val:\n",
      "   Loss: 6.748350620269775, Val MSE: 1.3152416944503784, Val RMSE: 1.1468398571014404\n",
      "Epoch 7,\n",
      "Train: \n",
      "  Loss: 23.521759033203125, MSE: 1.6811130046844482, RMSE: 1.2965774536132812, \n",
      "Val:\n",
      "   Loss: 6.7063798904418945, Val MSE: 1.2999404668807983, Val RMSE: 1.1401493549346924\n",
      "Epoch 8,\n",
      "Train: \n",
      "  Loss: 23.61226463317871, MSE: 1.659229040145874, RMSE: 1.288110613822937, \n",
      "Val:\n",
      "   Loss: 6.714828968048096, Val MSE: 1.301146388053894, Val RMSE: 1.14067804813385\n",
      "Epoch 9,\n",
      "Train: \n",
      "  Loss: 23.47580909729004, MSE: 1.6299701929092407, RMSE: 1.276702880859375, \n",
      "Val:\n",
      "   Loss: 6.7561750411987305, Val MSE: 1.329764723777771, Val RMSE: 1.1531542539596558\n",
      "Epoch 10,\n",
      "Train: \n",
      "  Loss: 23.461483001708984, MSE: 1.6247409582138062, RMSE: 1.2746533155441284, \n",
      "Val:\n",
      "   Loss: 6.749651908874512, Val MSE: 1.3134838342666626, Val RMSE: 1.1460732221603394\n",
      "Epoch 11,\n",
      "Train: \n",
      "  Loss: 23.61098289489746, MSE: 1.7127015590667725, RMSE: 1.3087022304534912, \n",
      "Val:\n",
      "   Loss: 6.713467597961426, Val MSE: 1.281245470046997, Val RMSE: 1.1319211721420288\n",
      "Epoch 12,\n",
      "Train: \n",
      "  Loss: 23.60438346862793, MSE: 1.6278785467147827, RMSE: 1.275883436203003, \n",
      "Val:\n",
      "   Loss: 6.775190353393555, Val MSE: 1.3362417221069336, Val RMSE: 1.1559592485427856\n",
      "Epoch 13,\n",
      "Train: \n",
      "  Loss: 23.383127212524414, MSE: 1.5722882747650146, RMSE: 1.2539092302322388, \n",
      "Val:\n",
      "   Loss: 6.827408790588379, Val MSE: 1.320704698562622, Val RMSE: 1.1492191553115845\n",
      "Epoch 14,\n",
      "Train: \n",
      "  Loss: 23.50860595703125, MSE: 1.6274210214614868, RMSE: 1.2757041454315186, \n",
      "Val:\n",
      "   Loss: 6.795208930969238, Val MSE: 1.3593610525131226, Val RMSE: 1.1659164428710938\n",
      "Epoch 15,\n",
      "Train: \n",
      "  Loss: 23.506332397460938, MSE: 1.639744758605957, RMSE: 1.2805252075195312, \n",
      "Val:\n",
      "   Loss: 6.719359397888184, Val MSE: 1.2796777486801147, Val RMSE: 1.1312284469604492\n",
      "Epoch 16,\n",
      "Train: \n",
      "  Loss: 23.25749397277832, MSE: 1.5088860988616943, RMSE: 1.2283672094345093, \n",
      "Val:\n",
      "   Loss: 6.747980117797852, Val MSE: 1.2733564376831055, Val RMSE: 1.128430962562561\n",
      "Epoch 17,\n",
      "Train: \n",
      "  Loss: 23.543071746826172, MSE: 1.6116746664047241, RMSE: 1.2695175409317017, \n",
      "Val:\n",
      "   Loss: 6.738605499267578, Val MSE: 1.3027795553207397, Val RMSE: 1.1413936614990234\n",
      "Epoch 18,\n",
      "Train: \n",
      "  Loss: 23.533428192138672, MSE: 1.6878347396850586, RMSE: 1.2991669178009033, \n",
      "Val:\n",
      "   Loss: 6.738724231719971, Val MSE: 1.2990549802780151, Val RMSE: 1.139760971069336\n",
      "Epoch 19,\n",
      "Train: \n",
      "  Loss: 23.511653900146484, MSE: 1.6676759719848633, RMSE: 1.291385293006897, \n",
      "Val:\n",
      "   Loss: 6.730598449707031, Val MSE: 1.2857577800750732, Val RMSE: 1.1339125633239746\n",
      "Epoch 20,\n",
      "Train: \n",
      "  Loss: 23.442480087280273, MSE: 1.630914330482483, RMSE: 1.277072548866272, \n",
      "Val:\n",
      "   Loss: 6.707695960998535, Val MSE: 1.2970383167266846, Val RMSE: 1.1388758420944214\n",
      "Epoch 21,\n",
      "Train: \n",
      "  Loss: 23.48880386352539, MSE: 1.7153985500335693, RMSE: 1.30973219871521, \n",
      "Val:\n",
      "   Loss: 6.7546515464782715, Val MSE: 1.3466129302978516, Val RMSE: 1.1604365110397339\n",
      "Epoch 22,\n",
      "Train: \n",
      "  Loss: 23.39609718322754, MSE: 1.6029475927352905, RMSE: 1.266075611114502, \n",
      "Val:\n",
      "   Loss: 6.775903701782227, Val MSE: 1.322880506515503, Val RMSE: 1.1501654386520386\n",
      "Epoch 23,\n",
      "Train: \n",
      "  Loss: 23.511831283569336, MSE: 1.644020676612854, RMSE: 1.282193660736084, \n",
      "Val:\n",
      "   Loss: 6.7650957107543945, Val MSE: 1.3074820041656494, Val RMSE: 1.1434518098831177\n",
      "Epoch 24,\n",
      "Train: \n",
      "  Loss: 23.549110412597656, MSE: 1.6534610986709595, RMSE: 1.285869836807251, \n",
      "Val:\n",
      "   Loss: 6.706092834472656, Val MSE: 1.260613203048706, Val RMSE: 1.1227703094482422\n",
      "Epoch 25,\n",
      "Train: \n",
      "  Loss: 23.438098907470703, MSE: 1.5600612163543701, RMSE: 1.2490241527557373, \n",
      "Val:\n",
      "   Loss: 6.7618842124938965, Val MSE: 1.326595664024353, Val RMSE: 1.151779294013977\n",
      "Epoch 26,\n",
      "Train: \n",
      "  Loss: 23.414064407348633, MSE: 1.6475026607513428, RMSE: 1.2835508584976196, \n",
      "Val:\n",
      "   Loss: 6.7432451248168945, Val MSE: 1.2812697887420654, Val RMSE: 1.1319319009780884\n",
      "Epoch 27,\n",
      "Train: \n",
      "  Loss: 23.505428314208984, MSE: 1.6507514715194702, RMSE: 1.284815788269043, \n",
      "Val:\n",
      "   Loss: 6.660167694091797, Val MSE: 1.2696746587753296, Val RMSE: 1.126798391342163\n",
      "Epoch 28,\n",
      "Train: \n",
      "  Loss: 23.473737716674805, MSE: 1.569398045539856, RMSE: 1.2527562379837036, \n",
      "Val:\n",
      "   Loss: 6.751574516296387, Val MSE: 1.3285319805145264, Val RMSE: 1.1526196002960205\n",
      "Epoch 29,\n",
      "Train: \n",
      "  Loss: 23.56877899169922, MSE: 1.6784403324127197, RMSE: 1.295546293258667, \n",
      "Val:\n",
      "   Loss: 6.736133098602295, Val MSE: 1.30378258228302, Val RMSE: 1.141832947731018\n",
      "Epoch 30,\n",
      "Train: \n",
      "  Loss: 23.512470245361328, MSE: 1.6458438634872437, RMSE: 1.2829045057296753, \n",
      "Val:\n",
      "   Loss: 6.77158260345459, Val MSE: 1.325056791305542, Val RMSE: 1.151111125946045\n",
      "Epoch 31,\n",
      "Train: \n",
      "  Loss: 23.463037490844727, MSE: 1.5813968181610107, RMSE: 1.2575360536575317, \n",
      "Val:\n",
      "   Loss: 6.742658615112305, Val MSE: 1.3010797500610352, Val RMSE: 1.1406488418579102\n",
      "Epoch 32,\n",
      "Train: \n",
      "  Loss: 23.574739456176758, MSE: 1.6340872049331665, RMSE: 1.278314232826233, \n",
      "Val:\n",
      "   Loss: 6.718640327453613, Val MSE: 1.2804205417633057, Val RMSE: 1.1315566301345825\n",
      "Epoch 33,\n",
      "Train: \n",
      "  Loss: 23.451261520385742, MSE: 1.6980764865875244, RMSE: 1.3031026124954224, \n",
      "Val:\n",
      "   Loss: 6.768299102783203, Val MSE: 1.2915796041488647, Val RMSE: 1.1364768743515015\n",
      "Epoch 34,\n",
      "Train: \n",
      "  Loss: 23.460311889648438, MSE: 1.619288682937622, RMSE: 1.2725127935409546, \n",
      "Val:\n",
      "   Loss: 6.75100040435791, Val MSE: 1.2981345653533936, Val RMSE: 1.139357089996338\n",
      "Epoch 35,\n",
      "Train: \n",
      "  Loss: 23.32962417602539, MSE: 1.5937855243682861, RMSE: 1.262452244758606, \n",
      "Val:\n",
      "   Loss: 6.704890251159668, Val MSE: 1.3063501119613647, Val RMSE: 1.1429567337036133\n",
      "Epoch 36,\n",
      "Train: \n",
      "  Loss: 23.456506729125977, MSE: 1.611291527748108, RMSE: 1.2693666219711304, \n",
      "Val:\n",
      "   Loss: 6.737354278564453, Val MSE: 1.2846869230270386, Val RMSE: 1.1334402561187744\n",
      "Epoch 37,\n",
      "Train: \n",
      "  Loss: 23.49066162109375, MSE: 1.6022846698760986, RMSE: 1.2658138275146484, \n",
      "Val:\n",
      "   Loss: 6.778507232666016, Val MSE: 1.3083674907684326, Val RMSE: 1.143838882446289\n",
      "Epoch 38,\n",
      "Train: \n",
      "  Loss: 23.581707000732422, MSE: 1.6676912307739258, RMSE: 1.2913912534713745, \n",
      "Val:\n",
      "   Loss: 6.739194869995117, Val MSE: 1.2911741733551025, Val RMSE: 1.136298418045044\n",
      "Epoch 39,\n",
      "Train: \n",
      "  Loss: 23.37477684020996, MSE: 1.5631968975067139, RMSE: 1.2502787113189697, \n",
      "Val:\n",
      "   Loss: 6.747101783752441, Val MSE: 1.3044240474700928, Val RMSE: 1.1421138048171997\n",
      "Epoch 40,\n",
      "Train: \n",
      "  Loss: 23.44957160949707, MSE: 1.6567027568817139, RMSE: 1.2871296405792236, \n",
      "Val:\n",
      "   Loss: 6.684291839599609, Val MSE: 1.3100382089614868, Val RMSE: 1.1445690393447876\n",
      "Epoch 41,\n",
      "Train: \n",
      "  Loss: 23.480194091796875, MSE: 1.5785232782363892, RMSE: 1.2563929557800293, \n",
      "Val:\n",
      "   Loss: 6.698763847351074, Val MSE: 1.2694556713104248, Val RMSE: 1.1267012357711792\n",
      "Epoch 42,\n",
      "Train: \n",
      "  Loss: 23.501283645629883, MSE: 1.6393592357635498, RMSE: 1.2803746461868286, \n",
      "Val:\n",
      "   Loss: 6.70391321182251, Val MSE: 1.2808738946914673, Val RMSE: 1.1317570209503174\n",
      "Epoch 43,\n",
      "Train: \n",
      "  Loss: 23.489418029785156, MSE: 1.6075990200042725, RMSE: 1.2679113149642944, \n",
      "Val:\n",
      "   Loss: 6.746907711029053, Val MSE: 1.31248939037323, Val RMSE: 1.1456393003463745\n",
      "Epoch 44,\n",
      "Train: \n",
      "  Loss: 23.581003189086914, MSE: 1.6350479125976562, RMSE: 1.2786898612976074, \n",
      "Val:\n",
      "   Loss: 6.7068305015563965, Val MSE: 1.292495608329773, Val RMSE: 1.136879801750183\n",
      "Epoch 45,\n",
      "Train: \n",
      "  Loss: 23.494834899902344, MSE: 1.5628900527954102, RMSE: 1.250156044960022, \n",
      "Val:\n",
      "   Loss: 6.781986236572266, Val MSE: 1.3388421535491943, Val RMSE: 1.157083511352539\n",
      "Epoch 46,\n",
      "Train: \n",
      "  Loss: 23.430240631103516, MSE: 1.618819236755371, RMSE: 1.27232825756073, \n",
      "Val:\n",
      "   Loss: 6.723934173583984, Val MSE: 1.3141140937805176, Val RMSE: 1.1463481187820435\n",
      "Epoch 47,\n",
      "Train: \n",
      "  Loss: 23.469009399414062, MSE: 1.5854616165161133, RMSE: 1.2591511011123657, \n",
      "Val:\n",
      "   Loss: 6.7303571701049805, Val MSE: 1.316087007522583, Val RMSE: 1.147208333015442\n",
      "Epoch 48,\n",
      "Train: \n",
      "  Loss: 23.466306686401367, MSE: 1.6099097728729248, RMSE: 1.268822193145752, \n",
      "Val:\n",
      "   Loss: 6.710126876831055, Val MSE: 1.28164803981781, Val RMSE: 1.132098913192749\n",
      "Epoch 49,\n",
      "Train: \n",
      "  Loss: 23.51920509338379, MSE: 1.6790939569473267, RMSE: 1.2957985401153564, \n",
      "Val:\n",
      "   Loss: 6.751991271972656, Val MSE: 1.2748510837554932, Val RMSE: 1.129093050956726\n",
      "Epoch 50,\n",
      "Train: \n",
      "  Loss: 23.60870361328125, MSE: 1.7643184661865234, RMSE: 1.328276515007019, \n",
      "Val:\n",
      "   Loss: 6.792758941650391, Val MSE: 1.3337483406066895, Val RMSE: 1.1548802852630615\n",
      "Epoch 51,\n",
      "Train: \n",
      "  Loss: 23.470436096191406, MSE: 1.581544280052185, RMSE: 1.2575945854187012, \n",
      "Val:\n",
      "   Loss: 6.715903282165527, Val MSE: 1.307729959487915, Val RMSE: 1.1435601711273193\n",
      "Epoch 52,\n",
      "Train: \n",
      "  Loss: 23.46270751953125, MSE: 1.6120164394378662, RMSE: 1.2696521282196045, \n",
      "Val:\n",
      "   Loss: 6.8085856437683105, Val MSE: 1.3610512018203735, Val RMSE: 1.1666409969329834\n",
      "Epoch 53,\n",
      "Train: \n",
      "  Loss: 23.45584487915039, MSE: 1.6293283700942993, RMSE: 1.2764514684677124, \n",
      "Val:\n",
      "   Loss: 6.655486106872559, Val MSE: 1.2506191730499268, Val RMSE: 1.118310809135437\n",
      "Epoch 54,\n",
      "Train: \n",
      "  Loss: 23.34170150756836, MSE: 1.5107333660125732, RMSE: 1.2291189432144165, \n",
      "Val:\n",
      "   Loss: 6.782628059387207, Val MSE: 1.2957987785339355, Val RMSE: 1.1383315324783325\n",
      "Epoch 55,\n",
      "Train: \n",
      "  Loss: 23.414262771606445, MSE: 1.6040912866592407, RMSE: 1.2665272951126099, \n",
      "Val:\n",
      "   Loss: 6.703226089477539, Val MSE: 1.3082467317581177, Val RMSE: 1.1437861919403076\n",
      "Epoch 56,\n",
      "Train: \n",
      "  Loss: 23.505367279052734, MSE: 1.647716760635376, RMSE: 1.2836341857910156, \n",
      "Val:\n",
      "   Loss: 6.675136089324951, Val MSE: 1.234480857849121, Val RMSE: 1.1110719442367554\n",
      "Epoch 57,\n",
      "Train: \n",
      "  Loss: 23.69095230102539, MSE: 1.7825376987457275, RMSE: 1.3351171016693115, \n",
      "Val:\n",
      "   Loss: 6.7306013107299805, Val MSE: 1.332247018814087, Val RMSE: 1.154229998588562\n",
      "Epoch 58,\n",
      "Train: \n",
      "  Loss: 23.496274948120117, MSE: 1.596985101699829, RMSE: 1.2637187242507935, \n",
      "Val:\n",
      "   Loss: 6.7820892333984375, Val MSE: 1.3401684761047363, Val RMSE: 1.1576564311981201\n",
      "Epoch 59,\n",
      "Train: \n",
      "  Loss: 23.548282623291016, MSE: 1.6098768711090088, RMSE: 1.268809199333191, \n",
      "Val:\n",
      "   Loss: 6.759767055511475, Val MSE: 1.3090381622314453, Val RMSE: 1.1441320180892944\n",
      "Epoch 60,\n",
      "Train: \n",
      "  Loss: 23.60363006591797, MSE: 1.722564935684204, RMSE: 1.3124651908874512, \n",
      "Val:\n",
      "   Loss: 6.756019115447998, Val MSE: 1.3004379272460938, Val RMSE: 1.1403675079345703\n",
      "Epoch 61,\n",
      "Train: \n",
      "  Loss: 23.57602882385254, MSE: 1.6241756677627563, RMSE: 1.2744314670562744, \n",
      "Val:\n",
      "   Loss: 6.7585601806640625, Val MSE: 1.290911316871643, Val RMSE: 1.1361827850341797\n",
      "Epoch 62,\n",
      "Train: \n",
      "  Loss: 23.434335708618164, MSE: 1.5897331237792969, RMSE: 1.2608461380004883, \n",
      "Val:\n",
      "   Loss: 6.702646732330322, Val MSE: 1.2837544679641724, Val RMSE: 1.1330288648605347\n",
      "Epoch 63,\n",
      "Train: \n",
      "  Loss: 23.48343276977539, MSE: 1.6140422821044922, RMSE: 1.2704496383666992, \n",
      "Val:\n",
      "   Loss: 6.742358207702637, Val MSE: 1.298530101776123, Val RMSE: 1.1395306587219238\n",
      "Epoch 64,\n",
      "Train: \n",
      "  Loss: 23.392559051513672, MSE: 1.6397689580917358, RMSE: 1.2805346250534058, \n",
      "Val:\n",
      "   Loss: 6.77020788192749, Val MSE: 1.3417171239852905, Val RMSE: 1.1583251953125\n",
      "Epoch 65,\n",
      "Train: \n",
      "  Loss: 23.432132720947266, MSE: 1.6207807064056396, RMSE: 1.2730988264083862, \n",
      "Val:\n",
      "   Loss: 6.685761451721191, Val MSE: 1.2862861156463623, Val RMSE: 1.1341454982757568\n",
      "Epoch 66,\n",
      "Train: \n",
      "  Loss: 23.573123931884766, MSE: 1.6677461862564087, RMSE: 1.2914124727249146, \n",
      "Val:\n",
      "   Loss: 6.747007846832275, Val MSE: 1.3169889450073242, Val RMSE: 1.1476013660430908\n",
      "Epoch 67,\n",
      "Train: \n",
      "  Loss: 23.49992561340332, MSE: 1.6733872890472412, RMSE: 1.2935947179794312, \n",
      "Val:\n",
      "   Loss: 6.668629169464111, Val MSE: 1.2859891653060913, Val RMSE: 1.13401460647583\n",
      "Epoch 68,\n",
      "Train: \n",
      "  Loss: 23.44671630859375, MSE: 1.5308953523635864, RMSE: 1.2372936010360718, \n",
      "Val:\n",
      "   Loss: 6.752947807312012, Val MSE: 1.325646996498108, Val RMSE: 1.151367425918579\n",
      "Epoch 69,\n",
      "Train: \n",
      "  Loss: 23.48463249206543, MSE: 1.6767383813858032, RMSE: 1.2948893308639526, \n",
      "Val:\n",
      "   Loss: 6.746015548706055, Val MSE: 1.3286066055297852, Val RMSE: 1.1526520252227783\n",
      "Epoch 70,\n",
      "Train: \n",
      "  Loss: 23.61126136779785, MSE: 1.6672441959381104, RMSE: 1.2912181615829468, \n",
      "Val:\n",
      "   Loss: 6.780637741088867, Val MSE: 1.3533861637115479, Val RMSE: 1.16335129737854\n",
      "Epoch 71,\n",
      "Train: \n",
      "  Loss: 23.568817138671875, MSE: 1.6108967065811157, RMSE: 1.2692110538482666, \n",
      "Val:\n",
      "   Loss: 6.859543800354004, Val MSE: 1.3636081218719482, Val RMSE: 1.167736291885376\n",
      "Epoch 72,\n",
      "Train: \n",
      "  Loss: 23.571910858154297, MSE: 1.6666021347045898, RMSE: 1.2909694910049438, \n",
      "Val:\n",
      "   Loss: 6.6645636558532715, Val MSE: 1.2768498659133911, Val RMSE: 1.129977822303772\n",
      "Epoch 73,\n",
      "Train: \n",
      "  Loss: 23.450769424438477, MSE: 1.554261565208435, RMSE: 1.2467002868652344, \n",
      "Val:\n",
      "   Loss: 6.689117431640625, Val MSE: 1.2579293251037598, Val RMSE: 1.1215745210647583\n",
      "Epoch 74,\n",
      "Train: \n",
      "  Loss: 23.441822052001953, MSE: 1.6320406198501587, RMSE: 1.2775135040283203, \n",
      "Val:\n",
      "   Loss: 6.69735860824585, Val MSE: 1.2716008424758911, Val RMSE: 1.1276527643203735\n",
      "Epoch 75,\n",
      "Train: \n",
      "  Loss: 23.528053283691406, MSE: 1.6124051809310913, RMSE: 1.2698051929473877, \n",
      "Val:\n",
      "   Loss: 6.789603233337402, Val MSE: 1.3375656604766846, Val RMSE: 1.1565316915512085\n",
      "Epoch 76,\n",
      "Train: \n",
      "  Loss: 23.636001586914062, MSE: 1.7662229537963867, RMSE: 1.3289932012557983, \n",
      "Val:\n",
      "   Loss: 6.76017427444458, Val MSE: 1.3243757486343384, Val RMSE: 1.1508152484893799\n",
      "Epoch 77,\n",
      "Train: \n",
      "  Loss: 23.49271583557129, MSE: 1.639562964439392, RMSE: 1.280454158782959, \n",
      "Val:\n",
      "   Loss: 6.703675270080566, Val MSE: 1.2719544172286987, Val RMSE: 1.1278095245361328\n",
      "Epoch 78,\n",
      "Train: \n",
      "  Loss: 23.508840560913086, MSE: 1.686096429824829, RMSE: 1.2984977960586548, \n",
      "Val:\n",
      "   Loss: 6.7809882164001465, Val MSE: 1.3268508911132812, Val RMSE: 1.1518901586532593\n",
      "Epoch 79,\n",
      "Train: \n",
      "  Loss: 23.491910934448242, MSE: 1.5796273946762085, RMSE: 1.256832242012024, \n",
      "Val:\n",
      "   Loss: 6.763214111328125, Val MSE: 1.3421995639801025, Val RMSE: 1.1585333347320557\n",
      "Epoch 80,\n",
      "Train: \n",
      "  Loss: 23.502490997314453, MSE: 1.547719120979309, RMSE: 1.2440736293792725, \n",
      "Val:\n",
      "   Loss: 6.758860111236572, Val MSE: 1.3131300210952759, Val RMSE: 1.145918846130371\n",
      "Epoch 81,\n",
      "Train: \n",
      "  Loss: 23.36651039123535, MSE: 1.6669353246688843, RMSE: 1.2910984754562378, \n",
      "Val:\n",
      "   Loss: 6.727630615234375, Val MSE: 1.3033263683319092, Val RMSE: 1.141633152961731\n",
      "Epoch 82,\n",
      "Train: \n",
      "  Loss: 23.541629791259766, MSE: 1.661893606185913, RMSE: 1.289144515991211, \n",
      "Val:\n",
      "   Loss: 6.813058376312256, Val MSE: 1.3447002172470093, Val RMSE: 1.1596120595932007\n",
      "Epoch 83,\n",
      "Train: \n",
      "  Loss: 23.457073211669922, MSE: 1.6350692510604858, RMSE: 1.278698205947876, \n",
      "Val:\n",
      "   Loss: 6.706963062286377, Val MSE: 1.2629103660583496, Val RMSE: 1.1237928867340088\n",
      "Epoch 84,\n",
      "Train: \n",
      "  Loss: 23.70595932006836, MSE: 1.7218446731567383, RMSE: 1.3121907711029053, \n",
      "Val:\n",
      "   Loss: 6.7720537185668945, Val MSE: 1.3291581869125366, Val RMSE: 1.1528912782669067\n",
      "Epoch 85,\n",
      "Train: \n",
      "  Loss: 23.455184936523438, MSE: 1.5770275592803955, RMSE: 1.2557976245880127, \n",
      "Val:\n",
      "   Loss: 6.7453813552856445, Val MSE: 1.2940504550933838, Val RMSE: 1.1375633478164673\n",
      "Epoch 86,\n",
      "Train: \n",
      "  Loss: 23.476123809814453, MSE: 1.5618627071380615, RMSE: 1.2497450113296509, \n",
      "Val:\n",
      "   Loss: 6.687803745269775, Val MSE: 1.2939926385879517, Val RMSE: 1.137537956237793\n",
      "Epoch 87,\n",
      "Train: \n",
      "  Loss: 23.40032196044922, MSE: 1.533432960510254, RMSE: 1.2383185625076294, \n",
      "Val:\n",
      "   Loss: 6.786609649658203, Val MSE: 1.3166950941085815, Val RMSE: 1.1474733352661133\n",
      "Epoch 88,\n",
      "Train: \n",
      "  Loss: 23.48920249938965, MSE: 1.6772457361221313, RMSE: 1.2950851917266846, \n",
      "Val:\n",
      "   Loss: 6.684511184692383, Val MSE: 1.2643821239471436, Val RMSE: 1.1244474649429321\n",
      "Epoch 89,\n",
      "Train: \n",
      "  Loss: 23.681196212768555, MSE: 1.7197391986846924, RMSE: 1.3113882541656494, \n",
      "Val:\n",
      "   Loss: 6.761112213134766, Val MSE: 1.304336667060852, Val RMSE: 1.142075538635254\n",
      "Epoch 90,\n",
      "Train: \n",
      "  Loss: 23.462406158447266, MSE: 1.6102726459503174, RMSE: 1.268965244293213, \n",
      "Val:\n",
      "   Loss: 6.754659652709961, Val MSE: 1.309356927871704, Val RMSE: 1.1442713737487793\n",
      "Epoch 91,\n",
      "Train: \n",
      "  Loss: 23.45680046081543, MSE: 1.5058224201202393, RMSE: 1.2271195650100708, \n",
      "Val:\n",
      "   Loss: 6.734914302825928, Val MSE: 1.3202816247940063, Val RMSE: 1.149035096168518\n",
      "Epoch 92,\n",
      "Train: \n",
      "  Loss: 23.619779586791992, MSE: 1.6787192821502686, RMSE: 1.2956539392471313, \n",
      "Val:\n",
      "   Loss: 6.769370079040527, Val MSE: 1.3236443996429443, Val RMSE: 1.1504974365234375\n",
      "Epoch 93,\n",
      "Train: \n",
      "  Loss: 23.411190032958984, MSE: 1.5201759338378906, RMSE: 1.2329541444778442, \n",
      "Val:\n",
      "   Loss: 6.6961164474487305, Val MSE: 1.2530053853988647, Val RMSE: 1.1193772554397583\n",
      "Epoch 94,\n",
      "Train: \n",
      "  Loss: 23.426668167114258, MSE: 1.6563163995742798, RMSE: 1.2869795560836792, \n",
      "Val:\n",
      "   Loss: 6.688715934753418, Val MSE: 1.278641939163208, Val RMSE: 1.1307705640792847\n",
      "Epoch 95,\n",
      "Train: \n",
      "  Loss: 23.502532958984375, MSE: 1.6184650659561157, RMSE: 1.2721891403198242, \n",
      "Val:\n",
      "   Loss: 6.770359516143799, Val MSE: 1.3334380388259888, Val RMSE: 1.1547458171844482\n",
      "Epoch 96,\n",
      "Train: \n",
      "  Loss: 23.51517677307129, MSE: 1.758803129196167, RMSE: 1.326198697090149, \n",
      "Val:\n",
      "   Loss: 6.706201553344727, Val MSE: 1.293477177619934, Val RMSE: 1.137311339378357\n",
      "Epoch 97,\n",
      "Train: \n",
      "  Loss: 23.464689254760742, MSE: 1.5944396257400513, RMSE: 1.2627111673355103, \n",
      "Val:\n",
      "   Loss: 6.710325241088867, Val MSE: 1.2529933452606201, Val RMSE: 1.1193718910217285\n",
      "Epoch 98,\n",
      "Train: \n",
      "  Loss: 23.36818504333496, MSE: 1.578721284866333, RMSE: 1.2564717531204224, \n",
      "Val:\n",
      "   Loss: 6.763550758361816, Val MSE: 1.3296247720718384, Val RMSE: 1.1530935764312744\n",
      "Epoch 99,\n",
      "Train: \n",
      "  Loss: 23.541961669921875, MSE: 1.6566789150238037, RMSE: 1.2871203422546387, \n",
      "Val:\n",
      "   Loss: 6.704715251922607, Val MSE: 1.282577395439148, Val RMSE: 1.1325093507766724\n",
      "Epoch 100,\n",
      "Train: \n",
      "  Loss: 23.445606231689453, MSE: 1.6304696798324585, RMSE: 1.2768985033035278, \n",
      "Val:\n",
      "   Loss: 6.74462890625, Val MSE: 1.3058631420135498, Val RMSE: 1.142743706703186\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, epochs, X_train, y_train, X_val, y_val):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred, stochastic_gates_pred = model(X_train)\n",
    "        loss = custom_loss_function(y_pred, y_train, stochastic_gates_pred, sigma=1, D=X_train.shape[1], lambda_reg=0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse = mse_metric(y_pred, y_train).item()\n",
    "        train_rmse = rmse_metric(y_pred, y_train).item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_pred, val_stochastic_gates = model(X_val)\n",
    "            val_loss = custom_loss_function(val_y_pred, y_val, val_stochastic_gates, sigma=1, D=X_val.shape[1], lambda_reg=0.05)\n",
    "            val_mse = mse_metric(val_y_pred, y_val).item()\n",
    "            val_rmse = rmse_metric(val_y_pred, y_val).item()\n",
    "        #\"Gates: {stochastic_gates_pred}\\n \n",
    "        print(f'Epoch {epoch+1},\\nTrain: \\n  Loss: {loss.item()}, MSE: {train_mse}, RMSE: {train_rmse}, \\nVal:\\n   Loss: {val_loss.item()}, Val MSE: {val_mse}, Val RMSE: {val_rmse}')\n",
    "    return stochastic_gates_pred\n",
    "\n",
    "# Initialize the hypernetwork, prediction network, loss criterion, and optimizer\n",
    "model = C_StochasticGates(input_dim=X_train_tensor.shape[1])\n",
    "optimizer = Adam(list(model.parameters()) , lr=0.001)\n",
    "# c_stg = C_StochasticGates(input_dim=X_train_tensor.shape[1])\n",
    "\n",
    "# Training\n",
    "epochs = 100  # Adjust as necessary based on convergence and performance\n",
    "gates = train(c_stg,  optimizer, epochs, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "## TESTING THE MODEL\n",
    "y_pred, _ = model(X_test_tensor)\n",
    "y_pred_array = y_pred.detach().numpy()\n",
    "spot_results_dict = {\n",
    "        'R2 Score': round(r2_score(y_test_tensor.detach().numpy(), y_pred_array),3),\n",
    "        'RMSE': round(mean_squared_error(y_test_tensor.detach().numpy(), y_pred_array, squared=False),3),\n",
    "        'MAE': round(mean_absolute_error(y_test_tensor.detach().numpy(), y_pred_array),3),\n",
    "        'MAPE': round(mean_absolute_percentage_error(y_test_tensor.detach().numpy(), y_pred_array),3),\n",
    "        # 'gates_found': model.get_gates(mode='prob').astype(str),\n",
    "    }\n",
    "\n",
    "spot_results_dict = pd.DataFrame(spot_results_dict, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.405</td>\n",
       "      <td>1.203</td>\n",
       "      <td>0.966</td>\n",
       "      <td>2.783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R2 Score   RMSE    MAE   MAPE\n",
       "0    -0.405  1.203  0.966  2.783"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define a function to train and evaluate models using STG algorithm\n",
    "# def train_evaluate_stg(X_train, y_train, X_test, y_test, context_dim):\n",
    "#     # Instantiate HyperNetwork and PredictionNetwork for STG\n",
    "#     hypernetwork_stg = HyperNetwork(context_dim, X_train.shape[1] - context_dim)\n",
    "#     prediction_network_stg = PredictionNetwork(X_train.shape[1] - context_dim)\n",
    "\n",
    "#     # Train the model\n",
    "#     stg_losses_train, stg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "#                                                     hypernetwork_stg, prediction_network_stg)\n",
    "#     return stg_losses_train, stg_losses_val\n",
    "\n",
    "# # Define a function to train and evaluate models using CSTG algorithm\n",
    "# def train_evaluate_cstg(X_train, y_train, X_test, y_test, context_dim):\n",
    "#     # Instantiate ConditionalStochasticGates model\n",
    "#     cstg_model = ConditionalStochasticGates(context_dim, X_train.shape[1] - context_dim)\n",
    "\n",
    "#     # Train the model\n",
    "#     cstg_losses_train, cstg_losses_val = train_model(X_train, y_train, X_test, y_test, \n",
    "#                                                       cstg_model, prediction_network)\n",
    "#     return cstg_losses_train, cstg_losses_val\n",
    "\n",
    "# # Define a function to train the model and return training and validation losses\n",
    "# def train_model(X_train, y_train, X_test, y_test, hypernetwork, prediction_network):\n",
    "#     optimizer = optim.Adam(list(hypernetwork.parameters()) + list(prediction_network.parameters()), lr=0.001)\n",
    "#     loss_fn = nn.MSELoss()\n",
    "\n",
    "#     num_epochs = 100\n",
    "#     stg_losses_train, stg_losses_val = [], []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training step\n",
    "#         hypernetwork.train()\n",
    "#         prediction_network.train()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         gates_train = hypernetwork(X_train[:, :context_dim])\n",
    "#         selected_features_train = X_train[:, context_dim:] * stochastic_gates(gates_train)\n",
    "#         predictions_train = prediction_network(selected_features_train)\n",
    "#         loss_train = loss_fn(predictions_train, y_train)\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss_train.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             hypernetwork.eval()\n",
    "#             prediction_network.eval()\n",
    "\n",
    "#             gates_val = hypernetwork(X_test[:, :context_dim])\n",
    "#             selected_features_val = X_test[:, context_dim:] * stochastic_gates(gates_val)\n",
    "#             predictions_val = prediction_network(selected_features_val)\n",
    "#             loss_val = loss_fn(predictions_val, y_test)\n",
    "\n",
    "#         stg_losses_train.append(loss_train.item())\n",
    "#         stg_losses_val.append(loss_val.item())\n",
    "\n",
    "#     return stg_losses_train, stg_losses_val\n",
    "\n",
    "# # Define the context dimension\n",
    "# context_dim = 3  # Assuming the first 3 features are used as context\n",
    "\n",
    "# # Train and evaluate models using STG algorithm\n",
    "# stg_losses_train, stg_losses_val = train_evaluate_stg(X_train_tensor, y_train_tensor, \n",
    "#                                                       X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# # Train and evaluate models using CSTG algorithm\n",
    "# cstg_losses_train, cstg_losses_val = train_evaluate_cstg(X_train_tensor, y_train_tensor, \n",
    "#                                                           X_test_tensor, y_test_tensor, context_dim)\n",
    "\n",
    "# # Plot training and validation losses for STG and CSTG algorithms\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, num_epochs + 1), stg_losses_train, label='STG Train Loss', color='blue')\n",
    "# plt.plot(range(1, num_epochs + 1), stg_losses_val, label='STG Validation Loss', linestyle='--', color='blue')\n",
    "# plt.plot(range(1, num_epochs + 1), cstg_losses_train, label='CSTG Train Loss', color='orange')\n",
    "# plt.plot(range(1, num_epochs + 1), cstg_losses_val, label='CSTG Validation Loss', linestyle='--', color='orange')\n",
    "# plt.title('Training and Validation Losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
